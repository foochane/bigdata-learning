2019-07-07 11:12:52,002   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-07 11:12:52,769   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-07 11:12:52,863   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-07 11:12:52,863   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-07 11:12:52,863   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-07 11:12:52,879   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-07 11:12:52,879   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-07 11:12:57,503   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 11373.
2019-07-07 11:12:57,550   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-07 11:12:57,581   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-07 11:12:57,581   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-07 11:12:57,581   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-07 11:12:57,613   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-c6ef1ed4-02d5-4902-8ca1-14a4266ddef1
2019-07-07 11:12:57,644   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-07 11:12:57,675   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-07 11:12:57,800   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @18463ms
2019-07-07 11:12:58,070   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-07 11:12:58,133   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @18794ms
2019-07-07 11:12:58,195   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@10c92f17{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-07 11:12:58,195   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-07 11:12:58,273   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,273   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,273   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-07 11:12:58,336   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-07 11:12:58,978   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-07 11:12:59,166   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11396.
2019-07-07 11:12:59,166   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:11396
2019-07-07 11:12:59,166   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-07 11:12:59,212   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 11396, None)
2019-07-07 11:12:59,212   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:11396 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 11396, None)
2019-07-07 11:12:59,212   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 11396, None)
2019-07-07 11:12:59,212   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 11396, None)
2019-07-07 11:12:59,494   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-07 11:13:00,728   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-07 11:13:00,900   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-07 11:13:00,900   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:11396 (size: 20.4 KB, free: 1426.5 MB)
2019-07-07 11:13:00,915   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at WordCount.scala:22
2019-07-07 11:13:01,275   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at WordCount.scala:25
2019-07-07 11:13:02,526   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-07 11:13:02,854   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at WordCount.scala:22)
2019-07-07 11:13:02,854   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at WordCount.scala:22)
2019-07-07 11:13:02,854   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at WordCount.scala:25) with 1 output partitions
2019-07-07 11:13:02,870   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at WordCount.scala:25)
2019-07-07 11:13:02,870   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-07 11:13:02,870   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-07 11:13:02,886   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:22), which has no missing parents
2019-07-07 11:13:02,979   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.6 KB, free 1426.3 MB)
2019-07-07 11:13:02,995   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-07 11:13:02,995   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:11396 (size: 3.3 KB, free: 1426.5 MB)
2019-07-07 11:13:02,995   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-07 11:13:03,026   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:22) (first 15 tasks are for partitions Vector(0, 1))
2019-07-07 11:13:03,026   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-07 11:13:03,120   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7360 bytes)
2019-07-07 11:13:03,120   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7360 bytes)
2019-07-07 11:13:03,151   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-07 11:13:03,151   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-07 11:13:04,151   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:21+21
2019-07-07 11:13:04,151   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:0+21
2019-07-07 11:13:05,682   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-07 11:13:05,682   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-07 11:13:05,698   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2578 ms on localhost (executor driver) (1/2)
2019-07-07 11:13:05,698   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2609 ms on localhost (executor driver) (2/2)
2019-07-07 11:13:05,698   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-07 11:13:05,714   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at WordCount.scala:22) finished in 2.766 s
2019-07-07 11:13:05,714   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-07 11:13:05,714   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-07 11:13:05,714   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-07 11:13:05,729   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-07 11:13:05,729   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:22), which has no missing parents
2019-07-07 11:13:05,745   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.1 KB, free 1426.3 MB)
2019-07-07 11:13:05,745   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 1426.3 MB)
2019-07-07 11:13:05,760   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:11396 (size: 3.0 KB, free: 1426.5 MB)
2019-07-07 11:13:05,760   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-07 11:13:05,760   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:22) (first 15 tasks are for partitions Vector(0))
2019-07-07 11:13:05,760   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-07 11:13:05,760   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-07 11:13:05,760   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-07 11:13:05,807   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-07 11:13:05,807   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-07 11:13:05,932   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-07 11:13:05,932   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 172 ms on localhost (executor driver) (1/1)
2019-07-07 11:13:05,932   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-07 11:13:05,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at WordCount.scala:22) finished in 0.203 s
2019-07-07 11:13:05,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-07 11:13:05,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-07 11:13:05,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-07 11:13:05,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-07 11:13:05,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:22), which has no missing parents
2019-07-07 11:13:05,948   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-07 11:13:05,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-07 11:13:05,964   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:11396 (size: 2.5 KB, free: 1426.5 MB)
2019-07-07 11:13:05,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-07 11:13:05,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:22) (first 15 tasks are for partitions Vector(0))
2019-07-07 11:13:05,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-07 11:13:05,979   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-07 11:13:05,979   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-07 11:13:06,010   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-07 11:13:06,010   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-07 11:13:06,026   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1387 bytes result sent to driver
2019-07-07 11:13:06,042   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 78 ms on localhost (executor driver) (1/1)
2019-07-07 11:13:06,042   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at WordCount.scala:25) finished in 0.094 s
2019-07-07 11:13:06,042   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-07 11:13:06,057   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at WordCount.scala:25, took 4.777262 s
2019-07-07 11:13:06,057   INFO --- [main]  WordCount$(line:30) : complete!
2019-07-07 11:13:06,073   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@10c92f17{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-07 11:13:06,073   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-07 11:13:06,104   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-07 11:13:06,260   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-07 11:13:06,260   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-07 11:13:06,260   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-07 11:13:06,276   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-07 11:13:06,292   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-07 11:13:06,292   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-07 11:13:06,292   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-dd15c74a-6b7b-4130-86f2-b8b7c2d2a545
2019-07-07 11:22:52,586   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-07 11:22:53,196   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-07 11:22:53,289   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-07 11:22:53,305   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-07 11:22:53,305   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-07 11:22:53,305   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-07 11:22:53,305   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-07 11:22:57,149   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 11621.
2019-07-07 11:22:57,180   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-07 11:22:57,211   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-07 11:22:57,227   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-07 11:22:57,227   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-07 11:22:57,242   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-f7e06f5a-50d6-44ec-bc10-44b150ba4ae0
2019-07-07 11:22:57,289   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-07 11:22:57,305   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-07 11:22:57,445   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16310ms
2019-07-07 11:22:57,539   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-07 11:22:57,555   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16421ms
2019-07-07 11:22:57,586   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-07 11:22:57,602   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-07 11:22:57,633   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,633   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,633   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,664   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,664   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,664   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,664   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,664   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,664   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,664   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,680   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,680   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,695   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,695   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,695   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-07 11:22:57,695   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-07 11:22:58,039   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-07 11:22:59,367   WARN --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:87) : Failed to connect to master Node02:7077
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:106)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to connect to Node02/192.168.1.112:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:198)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190)
	... 4 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: no further information: Node02/192.168.1.112:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	... 1 more
Caused by: java.net.ConnectException: Connection refused: no further information
	... 11 more
2019-07-07 11:23:18,054   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-07 11:23:19,101   WARN --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:87) : Failed to connect to master Node02:7077
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:106)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to connect to Node02/192.168.1.112:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:198)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190)
	... 4 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: no further information: Node02/192.168.1.112:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	... 1 more
Caused by: java.net.ConnectException: Connection refused: no further information
	... 11 more
2019-07-07 11:23:38,069   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-07 11:23:39,116   WARN --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:87) : Failed to connect to master Node02:7077
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:226)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)
	at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:106)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to connect to Node02/192.168.1.112:7077
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)
	at org.apache.spark.rpc.netty.NettyRpcEnv.createClient(NettyRpcEnv.scala:198)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:194)
	at org.apache.spark.rpc.netty.Outbox$$anon$1.call(Outbox.scala:190)
	... 4 more
Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: no further information: Node02/192.168.1.112:7077
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:323)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:633)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	... 1 more
Caused by: java.net.ConnectException: Connection refused: no further information
	... 11 more
2019-07-07 11:23:58,076   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-07-07 11:23:58,077  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-07-07 11:23:58,122   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-07 11:23:58,141   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-07 11:23:58,142   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11667.
2019-07-07 11:23:58,143   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:11667
2019-07-07 11:23:58,146   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-07 11:23:58,146   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-07 11:23:58,202   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-07 11:23:58,262   WARN --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-07-07 11:23:58,262   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 11667, None)
2019-07-07 11:23:58,278   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-07 11:23:58,278   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:11667 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 11667, None)
2019-07-07 11:23:58,325   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 11667, None)
2019-07-07 11:23:58,325   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 11667, None)
2019-07-07 11:23:58,356   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-07 11:23:58,356   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-07 11:23:58,372   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-07 11:23:58,372   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-07 11:23:58,387   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-07 11:23:58,622  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem
	at scala.Predef$.require(Predef.scala:277)
	at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:91)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:516)
	at WordCount$.main(WordCount.scala:22)
	at WordCount.main(WordCount.scala)
2019-07-07 11:23:58,622   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-07-07 11:23:59,438   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-07 11:23:59,441   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-f8b9f2b2-cbd0-45c1-913e-8c0251d70113
2019-07-07 11:28:43,645   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-07 11:28:44,254   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-07 11:28:44,348   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-07 11:28:44,348   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-07 11:28:44,348   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-07 11:28:44,348   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-07 11:28:44,364   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-07 11:28:48,270   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 11799.
2019-07-07 11:28:48,315   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-07 11:28:48,356   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-07 11:28:48,363   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-07 11:28:48,365   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-07 11:28:48,391   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-f6d2a98e-266d-41a6-bc43-78d0f99857eb
2019-07-07 11:28:48,452   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-07 11:28:48,491   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-07 11:28:48,695   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16523ms
2019-07-07 11:28:48,789   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-07 11:28:48,820   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16650ms
2019-07-07 11:28:48,856   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@77be8c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-07 11:28:48,857   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-07 11:28:48,909   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,920   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,923   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,925   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,926   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,930   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,932   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,933   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,934   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,936   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,938   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,942   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,943   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,947   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,948   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,950   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,952   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,954   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,970   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,973   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,974   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,981   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-07 11:28:48,985   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-07 11:28:49,341   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-07 11:28:49,420   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 47 ms (0 ms spent in bootstraps)
2019-07-07 11:29:09,344   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-07 11:29:29,353   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-07 11:29:49,368  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-07-07 11:29:49,368   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-07-07 11:29:49,384   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@77be8c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-07 11:29:49,384   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-07 11:29:49,384   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11834.
2019-07-07 11:29:49,399   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:11834
2019-07-07 11:29:49,399   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-07 11:29:49,399   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-07 11:29:49,399   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-07 11:29:49,415   WARN --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-07-07 11:29:49,446   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-07 11:29:49,462   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-07 11:29:49,475   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 11834, None)
2019-07-07 11:29:49,476   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-07 11:29:49,489   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-07 11:29:49,490   WARN --- [stop-spark-context]  org.apache.spark.metrics.MetricsSystem(line:66) : Stopping a MetricsSystem that is not running
2019-07-07 11:29:49,493   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:11834 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 11834, None)
2019-07-07 11:29:49,495   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-07 11:29:49,500   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 11834, None)
2019-07-07 11:29:49,502   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 11834, None)
2019-07-07 11:29:49,513   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-07 11:29:49,880   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2b27cc70{/metrics/json,null,AVAILABLE,@Spark}
2019-07-07 11:29:49,896  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.NullPointerException
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:560)
	at WordCount$.main(WordCount.scala:22)
	at WordCount.main(WordCount.scala)
2019-07-07 11:29:49,896   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-07-07 11:29:49,912   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-07 11:29:49,912  ERROR --- [spark-listener-group-appStatus]  org.apache.spark.scheduler.AsyncEventQueue(line:91) : Listener AppStatusListener threw an exception
java.lang.NullPointerException
	at org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:167)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:91)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:76)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:92)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:83)
2019-07-07 11:29:49,912   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-438ed205-ea69-4062-ae09-a9b8a31d045a
2019-07-07 11:36:17,618   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-07 11:36:18,571   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-07 11:36:18,727   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-07 11:36:18,743   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-07 11:36:18,743   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-07 11:36:18,743   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-07 11:36:18,743   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-07 11:36:22,821   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 11976.
2019-07-07 11:36:22,884   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-07 11:36:22,930   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-07 11:36:22,946   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-07 11:36:22,946   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-07 11:36:22,977   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-60044c8c-a706-4f39-bbe8-01a4c9c7b1af
2019-07-07 11:36:23,009   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-07 11:36:23,024   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-07 11:36:23,165   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17048ms
2019-07-07 11:36:23,243   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-07 11:36:23,259   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17151ms
2019-07-07 11:36:23,290   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-07 11:36:23,290   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,352   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,352   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,352   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,352   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,352   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,352   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,352   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,352   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,368   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,368   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,368   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,368   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,368   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-07 11:36:23,368   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-07 11:36:23,634   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-07 11:36:23,743   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11997.
2019-07-07 11:36:23,743   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:11997
2019-07-07 11:36:23,743   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-07 11:36:23,774   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 11997, None)
2019-07-07 11:36:23,790   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:11997 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 11997, None)
2019-07-07 11:36:23,790   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 11997, None)
2019-07-07 11:36:23,790   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 11997, None)
2019-07-07 11:36:24,055   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-07 11:36:25,149   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-07 11:36:25,274   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-07 11:36:25,290   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:11997 (size: 20.4 KB, free: 1426.5 MB)
2019-07-07 11:36:25,290   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at WordCount.scala:31
2019-07-07 11:36:25,618   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at WordCount.scala:37
2019-07-07 11:36:26,477   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-07 11:36:26,821   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at WordCount.scala:31)
2019-07-07 11:36:26,821   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at WordCount.scala:31)
2019-07-07 11:36:26,837   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at WordCount.scala:37) with 1 output partitions
2019-07-07 11:36:26,837   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at WordCount.scala:37)
2019-07-07 11:36:26,837   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-07 11:36:26,837   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-07 11:36:26,852   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:31), which has no missing parents
2019-07-07 11:36:26,946   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.6 KB, free 1426.3 MB)
2019-07-07 11:36:26,962   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-07 11:36:26,962   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:11997 (size: 3.3 KB, free: 1426.5 MB)
2019-07-07 11:36:26,962   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-07 11:36:26,977   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:31) (first 15 tasks are for partitions Vector(0, 1))
2019-07-07 11:36:26,993   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-07 11:36:27,071   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7360 bytes)
2019-07-07 11:36:27,087   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7360 bytes)
2019-07-07 11:36:27,102   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-07 11:36:27,102   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-07 11:36:28,118   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:0+21
2019-07-07 11:36:28,118   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:21+21
2019-07-07 11:36:29,524   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-07 11:36:29,524   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-07 11:36:29,555   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2453 ms on localhost (executor driver) (1/2)
2019-07-07 11:36:29,555   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2500 ms on localhost (executor driver) (2/2)
2019-07-07 11:36:29,555   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-07 11:36:29,571   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at WordCount.scala:31) finished in 2.656 s
2019-07-07 11:36:29,571   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-07 11:36:29,571   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-07 11:36:29,571   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-07 11:36:29,586   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-07 11:36:29,586   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:31), which has no missing parents
2019-07-07 11:36:29,602   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.1 KB, free 1426.3 MB)
2019-07-07 11:36:29,602   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 1426.3 MB)
2019-07-07 11:36:29,618   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:11997 (size: 3.0 KB, free: 1426.5 MB)
2019-07-07 11:36:29,618   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-07 11:36:29,618   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:31) (first 15 tasks are for partitions Vector(0))
2019-07-07 11:36:29,618   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-07 11:36:29,618   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-07 11:36:29,618   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-07 11:36:29,665   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-07 11:36:29,665   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-07 11:36:29,758   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-07 11:36:29,774   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 156 ms on localhost (executor driver) (1/1)
2019-07-07 11:36:29,774   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-07 11:36:29,774   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at WordCount.scala:31) finished in 0.188 s
2019-07-07 11:36:29,774   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-07 11:36:29,774   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-07 11:36:29,774   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-07 11:36:29,774   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-07 11:36:29,774   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:31), which has no missing parents
2019-07-07 11:36:29,774   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-07 11:36:29,790   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-07 11:36:29,790   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:11997 (size: 2.5 KB, free: 1426.5 MB)
2019-07-07 11:36:29,790   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-07 11:36:29,790   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:31) (first 15 tasks are for partitions Vector(0))
2019-07-07 11:36:29,790   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-07 11:36:29,805   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-07 11:36:29,805   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-07 11:36:29,821   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-07 11:36:29,821   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-07 11:36:29,836   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1387 bytes result sent to driver
2019-07-07 11:36:29,836   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 31 ms on localhost (executor driver) (1/1)
2019-07-07 11:36:29,836   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-07 11:36:29,852   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at WordCount.scala:37) finished in 0.078 s
2019-07-07 11:36:29,868   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at WordCount.scala:37, took 4.244468 s
2019-07-07 11:36:29,883   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-07 11:36:29,899   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-07 11:36:29,915   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-07 11:36:30,040   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-07 11:36:30,040   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-07 11:36:30,055   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-07 11:36:30,055   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-07 11:36:30,071   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-07 11:36:30,071   INFO --- [main]  WordCount$(line:46) : complete!
2019-07-07 11:36:30,071   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-07 11:36:30,071   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-5795c791-48ad-4619-acc6-feb85bd7b23e
2019-07-09 19:01:03,172   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:01:06,347   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:01:06,965   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:01:06,974   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:01:06,976   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:01:06,980   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:01:06,982   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:01:12,031   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14135.
2019-07-09 19:01:12,281   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:01:12,609   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:01:12,625   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:01:12,625   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:01:12,672   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-7885deaa-3a1e-42a8-9e3a-06d395fd6266
2019-07-09 19:01:12,983   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:01:13,194   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:01:13,879   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @24354ms
2019-07-09 19:01:14,259   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:01:14,391   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @24869ms
2019-07-09 19:01:14,491   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:01:14,491   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:01:14,614   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,616   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,671   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,674   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,675   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,677   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,678   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,680   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,681   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,683   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,685   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,686   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,688   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,690   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,692   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,694   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,696   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,697   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,699   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,701   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,746   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,747   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,749   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,750   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,755   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:01:14,758   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:01:15,446   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:01:16,294   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14156.
2019-07-09 19:01:16,295   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14156
2019-07-09 19:01:16,376   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:01:16,601   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14156, None)
2019-07-09 19:01:16,643   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14156 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14156, None)
2019-07-09 19:01:16,721   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14156, None)
2019-07-09 19:01:16,723   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14156, None)
2019-07-09 19:01:17,212   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:01:19,313   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:01:19,719   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:01:19,735   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14156 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:01:19,735   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:01:20,485   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:37
2019-07-09 19:01:20,757   WARN --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:87) : Creating new stage failed due to exception - job: 0
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/d/data/word.txt
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:94)
	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:87)
	at org.apache.spark.rdd.RDD.$anonfun$dependencies$2(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:238)
	at org.apache.spark.scheduler.DAGScheduler.getShuffleDependencies(DAGScheduler.scala:512)
	at org.apache.spark.scheduler.DAGScheduler.getMissingAncestorShuffleDependencies(DAGScheduler.scala:479)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:346)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:462)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:46)
	at scala.collection.SetLike.map(SetLike.scala:100)
	at scala.collection.SetLike.map$(SetLike.scala:100)
	at scala.collection.mutable.AbstractSet.map(Set.scala:46)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:461)
	at org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:448)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:962)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2019-07-09 19:01:20,823   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 failed: collect at test.scala:37, took 0.277195 s
2019-07-09 19:01:20,855   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-07-09 19:01:20,901   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:01:20,917   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:01:21,120   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:01:21,167   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:01:21,167   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:01:21,276   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:01:21,292   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:01:21,308   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:01:21,308   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:01:21,308   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-917cdbc2-4ffd-49bd-8b86-619f251a689d
2019-07-09 19:02:29,329   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:02:29,985   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:02:30,094   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:02:30,094   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:02:30,094   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:02:30,094   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:02:30,094   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:02:34,343   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14189.
2019-07-09 19:02:34,374   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:02:34,405   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:02:34,405   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:02:34,405   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:02:34,437   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-18a89e14-169b-4c44-bf1a-2abdfc5dc690
2019-07-09 19:02:34,468   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:02:34,499   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:02:34,624   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16633ms
2019-07-09 19:02:34,718   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:02:34,733   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16745ms
2019-07-09 19:02:34,765   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@46354914{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:02:34,765   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:02:34,820   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,821   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,822   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,823   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,825   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,827   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,829   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,830   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,831   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,833   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,835   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,836   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,837   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,839   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,840   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,842   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,843   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,845   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,859   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,862   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,865   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,872   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,879   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:02:34,884   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:02:35,219   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:02:35,335   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14210.
2019-07-09 19:02:35,336   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14210
2019-07-09 19:02:35,339   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:02:35,375   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14210, None)
2019-07-09 19:02:35,382   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14210 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14210, None)
2019-07-09 19:02:35,386   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14210, None)
2019-07-09 19:02:35,388   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14210, None)
2019-07-09 19:02:35,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:02:36,717   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:02:36,845   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:02:36,845   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14210 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:02:36,861   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:02:37,236   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:37
2019-07-09 19:02:37,298   WARN --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:87) : Creating new stage failed due to exception - job: 0
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:D:/data
	at org.apache.hadoop.fs.Path.initialize(Path.java:206)
	at org.apache.hadoop.fs.Path.<init>(Path.java:172)
	at org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:244)
	at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:409)
	at org.apache.spark.SparkContext.$anonfun$hadoopFile$2(SparkContext.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$hadoopFile$2$adapted(SparkContext.scala:1036)
	at org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8(HadoopRDD.scala:180)
	at org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8$adapted(HadoopRDD.scala:180)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$6(HadoopRDD.scala:180)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:177)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:94)
	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:87)
	at org.apache.spark.rdd.RDD.$anonfun$dependencies$2(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:238)
	at org.apache.spark.scheduler.DAGScheduler.getShuffleDependencies(DAGScheduler.scala:512)
	at org.apache.spark.scheduler.DAGScheduler.getMissingAncestorShuffleDependencies(DAGScheduler.scala:479)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:346)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:462)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:46)
	at scala.collection.SetLike.map(SetLike.scala:100)
	at scala.collection.SetLike.map$(SetLike.scala:100)
	at scala.collection.mutable.AbstractSet.map(Set.scala:46)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:461)
	at org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:448)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:962)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: file:D:/data
	at java.net.URI.checkPath(URI.java:1823)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:203)
	... 51 more
2019-07-09 19:02:37,314   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 failed: collect at test.scala:37, took 0.077788 s
2019-07-09 19:02:37,345   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-07-09 19:02:37,361   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@46354914{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:02:37,361   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:02:37,392   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:02:37,439   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:02:37,439   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:02:37,455   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:02:37,564   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:02:37,658   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:02:37,658   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:02:37,658   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-5e88cc0f-d012-4009-994b-70114e67d82a
2019-07-09 19:03:32,637   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:03:33,251   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:03:33,360   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:03:33,360   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:03:33,360   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:03:33,360   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:03:33,360   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:03:37,265   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14243.
2019-07-09 19:03:37,297   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:03:37,328   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:03:37,344   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:03:37,344   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:03:37,359   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-aff315eb-6b60-4258-8512-b2ecfd397a50
2019-07-09 19:03:37,406   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:03:37,422   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:03:37,562   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16755ms
2019-07-09 19:03:37,640   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:03:37,656   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16855ms
2019-07-09 19:03:37,687   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:03:37,687   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:03:37,734   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,734   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,734   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,734   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,734   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,734   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,734   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,734   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,734   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,750   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,750   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,781   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,797   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,797   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,812   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,812   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,812   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:03:37,829   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:03:38,127   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:03:38,230   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14264.
2019-07-09 19:03:38,231   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14264
2019-07-09 19:03:38,235   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:03:38,275   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14264, None)
2019-07-09 19:03:38,283   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14264 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14264, None)
2019-07-09 19:03:38,288   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14264, None)
2019-07-09 19:03:38,289   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14264, None)
2019-07-09 19:03:38,590   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:03:39,581   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:03:39,713   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:03:39,733   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14264 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:03:39,741   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:03:40,072   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:37
2019-07-09 19:03:40,134   WARN --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:87) : Creating new stage failed due to exception - job: 0
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:D:/data/word.txt
	at org.apache.hadoop.fs.Path.initialize(Path.java:206)
	at org.apache.hadoop.fs.Path.<init>(Path.java:172)
	at org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:244)
	at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:409)
	at org.apache.spark.SparkContext.$anonfun$hadoopFile$2(SparkContext.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$hadoopFile$2$adapted(SparkContext.scala:1036)
	at org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8(HadoopRDD.scala:180)
	at org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8$adapted(HadoopRDD.scala:180)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$6(HadoopRDD.scala:180)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:177)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:94)
	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:87)
	at org.apache.spark.rdd.RDD.$anonfun$dependencies$2(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:238)
	at org.apache.spark.scheduler.DAGScheduler.getShuffleDependencies(DAGScheduler.scala:512)
	at org.apache.spark.scheduler.DAGScheduler.getMissingAncestorShuffleDependencies(DAGScheduler.scala:479)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:346)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:462)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:46)
	at scala.collection.SetLike.map(SetLike.scala:100)
	at scala.collection.SetLike.map$(SetLike.scala:100)
	at scala.collection.mutable.AbstractSet.map(Set.scala:46)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:461)
	at org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:448)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:962)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: file:D:/data/word.txt
	at java.net.URI.checkPath(URI.java:1823)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:203)
	... 51 more
2019-07-09 19:03:40,165   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 failed: collect at test.scala:37, took 0.087369 s
2019-07-09 19:03:40,212   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-07-09 19:03:40,243   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:03:40,243   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:03:40,275   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:03:40,540   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:03:40,540   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:03:40,556   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:03:40,572   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:03:40,587   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:03:40,587   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:03:40,587   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-88d6f9a9-658c-45ab-ab82-55fa1b32dabc
2019-07-09 19:04:51,588   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:04:52,204   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:04:52,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:04:52,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:04:52,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:04:52,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:04:52,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:04:56,289   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14298.
2019-07-09 19:04:56,320   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:04:56,352   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:04:56,352   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:04:56,352   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:04:56,383   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-b845e9e4-aa24-4f3e-aca0-cb6f6b767baa
2019-07-09 19:04:56,414   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:04:56,445   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:04:56,602   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @18006ms
2019-07-09 19:04:56,680   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:04:56,711   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @18111ms
2019-07-09 19:04:56,742   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:04:56,742   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:04:56,773   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,773   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,773   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,773   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,773   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,773   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,773   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,773   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,789   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,812   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,813   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,815   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,820   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,821   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:04:56,825   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:04:57,214   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:04:57,318   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14319.
2019-07-09 19:04:57,319   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14319
2019-07-09 19:04:57,322   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:04:57,360   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14319, None)
2019-07-09 19:04:57,367   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14319 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14319, None)
2019-07-09 19:04:57,372   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14319, None)
2019-07-09 19:04:57,373   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14319, None)
2019-07-09 19:04:57,698   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:04:58,778   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:04:58,938   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:04:58,938   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14319 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:04:58,954   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:04:59,267   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:37
2019-07-09 19:04:59,313   WARN --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:87) : Creating new stage failed due to exception - job: 0
java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:D:/data/word.txt
	at org.apache.hadoop.fs.Path.initialize(Path.java:206)
	at org.apache.hadoop.fs.Path.<init>(Path.java:172)
	at org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:244)
	at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:409)
	at org.apache.spark.SparkContext.$anonfun$hadoopFile$2(SparkContext.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$hadoopFile$2$adapted(SparkContext.scala:1036)
	at org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8(HadoopRDD.scala:180)
	at org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$8$adapted(HadoopRDD.scala:180)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.rdd.HadoopRDD.$anonfun$getJobConf$6(HadoopRDD.scala:180)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:177)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:94)
	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:87)
	at org.apache.spark.rdd.RDD.$anonfun$dependencies$2(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:238)
	at org.apache.spark.scheduler.DAGScheduler.getShuffleDependencies(DAGScheduler.scala:512)
	at org.apache.spark.scheduler.DAGScheduler.getMissingAncestorShuffleDependencies(DAGScheduler.scala:479)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:346)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:462)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:46)
	at scala.collection.SetLike.map(SetLike.scala:100)
	at scala.collection.SetLike.map$(SetLike.scala:100)
	at scala.collection.mutable.AbstractSet.map(Set.scala:46)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:461)
	at org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:448)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:962)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: file:D:/data/word.txt
	at java.net.URI.checkPath(URI.java:1823)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:203)
	... 51 more
2019-07-09 19:04:59,345   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 failed: collect at test.scala:37, took 0.083686 s
2019-07-09 19:04:59,376   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-07-09 19:04:59,392   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:04:59,392   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:04:59,407   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:04:59,438   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:04:59,454   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:04:59,454   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:04:59,470   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:04:59,485   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:04:59,485   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:04:59,501   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-f8326885-88d2-469e-baad-c9cfd543bcb2
2019-07-09 19:08:20,140   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:08:20,750   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:08:20,859   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:08:20,859   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:08:20,859   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:08:20,859   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:08:20,875   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:08:24,752   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14479.
2019-07-09 19:08:24,783   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:08:24,832   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:08:24,836   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:08:24,838   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:08:24,857   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-f3e1f794-500d-4884-a160-5a833fc3a7fd
2019-07-09 19:08:24,900   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:08:24,925   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:08:25,106   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17114ms
2019-07-09 19:08:25,195   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:08:25,216   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17225ms
2019-07-09 19:08:25,248   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:08:25,249   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:08:25,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,290   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,291   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,293   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,294   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,296   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,298   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,299   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,301   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,302   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,304   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,306   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,309   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,310   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,312   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,314   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,315   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,324   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,349   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,351   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,353   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,360   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,361   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:08:25,364   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:08:25,658   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:08:25,767   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14500.
2019-07-09 19:08:25,768   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14500
2019-07-09 19:08:25,772   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:08:25,812   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14500, None)
2019-07-09 19:08:25,819   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14500 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14500, None)
2019-07-09 19:08:25,823   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14500, None)
2019-07-09 19:08:25,824   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14500, None)
2019-07-09 19:08:26,101   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:08:27,175   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:08:27,316   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:08:27,316   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14500 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:08:27,332   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:08:27,644   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:37
2019-07-09 19:08:27,753   WARN --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:87) : Creating new stage failed due to exception - job: 0
java.lang.IllegalArgumentException: Wrong FS: file://D:/data/word.txt, expected: file:///
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:80)
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:752)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:252)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1644)
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:257)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:94)
	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:87)
	at org.apache.spark.rdd.RDD.$anonfun$dependencies$2(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:238)
	at org.apache.spark.scheduler.DAGScheduler.getShuffleDependencies(DAGScheduler.scala:512)
	at org.apache.spark.scheduler.DAGScheduler.getMissingAncestorShuffleDependencies(DAGScheduler.scala:479)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:346)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:462)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:46)
	at scala.collection.SetLike.map(SetLike.scala:100)
	at scala.collection.SetLike.map$(SetLike.scala:100)
	at scala.collection.mutable.AbstractSet.map(Set.scala:46)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:461)
	at org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:448)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:962)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2019-07-09 19:08:27,753   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 failed: collect at test.scala:37, took 0.112092 s
2019-07-09 19:08:27,785   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-07-09 19:08:27,800   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:08:27,800   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:08:27,835   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:08:27,985   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:08:27,988   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:08:28,006   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:08:28,031   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:08:28,047   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:08:28,049   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:08:28,052   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-8b603042-ee97-4aa2-a4f8-0cac7057568f
2019-07-09 19:09:19,487   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:09:20,108   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:09:20,232   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:09:20,236   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:09:20,237   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:09:20,238   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:09:20,240   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:09:24,068   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14536.
2019-07-09 19:09:24,139   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:09:24,171   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:09:24,176   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:09:24,177   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:09:24,197   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-96e375ca-7440-4944-aade-f0725b7ff8f1
2019-07-09 19:09:24,241   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:09:24,267   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:09:24,407   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16301ms
2019-07-09 19:09:24,495   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:09:24,514   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16410ms
2019-07-09 19:09:24,551   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:09:24,552   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:09:24,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,601   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,602   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,605   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,606   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,609   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,610   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,613   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,615   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,616   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,620   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,627   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,630   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,632   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,634   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,635   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,636   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,637   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,639   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,640   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,653   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,655   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,659   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,661   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,667   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:09:24,670   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:09:24,919   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:09:25,028   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14557.
2019-07-09 19:09:25,028   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14557
2019-07-09 19:09:25,028   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:09:25,060   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14557, None)
2019-07-09 19:09:25,060   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14557 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14557, None)
2019-07-09 19:09:25,075   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14557, None)
2019-07-09 19:09:25,075   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14557, None)
2019-07-09 19:09:25,310   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:09:26,216   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:09:26,341   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:09:26,341   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14557 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:09:26,356   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:09:26,685   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:37
2019-07-09 19:09:26,794   WARN --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:87) : Creating new stage failed due to exception - job: 0
java.lang.IllegalArgumentException: Wrong FS: file://\D:\data\word.txt/D:/home/code/bigdata/bigdata-learning/06spark, expected: file:///
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:80)
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:752)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:529)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:252)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1644)
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:257)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:253)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:94)
	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:87)
	at org.apache.spark.rdd.RDD.$anonfun$dependencies$2(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.rdd.RDD.dependencies(RDD.scala:238)
	at org.apache.spark.scheduler.DAGScheduler.getShuffleDependencies(DAGScheduler.scala:512)
	at org.apache.spark.scheduler.DAGScheduler.getMissingAncestorShuffleDependencies(DAGScheduler.scala:479)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateShuffleMapStage(DAGScheduler.scala:346)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$getOrCreateParentStages$1(DAGScheduler.scala:462)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at scala.collection.TraversableLike.map(TraversableLike.scala:233)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:226)
	at scala.collection.mutable.AbstractSet.scala$collection$SetLike$$super$map(Set.scala:46)
	at scala.collection.SetLike.map(SetLike.scala:100)
	at scala.collection.SetLike.map$(SetLike.scala:100)
	at scala.collection.mutable.AbstractSet.map(Set.scala:46)
	at org.apache.spark.scheduler.DAGScheduler.getOrCreateParentStages(DAGScheduler.scala:461)
	at org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:448)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:962)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2067)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
2019-07-09 19:09:26,810   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 failed: collect at test.scala:37, took 0.119260 s
2019-07-09 19:09:26,825   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-07-09 19:09:26,841   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:09:26,841   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:09:26,872   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:09:27,013   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:09:27,028   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:09:27,044   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:09:27,091   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:09:27,106   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:09:27,106   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:09:27,122   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-4944f112-c3f1-4a2d-99ba-6fcfba601d19
2019-07-09 19:10:26,519   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:10:27,166   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:10:27,280   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:10:27,283   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:10:27,285   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:10:27,286   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:10:27,287   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:10:31,138   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14587.
2019-07-09 19:10:31,178   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:10:31,212   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:10:31,217   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:10:31,218   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:10:31,237   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-e35ef251-78de-4ab0-b23d-78a28e7f9a6d
2019-07-09 19:10:31,278   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:10:31,302   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:10:31,441   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16336ms
2019-07-09 19:10:31,529   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:10:31,549   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16445ms
2019-07-09 19:10:31,581   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:10:31,582   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:10:31,622   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,624   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,625   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,627   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,629   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,630   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,631   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,633   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,634   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,635   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,637   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,638   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,640   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,642   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,643   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,644   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,646   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,647   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,648   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,664   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,666   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,669   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,670   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,674   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:10:31,678   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:10:31,946   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:10:32,055   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14608.
2019-07-09 19:10:32,055   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14608
2019-07-09 19:10:32,055   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:10:32,087   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14608, None)
2019-07-09 19:10:32,102   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14608 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14608, None)
2019-07-09 19:10:32,102   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14608, None)
2019-07-09 19:10:32,102   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14608, None)
2019-07-09 19:10:32,368   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:10:33,413   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:10:33,583   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:10:33,592   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14608 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:10:33,603   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:10:33,916   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:37
2019-07-09 19:10:34,041   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:10:37,633   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at test.scala:31)
2019-07-09 19:10:37,660   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at test.scala:31)
2019-07-09 19:10:37,664   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:37) with 1 output partitions
2019-07-09 19:10:37,665   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:37)
2019-07-09 19:10:37,666   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 19:10:37,670   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 19:10:37,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at test.scala:31), which has no missing parents
2019-07-09 19:10:37,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.6 KB, free 1426.3 MB)
2019-07-09 19:10:37,992   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 19:10:37,992   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:14608 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 19:10:37,992   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:10:38,071   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at test.scala:31) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:10:38,071   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:10:38,367   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7350 bytes)
2019-07-09 19:10:38,367   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7350 bytes)
2019-07-09 19:10:38,399   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:10:38,399   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:10:39,446   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:18+18
2019-07-09 19:10:39,446   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+18
2019-07-09 19:10:40,477   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 19:10:40,477   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 19:10:40,492   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2187 ms on localhost (executor driver) (1/2)
2019-07-09 19:10:40,524   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2157 ms on localhost (executor driver) (2/2)
2019-07-09 19:10:40,524   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:10:40,539   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:31) finished in 2.656 s
2019-07-09 19:10:40,539   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 19:10:40,539   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 19:10:40,539   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 19:10:40,539   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 19:10:40,555   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at test.scala:31), which has no missing parents
2019-07-09 19:10:40,586   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.1 KB, free 1426.3 MB)
2019-07-09 19:10:40,586   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 1426.3 MB)
2019-07-09 19:10:40,586   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:14608 (size: 3.0 KB, free: 1426.5 MB)
2019-07-09 19:10:40,602   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:10:40,602   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at test.scala:31) (first 15 tasks are for partitions Vector(0))
2019-07-09 19:10:40,602   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 19:10:40,602   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 19:10:40,602   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 19:10:40,664   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 19:10:40,664   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 19:10:41,039   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1195 bytes result sent to driver
2019-07-09 19:10:41,046   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 444 ms on localhost (executor driver) (1/1)
2019-07-09 19:10:41,047   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 19:10:41,052   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:31) finished in 0.481 s
2019-07-09 19:10:41,053   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 19:10:41,054   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 19:10:41,055   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 19:10:41,055   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 19:10:41,059   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at test.scala:31), which has no missing parents
2019-07-09 19:10:41,069   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 19:10:41,075   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 19:10:41,076   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:14608 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:10:41,078   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:10:41,080   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at test.scala:31) (first 15 tasks are for partitions Vector(0))
2019-07-09 19:10:41,081   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 19:10:41,086   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 19:10:41,087   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 19:10:41,101   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 19:10:41,102   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 19:10:41,141   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1353 bytes result sent to driver
2019-07-09 19:10:41,143   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 59 ms on localhost (executor driver) (1/1)
2019-07-09 19:10:41,143   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 19:10:41,145   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:37) finished in 0.079 s
2019-07-09 19:10:41,156   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:37, took 7.236598 s
2019-07-09 19:10:41,192   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:10:41,196   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:10:41,214   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:10:41,353   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:10:41,354   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:10:41,361   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:10:41,365   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:10:41,381   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:10:41,381   INFO --- [main]  WordCount$(line:46) : complete!
2019-07-09 19:10:41,386   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:10:41,387   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-69f8ee5e-554c-4623-90e6-c41ffebc9bec
2019-07-09 19:15:06,547   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:15:07,204   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:15:07,297   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:15:07,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:15:07,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:15:07,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:15:07,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:15:11,548   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14676.
2019-07-09 19:15:11,595   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:15:11,626   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:15:11,626   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:15:11,626   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:15:11,642   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-b090761f-9c9d-4ad6-9c9f-9f5578ec48b7
2019-07-09 19:15:11,688   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:15:11,704   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:15:11,845   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16752ms
2019-07-09 19:15:11,931   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:15:11,951   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16859ms
2019-07-09 19:15:11,982   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:15:11,982   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:15:12,025   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,026   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,028   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,030   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,031   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,033   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,034   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,037   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,038   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,040   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,044   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,046   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,048   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,049   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,050   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,051   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,053   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,055   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,057   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,071   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,073   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,076   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,081   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,083   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:15:12,088   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:15:12,375   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:15:12,492   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14697.
2019-07-09 19:15:12,493   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14697
2019-07-09 19:15:12,496   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:15:12,531   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14697, None)
2019-07-09 19:15:12,538   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14697 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14697, None)
2019-07-09 19:15:12,542   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14697, None)
2019-07-09 19:15:12,543   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14697, None)
2019-07-09 19:15:12,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:15:14,096   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:15:14,252   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:15:14,268   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14697 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:15:14,268   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:15:14,549   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:41
2019-07-09 19:15:14,658   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:15:15,116   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at test.scala:33)
2019-07-09 19:15:15,120   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at test.scala:35)
2019-07-09 19:15:15,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:41) with 1 output partitions
2019-07-09 19:15:15,128   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:41)
2019-07-09 19:15:15,130   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 19:15:15,136   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 19:15:15,180   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at test.scala:33), which has no missing parents
2019-07-09 19:15:15,296   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.6 KB, free 1426.3 MB)
2019-07-09 19:15:15,326   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 19:15:15,333   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:14697 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 19:15:15,335   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:15:15,366   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at test.scala:33) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:15:15,368   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:15:15,447   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7350 bytes)
2019-07-09 19:15:15,454   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7350 bytes)
2019-07-09 19:15:15,472   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:15:15,472   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:15:16,317   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:15:16,317   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:15:16,608   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
2019-07-09 19:15:16,609   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1152 bytes result sent to driver
2019-07-09 19:15:16,628   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1172 ms on localhost (executor driver) (1/2)
2019-07-09 19:15:16,633   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1208 ms on localhost (executor driver) (2/2)
2019-07-09 19:15:16,635   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:15:16,652   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:33) finished in 1.391 s
2019-07-09 19:15:16,654   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 19:15:16,655   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 19:15:16,656   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 19:15:16,657   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 19:15:16,665   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at test.scala:35), which has no missing parents
2019-07-09 19:15:16,688   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.1 KB, free 1426.3 MB)
2019-07-09 19:15:16,694   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 1426.3 MB)
2019-07-09 19:15:16,700   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:14697 (size: 3.0 KB, free: 1426.5 MB)
2019-07-09 19:15:16,702   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:15:16,707   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at test.scala:35) (first 15 tasks are for partitions Vector(0))
2019-07-09 19:15:16,708   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 19:15:16,716   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 19:15:16,717   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 19:15:16,777   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 19:15:16,780   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 12 ms
2019-07-09 19:15:16,929   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 19:15:16,929   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 215 ms on localhost (executor driver) (1/1)
2019-07-09 19:15:16,929   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 19:15:16,929   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:35) finished in 0.247 s
2019-07-09 19:15:16,929   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 19:15:16,929   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 19:15:16,929   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 19:15:16,929   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 19:15:16,945   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at test.scala:35), which has no missing parents
2019-07-09 19:15:16,945   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 19:15:16,945   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 19:15:16,961   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:14697 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:15:16,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:15:16,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at test.scala:35) (first 15 tasks are for partitions Vector(0))
2019-07-09 19:15:16,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 19:15:16,961   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 19:15:16,961   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 19:15:16,976   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 19:15:16,976   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 19:15:17,023   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 7280 bytes result sent to driver
2019-07-09 19:15:17,023   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 62 ms on localhost (executor driver) (1/1)
2019-07-09 19:15:17,023   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 19:15:17,023   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:41) finished in 0.078 s
2019-07-09 19:15:17,039   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:41, took 2.487109 s
2019-07-09 19:15:17,086   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:15:17,086   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:15:17,117   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:15:17,257   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:15:17,257   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:15:17,273   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:15:17,273   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:15:17,289   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:15:17,289   INFO --- [main]  WordCount$(line:50) : complete!
2019-07-09 19:15:17,289   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:15:17,289   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-ba32953e-f42a-4875-8c3a-9a9f565a60ba
2019-07-09 19:16:56,347   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:16:56,991   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:16:57,118   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:16:57,124   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:16:57,126   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:16:57,127   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:16:57,128   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:17:00,973   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14735.
2019-07-09 19:17:01,013   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:17:01,045   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:17:01,050   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:17:01,051   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:17:01,069   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-4df3df16-7444-4acd-b1a6-dc0f066a9188
2019-07-09 19:17:01,111   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:17:01,135   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:17:01,274   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16330ms
2019-07-09 19:17:01,368   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:17:01,387   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16446ms
2019-07-09 19:17:01,425   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:17:01,425   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:17:01,474   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,475   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,476   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,483   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,486   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,488   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,490   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,493   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,497   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,501   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,505   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,507   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,509   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,512   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,513   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,517   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,519   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,521   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,523   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,534   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,535   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,537   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,539   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,543   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:17:01,546   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:17:01,795   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:17:01,891   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14756.
2019-07-09 19:17:01,891   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14756
2019-07-09 19:17:01,891   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:17:01,922   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14756, None)
2019-07-09 19:17:01,938   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14756 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14756, None)
2019-07-09 19:17:01,938   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14756, None)
2019-07-09 19:17:01,938   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14756, None)
2019-07-09 19:17:02,188   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:17:03,144   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:17:03,308   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:17:03,316   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14756 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:17:03,324   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:17:03,678   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:41
2019-07-09 19:17:03,785   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:17:04,268   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at test.scala:33)
2019-07-09 19:17:04,272   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at test.scala:35)
2019-07-09 19:17:04,279   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:41) with 1 output partitions
2019-07-09 19:17:04,281   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:41)
2019-07-09 19:17:04,282   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 19:17:04,287   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 19:17:04,308   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at test.scala:33), which has no missing parents
2019-07-09 19:17:04,449   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.6 KB, free 1426.3 MB)
2019-07-09 19:17:04,461   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 19:17:04,463   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:14756 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 19:17:04,466   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:17:04,542   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at test.scala:33) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:17:04,547   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:17:04,644   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 19:17:04,651   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 19:17:04,669   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:17:04,669   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:17:05,438   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 19:17:05,438   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 19:17:06,621   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
2019-07-09 19:17:06,621   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1152 bytes result sent to driver
2019-07-09 19:17:06,639   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2022 ms on localhost (executor driver) (1/2)
2019-07-09 19:17:06,646   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1997 ms on localhost (executor driver) (2/2)
2019-07-09 19:17:06,648   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:17:06,666   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:33) finished in 2.255 s
2019-07-09 19:17:06,668   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 19:17:06,669   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 19:17:06,670   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 19:17:06,671   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 19:17:06,679   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at test.scala:35), which has no missing parents
2019-07-09 19:17:06,697   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.1 KB, free 1426.3 MB)
2019-07-09 19:17:06,702   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 1426.3 MB)
2019-07-09 19:17:06,705   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:14756 (size: 3.0 KB, free: 1426.5 MB)
2019-07-09 19:17:06,706   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:17:06,710   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at test.scala:35) (first 15 tasks are for partitions Vector(0))
2019-07-09 19:17:06,711   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 19:17:06,717   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 19:17:06,718   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 19:17:06,763   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 19:17:06,767   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 17 ms
2019-07-09 19:17:07,816   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1238 bytes result sent to driver
2019-07-09 19:17:07,816   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 1100 ms on localhost (executor driver) (1/1)
2019-07-09 19:17:07,816   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:35) finished in 1.126 s
2019-07-09 19:17:07,816   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 19:17:07,816   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 19:17:07,816   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 19:17:07,816   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 19:17:07,816   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 19:17:07,816   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at test.scala:35), which has no missing parents
2019-07-09 19:17:07,832   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 19:17:07,832   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 19:17:07,832   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:14756 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:17:07,832   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:17:07,847   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at test.scala:35) (first 15 tasks are for partitions Vector(0))
2019-07-09 19:17:07,847   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 19:17:07,847   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 19:17:07,847   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 19:17:07,863   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 19:17:07,863   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 19:17:08,004   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:14756 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 19:17:08,050   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:14756 in memory (size: 3.0 KB, free: 1426.5 MB)
2019-07-09 19:17:08,566   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_3 stored as bytes in memory (estimated size 4.9 MB, free 1421.4 MB)
2019-07-09 19:17:08,597   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_3 in memory on fc-pc:14756 (size: 4.9 MB, free: 1421.6 MB)
2019-07-09 19:17:08,597   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 5101635 bytes result sent via BlockManager)
2019-07-09 19:17:09,707   INFO --- [task-result-getter-3]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:14756 after 154 ms (0 ms spent in bootstraps)
2019-07-09 19:17:10,182   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 2335 ms on localhost (executor driver) (1/1)
2019-07-09 19:17:10,183   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 19:17:10,186   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:41) finished in 2.354 s
2019-07-09 19:17:10,191   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_3 on fc-pc:14756 in memory (size: 4.9 MB, free: 1426.5 MB)
2019-07-09 19:17:10,201   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:41, took 6.521936 s
2019-07-09 19:17:11,126   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:17:11,126   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:17:11,141   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:17:11,282   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:17:11,282   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:17:11,298   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:17:11,298   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:17:11,313   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:17:11,313   INFO --- [main]  WordCount$(line:50) : complete!
2019-07-09 19:17:11,313   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:17:11,313   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-dd425eda-51e9-4927-8f17-5b422d1a1d4c
2019-07-09 19:20:52,594   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:20:53,223   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:20:53,317   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:20:53,317   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:20:53,317   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:20:53,317   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:20:53,317   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:20:57,161   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14811.
2019-07-09 19:20:57,200   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:20:57,233   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:20:57,237   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:20:57,238   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:20:57,259   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-76d3370d-b00e-4ec2-baca-e15944f5e680
2019-07-09 19:20:57,300   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:20:57,324   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:20:57,466   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16887ms
2019-07-09 19:20:57,557   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:20:57,577   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16999ms
2019-07-09 19:20:57,611   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@590aa090{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:20:57,613   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:20:57,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,651   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,652   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,654   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,655   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,656   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,657   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,659   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,661   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,661   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,663   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,664   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,666   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,667   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,669   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,670   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,671   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,672   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,673   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,675   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,686   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,690   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,697   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,699   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:20:57,703   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:20:57,958   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:20:58,052   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14832.
2019-07-09 19:20:58,067   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14832
2019-07-09 19:20:58,067   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:20:58,098   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14832, None)
2019-07-09 19:20:58,114   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14832 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14832, None)
2019-07-09 19:20:58,114   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14832, None)
2019-07-09 19:20:58,114   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14832, None)
2019-07-09 19:20:58,380   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:20:59,302   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:20:59,427   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:20:59,427   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14832 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:20:59,442   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:20:59,661   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:20:59,770   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:41
2019-07-09 19:20:59,812   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:41) with 2 output partitions
2019-07-09 19:20:59,813   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:41)
2019-07-09 19:20:59,814   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:20:59,818   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:20:59,831   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[3] at map at test.scala:33), which has no missing parents
2019-07-09 19:20:59,933   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 1426.3 MB)
2019-07-09 19:20:59,949   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 19:20:59,960   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:14832 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:20:59,963   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:21:00,015   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at test.scala:33) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:21:00,018   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:21:00,164   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:21:00,185   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:21:00,220   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:21:00,220   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:21:01,058   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 19:21:01,068   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 19:21:01,763   INFO --- [Executor task launch worker for task 0]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_0 stored as bytes in memory (estimated size 3.3 MB, free 1419.7 MB)
2019-07-09 19:21:01,763   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_0 in memory on fc-pc:14832 (size: 3.3 MB, free: 1423.2 MB)
2019-07-09 19:21:01,763   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 3430191 bytes result sent via BlockManager)
2019-07-09 19:21:01,763   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_1 stored as bytes in memory (estimated size 3.3 MB, free 1419.7 MB)
2019-07-09 19:21:01,779   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_1 in memory on fc-pc:14832 (size: 3.3 MB, free: 1419.9 MB)
2019-07-09 19:21:01,779   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3427510 bytes result sent via BlockManager)
2019-07-09 19:21:01,888   INFO --- [task-result-getter-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:14832 after 48 ms (0 ms spent in bootstraps)
2019-07-09 19:21:02,357   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2245 ms on localhost (executor driver) (1/2)
2019-07-09 19:21:02,372   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2199 ms on localhost (executor driver) (2/2)
2019-07-09 19:21:02,372   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:21:02,403   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_0 on fc-pc:14832 in memory (size: 3.3 MB, free: 1423.2 MB)
2019-07-09 19:21:02,403   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_1 on fc-pc:14832 in memory (size: 3.3 MB, free: 1426.5 MB)
2019-07-09 19:21:02,403   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:41) finished in 2.490 s
2019-07-09 19:21:02,435   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:41, took 2.663601 s
2019-07-09 19:21:02,747   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:14832 in memory (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:21:05,133   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@590aa090{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:21:05,190   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:21:05,243   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:21:05,332   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:21:05,333   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:21:05,337   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:21:05,376   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:21:05,410   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:21:05,410   INFO --- [main]  WordCount$(line:50) : complete!
2019-07-09 19:21:05,417   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:21:05,419   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-5d68e91b-9472-4a55-86d4-918fd68e5325
2019-07-09 19:21:45,233   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:21:45,874   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:21:45,983   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:21:45,983   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:21:45,983   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:21:45,983   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:21:45,983   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:21:50,013   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14866.
2019-07-09 19:21:50,052   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:21:50,082   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:21:50,086   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:21:50,087   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:21:50,105   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-6888a3a6-e374-44e7-9a52-21abfce50035
2019-07-09 19:21:50,145   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:21:50,168   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:21:50,322   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16411ms
2019-07-09 19:21:50,412   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:21:50,431   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16523ms
2019-07-09 19:21:50,497   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:21:50,498   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:21:50,558   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,560   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,562   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,564   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,566   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,568   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,570   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,574   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,575   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,577   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,579   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,586   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,589   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,590   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,594   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,595   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,598   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,602   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,617   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,618   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,622   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,625   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,630   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:21:50,633   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:21:50,904   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:21:51,022   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14887.
2019-07-09 19:21:51,023   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14887
2019-07-09 19:21:51,026   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:21:51,063   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14887, None)
2019-07-09 19:21:51,070   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14887 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14887, None)
2019-07-09 19:21:51,074   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14887, None)
2019-07-09 19:21:51,075   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14887, None)
2019-07-09 19:21:51,416   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:21:52,500   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:21:52,672   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:21:52,687   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14887 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:21:52,687   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:21:52,875   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:21:53,000   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:41
2019-07-09 19:21:53,047   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:41) with 2 output partitions
2019-07-09 19:21:53,047   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:41)
2019-07-09 19:21:53,047   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:21:53,062   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:21:53,078   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMap at test.scala:32), which has no missing parents
2019-07-09 19:21:53,140   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 1426.3 MB)
2019-07-09 19:21:53,156   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1426.3 MB)
2019-07-09 19:21:53,172   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:14887 (size: 2.4 KB, free: 1426.5 MB)
2019-07-09 19:21:53,172   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:21:53,203   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMap at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:21:53,218   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:21:53,328   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:21:53,343   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:21:53,359   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:21:53,359   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:21:54,140   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 19:21:54,140   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 19:21:54,640   INFO --- [Executor task launch worker for task 0]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_0 stored as bytes in memory (estimated size 2.5 MB, free 1423.7 MB)
2019-07-09 19:21:54,640   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_0 in memory on fc-pc:14887 (size: 2.5 MB, free: 1423.9 MB)
2019-07-09 19:21:54,640   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_1 stored as bytes in memory (estimated size 2.5 MB, free 1421.2 MB)
2019-07-09 19:21:54,656   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 2655898 bytes result sent via BlockManager)
2019-07-09 19:21:54,656   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_1 in memory on fc-pc:14887 (size: 2.5 MB, free: 1421.4 MB)
2019-07-09 19:21:54,656   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 2655291 bytes result sent via BlockManager)
2019-07-09 19:21:54,765   INFO --- [task-result-getter-1]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:14887 after 65 ms (0 ms spent in bootstraps)
2019-07-09 19:21:55,042   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1743 ms on localhost (executor driver) (1/2)
2019-07-09 19:21:55,050   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1707 ms on localhost (executor driver) (2/2)
2019-07-09 19:21:55,052   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_0 on fc-pc:14887 in memory (size: 2.5 MB, free: 1423.9 MB)
2019-07-09 19:21:55,054   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:21:55,058   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_1 on fc-pc:14887 in memory (size: 2.5 MB, free: 1426.5 MB)
2019-07-09 19:21:55,068   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:41) finished in 1.922 s
2019-07-09 19:21:55,083   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:41, took 2.072204 s
2019-07-09 19:21:56,640   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:21:56,640   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 18
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 23
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 21
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 7
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 10
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 6
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 11
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 3
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 1
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 4
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 0
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 20
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 13
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 9
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 8
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 5
2019-07-09 19:21:56,656   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 12
2019-07-09 19:21:56,656   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:21:56,671   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:14887 in memory (size: 2.4 KB, free: 1426.5 MB)
2019-07-09 19:21:56,702   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:21:56,781   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:21:56,781   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:21:56,781   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:21:56,796   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:21:56,812   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:21:56,812   INFO --- [main]  WordCount$(line:50) : complete!
2019-07-09 19:21:56,812   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:21:56,812   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-ae362260-50cd-4c8c-96fc-5ab5dc7dfcf8
2019-07-09 19:25:50,549   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:25:51,158   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:25:51,267   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:25:51,267   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:25:51,267   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:25:51,267   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:25:51,267   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:25:55,189   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14943.
2019-07-09 19:25:55,220   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:25:55,251   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:25:55,267   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:25:55,267   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:25:55,282   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-7df45dde-b7f4-47f9-9f5a-6c3200c15a6c
2019-07-09 19:25:55,314   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:25:55,345   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:25:55,486   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16368ms
2019-07-09 19:25:55,564   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:25:55,595   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16476ms
2019-07-09 19:25:55,626   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:25:55,626   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:25:55,673   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,673   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,673   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,673   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,704   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,704   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,704   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,704   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,704   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,736   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,736   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,736   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,736   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,736   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:25:55,736   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:25:56,016   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:25:56,119   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 14964.
2019-07-09 19:25:56,120   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:14964
2019-07-09 19:25:56,122   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:25:56,158   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 14964, None)
2019-07-09 19:25:56,165   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:14964 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 14964, None)
2019-07-09 19:25:56,170   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 14964, None)
2019-07-09 19:25:56,171   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 14964, None)
2019-07-09 19:25:56,453   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:25:57,558   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:25:57,699   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:25:57,699   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:14964 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:25:57,715   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:25:57,902   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:25:58,012   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:40
2019-07-09 19:25:58,058   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:40) with 2 output partitions
2019-07-09 19:25:58,058   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:40)
2019-07-09 19:25:58,058   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:25:58,058   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:25:58,074   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at map at test.scala:32), which has no missing parents
2019-07-09 19:25:58,168   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 1426.3 MB)
2019-07-09 19:25:58,168   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1426.3 MB)
2019-07-09 19:25:58,183   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:14964 (size: 2.4 KB, free: 1426.5 MB)
2019-07-09 19:25:58,183   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:25:58,230   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:25:58,230   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:25:58,340   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:25:58,355   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:25:58,386   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:25:58,386   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:25:59,215   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 19:25:59,215   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 19:25:59,788   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_1 stored as bytes in memory (estimated size 2.6 MB, free 1423.7 MB)
2019-07-09 19:25:59,795   INFO --- [Executor task launch worker for task 0]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_0 stored as bytes in memory (estimated size 2.6 MB, free 1421.0 MB)
2019-07-09 19:25:59,799   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_1 in memory on fc-pc:14964 (size: 2.6 MB, free: 1423.9 MB)
2019-07-09 19:25:59,801   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_0 in memory on fc-pc:14964 (size: 2.6 MB, free: 1421.2 MB)
2019-07-09 19:25:59,803   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 2740857 bytes result sent via BlockManager)
2019-07-09 19:25:59,803   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 2741221 bytes result sent via BlockManager)
2019-07-09 19:25:59,903   INFO --- [task-result-getter-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:14964 after 45 ms (0 ms spent in bootstraps)
2019-07-09 19:26:00,201   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1891 ms on localhost (executor driver) (1/2)
2019-07-09 19:26:00,216   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_0 on fc-pc:14964 in memory (size: 2.6 MB, free: 1423.9 MB)
2019-07-09 19:26:00,228   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_1 on fc-pc:14964 in memory (size: 2.6 MB, free: 1426.5 MB)
2019-07-09 19:26:00,234   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1879 ms on localhost (executor driver) (2/2)
2019-07-09 19:26:00,249   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:40) finished in 2.110 s
2019-07-09 19:26:00,267   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:40, took 2.249335 s
2019-07-09 19:26:00,280   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:26:00,610   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:26:00,626   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:26:00,641   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:26:00,735   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:26:00,735   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:26:00,735   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:26:00,751   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:26:00,798   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:26:00,798   INFO --- [main]  WordCount$(line:49) : complete!
2019-07-09 19:26:00,798   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:26:00,813   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-abe3cf99-24b5-432b-8ae2-bcf264f5427f
2019-07-09 19:27:12,725   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:27:13,379   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:27:13,507   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:27:13,511   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:27:13,513   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:27:13,514   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:27:13,515   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:27:17,346   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 14998.
2019-07-09 19:27:17,389   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:27:17,422   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:27:17,426   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:27:17,428   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:27:17,447   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-340fa26a-be59-4efd-b82c-e74b81ccd60d
2019-07-09 19:27:17,497   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:27:17,523   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:27:17,665   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16371ms
2019-07-09 19:27:17,754   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:27:17,774   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16481ms
2019-07-09 19:27:17,797   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:27:17,797   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:27:17,829   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,829   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,829   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,829   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,860   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,860   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,860   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,860   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,860   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,876   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,876   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:27:17,876   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:27:18,157   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:27:18,266   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1043.
2019-07-09 19:27:18,266   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1043
2019-07-09 19:27:18,266   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:27:18,297   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1043, None)
2019-07-09 19:27:18,313   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1043 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1043, None)
2019-07-09 19:27:18,313   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1043, None)
2019-07-09 19:27:18,313   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1043, None)
2019-07-09 19:27:18,579   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:27:19,567   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:27:19,696   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:27:19,703   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1043 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:27:19,711   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:27:19,930   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:27:20,051   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:40
2019-07-09 19:27:20,092   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:40) with 2 output partitions
2019-07-09 19:27:20,093   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:40)
2019-07-09 19:27:20,094   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:27:20,097   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:27:20,125   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMap at test.scala:32), which has no missing parents
2019-07-09 19:27:20,238   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 1426.3 MB)
2019-07-09 19:27:20,254   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1426.3 MB)
2019-07-09 19:27:20,258   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1043 (size: 2.4 KB, free: 1426.5 MB)
2019-07-09 19:27:20,263   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:27:20,303   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMap at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:27:20,305   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:27:20,418   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:27:20,426   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:27:20,501   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:27:20,534   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:27:21,241   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 19:27:21,241   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 19:27:21,726   INFO --- [Executor task launch worker for task 0]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_0 stored as bytes in memory (estimated size 2.5 MB, free 1423.7 MB)
2019-07-09 19:27:21,726   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_1 stored as bytes in memory (estimated size 2.5 MB, free 1421.2 MB)
2019-07-09 19:27:21,741   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_0 in memory on fc-pc:1043 (size: 2.5 MB, free: 1423.9 MB)
2019-07-09 19:27:21,741   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_1 in memory on fc-pc:1043 (size: 2.5 MB, free: 1421.4 MB)
2019-07-09 19:27:21,741   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 2655291 bytes result sent via BlockManager)
2019-07-09 19:27:21,741   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 2655898 bytes result sent via BlockManager)
2019-07-09 19:27:21,846   INFO --- [task-result-getter-1]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:1043 after 51 ms (0 ms spent in bootstraps)
2019-07-09 19:27:22,135   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1707 ms on localhost (executor driver) (1/2)
2019-07-09 19:27:22,155   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1769 ms on localhost (executor driver) (2/2)
2019-07-09 19:27:22,164   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:40) finished in 1.939 s
2019-07-09 19:27:22,191   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:27:22,202   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_0 on fc-pc:1043 in memory (size: 2.5 MB, free: 1423.9 MB)
2019-07-09 19:27:22,204   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_1 on fc-pc:1043 in memory (size: 2.5 MB, free: 1426.5 MB)
2019-07-09 19:27:22,208   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:40, took 2.155722 s
2019-07-09 19:27:23,625   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:27:23,628   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:27:23,689   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:27:23,788   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:27:23,789   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:27:23,792   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:27:23,797   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:27:23,820   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:27:23,822   INFO --- [main]  WordCount$(line:49) : complete!
2019-07-09 19:27:23,829   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:27:23,829   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-c62540a7-dd8f-4ad1-a7c4-9ad1dc3adcc5
2019-07-09 19:28:41,095   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:28:41,704   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:28:41,828   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:28:41,832   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:28:41,834   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:28:41,835   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:28:41,836   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:28:45,749   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1076.
2019-07-09 19:28:45,788   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:28:45,822   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:28:45,827   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:28:45,828   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:28:45,847   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-0f5c77c1-8c36-42d9-a81b-73e21a46190d
2019-07-09 19:28:45,887   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:28:45,911   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:28:46,066   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16454ms
2019-07-09 19:28:46,156   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:28:46,176   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16565ms
2019-07-09 19:28:46,214   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@63465256{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:28:46,215   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:28:46,263   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@71b3bc45{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,264   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@76f7d241{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4a335fa8{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,268   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,269   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,270   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,273   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,275   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@44828f6b{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,277   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,279   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,285   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,290   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,292   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,294   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,297   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,299   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,301   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,313   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/static,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,315   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@78fb9a67{/,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,317   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@73ff4fae{/api,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2611b9a3{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,323   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54227100{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:28:46,326   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:28:46,600   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:28:46,702   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1097.
2019-07-09 19:28:46,703   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1097
2019-07-09 19:28:46,705   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:28:46,745   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1097, None)
2019-07-09 19:28:46,752   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1097 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1097, None)
2019-07-09 19:28:46,756   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1097, None)
2019-07-09 19:28:46,757   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1097, None)
2019-07-09 19:28:47,001   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6548bb7d{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:28:48,048   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:28:48,204   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:28:48,204   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1097 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:28:48,220   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:28:48,407   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:28:48,532   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:40
2019-07-09 19:28:48,579   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:40) with 2 output partitions
2019-07-09 19:28:48,579   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:40)
2019-07-09 19:28:48,579   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:28:48,579   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:28:48,595   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[3] at flatMap at test.scala:32), which has no missing parents
2019-07-09 19:28:48,704   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 1426.3 MB)
2019-07-09 19:28:48,720   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 19:28:48,720   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1097 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:28:48,735   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:28:48,782   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at flatMap at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:28:48,782   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:28:48,907   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:28:48,938   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:28:48,954   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:28:48,954   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:28:49,773   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 19:28:49,773   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 19:28:50,343   INFO --- [Executor task launch worker for task 0]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_0 stored as bytes in memory (estimated size 2.6 MB, free 1423.7 MB)
2019-07-09 19:28:50,344   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_1 stored as bytes in memory (estimated size 2.6 MB, free 1421.0 MB)
2019-07-09 19:28:50,350   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_0 in memory on fc-pc:1097 (size: 2.6 MB, free: 1423.9 MB)
2019-07-09 19:28:50,354   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 2734037 bytes result sent via BlockManager)
2019-07-09 19:28:50,377   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_1 in memory on fc-pc:1097 (size: 2.6 MB, free: 1421.3 MB)
2019-07-09 19:28:50,378   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 2733267 bytes result sent via BlockManager)
2019-07-09 19:28:50,527   INFO --- [task-result-getter-1]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:1097 after 96 ms (0 ms spent in bootstraps)
2019-07-09 19:28:51,116   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2256 ms on localhost (executor driver) (1/2)
2019-07-09 19:28:51,131   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_0 on fc-pc:1097 in memory (size: 2.6 MB, free: 1423.9 MB)
2019-07-09 19:28:51,131   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2193 ms on localhost (executor driver) (2/2)
2019-07-09 19:28:51,131   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:28:51,147   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:40) finished in 2.459 s
2019-07-09 19:28:51,147   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_1 on fc-pc:1097 in memory (size: 2.6 MB, free: 1426.5 MB)
2019-07-09 19:28:51,163   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:40, took 2.635440 s
2019-07-09 19:28:54,130   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@63465256{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:28:54,130   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:28:54,161   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:28:54,255   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:28:54,255   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:28:54,255   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:28:54,255   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:28:54,270   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:28:54,270   INFO --- [main]  WordCount$(line:49) : complete!
2019-07-09 19:28:54,270   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:28:54,286   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-c5aa65db-a972-4242-b4f5-2bd6c3c3ab90
2019-07-09 19:31:24,579   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:31:25,344   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:31:25,485   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:31:25,485   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:31:25,500   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:31:25,500   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:31:25,500   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:31:29,606   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1139.
2019-07-09 19:31:29,652   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:31:29,697   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:31:29,704   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:31:29,707   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:31:29,735   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-99b45e84-00f7-4cf9-8319-fe38e90cc91f
2019-07-09 19:31:29,807   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:31:29,859   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:31:30,049   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @19173ms
2019-07-09 19:31:30,178   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:31:30,204   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @19329ms
2019-07-09 19:31:30,265   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1abc9f14{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:31:30,266   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:31:30,334   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5136207f{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@ba354ca{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,337   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4c4f4365{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,340   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7ce7e83c{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,342   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4a05d8ae{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,343   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c904f1e{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,345   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4eb30d44{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,347   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3910fe11{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,349   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@460510aa{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,350   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@351e414e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,351   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6fd77352{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,354   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5109e8cf{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,355   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f672204{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,357   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@78b41097{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,359   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c2db130{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,361   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@327c7bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,365   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@348d18a3{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,367   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6c65860d{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,368   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d000e80{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,371   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7cf283e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,384   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@20e6c4dc{/static,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,389   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7a5b769b{/,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,391   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f4c0e4e{/api,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,393   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7a7d1b47{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6eb82908{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:31:30,398   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:31:30,704   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:31:30,844   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1160.
2019-07-09 19:31:30,844   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1160
2019-07-09 19:31:30,844   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:31:30,891   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1160, None)
2019-07-09 19:31:30,907   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1160 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1160, None)
2019-07-09 19:31:30,907   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1160, None)
2019-07-09 19:31:30,907   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1160, None)
2019-07-09 19:31:31,219   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4e1a46fb{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:31:32,438   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:31:32,610   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:31:32,626   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1160 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:31:32,626   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:32:56,546   WARN --- [driver-heartbeater]  org.apache.spark.executor.Executor(line:87) : Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:841)
	at org.apache.spark.executor.Executor$$anon$2.$anonfun$run$24(Executor.scala:870)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:870)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:255)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:259)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:220)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 13 more
2019-07-09 19:33:14,829   WARN --- [driver-heartbeater]  org.apache.spark.executor.Executor(line:87) : Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:841)
	at org.apache.spark.executor.Executor$$anon$2.$anonfun$run$24(Executor.scala:870)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:870)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:255)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:259)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:220)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 13 more
2019-07-09 19:33:41,172   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:33:41,829   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:33:41,938   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:33:41,938   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:33:41,938   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:33:41,938   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:33:41,938   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:33:45,956   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1198.
2019-07-09 19:33:45,999   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:33:46,032   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:33:46,037   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:33:46,038   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:33:46,057   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-55d4e1e3-f52b-447c-856e-e47f3f789f22
2019-07-09 19:33:46,100   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:33:46,126   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:33:46,276   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16519ms
2019-07-09 19:33:46,369   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:33:46,389   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16633ms
2019-07-09 19:33:46,413   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-07-09 19:33:46,426   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@7894f09b{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2019-07-09 19:33:46,427   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4041.
2019-07-09 19:33:46,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3a62c01e{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,482   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,485   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,494   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,497   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,499   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@44828f6b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,501   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,506   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,508   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,509   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,512   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,520   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,522   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,524   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,526   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,527   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,528   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,530   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,531   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,533   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38be305c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,545   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@269f4bad{/static,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,547   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2611b9a3{/,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,555   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54227100{/api,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,556   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7829b776{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,558   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5778826f{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:33:46,561   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4041
2019-07-09 19:33:46,848   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:33:46,953   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1219.
2019-07-09 19:33:46,955   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1219
2019-07-09 19:33:46,958   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:33:46,995   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1219, None)
2019-07-09 19:33:47,003   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1219 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1219, None)
2019-07-09 19:33:47,007   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1219, None)
2019-07-09 19:33:47,008   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1219, None)
2019-07-09 19:33:47,297   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62577d6{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:33:48,219   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:33:48,344   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:33:48,344   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1219 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:33:48,360   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:33:48,594   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:33:48,704   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:40
2019-07-09 19:33:48,751   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:40) with 2 output partitions
2019-07-09 19:33:48,766   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:40)
2019-07-09 19:33:48,766   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:33:48,766   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:33:48,798   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[3] at flatMap at test.scala:32), which has no missing parents
2019-07-09 19:33:48,985   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 1426.3 MB)
2019-07-09 19:33:49,026   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 19:33:49,029   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1219 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:33:49,031   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:33:49,075   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at flatMap at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:33:49,077   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:33:49,200   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:33:49,220   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:33:49,252   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:33:49,252   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:33:50,053   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 19:33:50,053   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 19:33:50,584   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_1 stored as bytes in memory (estimated size 2.6 MB, free 1423.7 MB)
2019-07-09 19:33:50,584   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_1 in memory on fc-pc:1219 (size: 2.6 MB, free: 1423.9 MB)
2019-07-09 19:33:50,584   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 2733267 bytes result sent via BlockManager)
2019-07-09 19:33:50,599   INFO --- [Executor task launch worker for task 0]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_0 stored as bytes in memory (estimated size 2.6 MB, free 1421.0 MB)
2019-07-09 19:33:50,615   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_0 in memory on fc-pc:1219 (size: 2.6 MB, free: 1421.3 MB)
2019-07-09 19:33:50,615   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 2734037 bytes result sent via BlockManager)
2019-07-09 19:33:50,709   INFO --- [task-result-getter-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:1219 after 55 ms (0 ms spent in bootstraps)
2019-07-09 19:33:51,209   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2040 ms on localhost (executor driver) (1/2)
2019-07-09 19:33:51,224   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2005 ms on localhost (executor driver) (2/2)
2019-07-09 19:33:51,224   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:33:51,240   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:40) finished in 2.255 s
2019-07-09 19:33:51,240   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_0 on fc-pc:1219 in memory (size: 2.6 MB, free: 1423.9 MB)
2019-07-09 19:33:51,240   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_1 on fc-pc:1219 in memory (size: 2.6 MB, free: 1426.5 MB)
2019-07-09 19:33:51,256   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:40, took 2.551384 s
2019-07-09 19:33:53,820   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@7894f09b{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2019-07-09 19:33:53,823   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4041
2019-07-09 19:33:53,846   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:33:53,945   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:33:53,946   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:33:53,948   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:33:53,953   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:33:53,970   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:33:53,971   INFO --- [main]  WordCount$(line:49) : complete!
2019-07-09 19:33:53,976   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:33:53,977   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-6fd4242f-8a59-4dc2-8578-c199ced46683
2019-07-09 19:34:56,914   WARN --- [driver-heartbeater]  org.apache.spark.executor.Executor(line:87) : Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:841)
	at org.apache.spark.executor.Executor$$anon$2.$anonfun$run$24(Executor.scala:870)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:870)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:255)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:259)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:220)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 13 more
2019-07-09 19:34:57,774   WARN --- [heartbeat-receiver-event-loop-thread]  org.apache.spark.rpc.netty.NettyRpcEnv(line:66) : Ignored message: HeartbeatResponse(false)
2019-07-09 19:34:57,774   WARN --- [heartbeat-receiver-event-loop-thread]  org.apache.spark.rpc.netty.NettyRpcEnv(line:66) : Ignored message: HeartbeatResponse(false)
2019-07-09 19:34:57,805   WARN --- [heartbeat-receiver-event-loop-thread]  org.apache.spark.rpc.netty.NettyRpcEnv(line:66) : Ignored message: HeartbeatResponse(false)
2019-07-09 19:35:05,751   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:35:06,416   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:35:06,543   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:35:06,547   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:35:06,548   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:35:06,550   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:35:06,551   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:35:10,284   WARN --- [driver-heartbeater]  org.apache.spark.executor.Executor(line:87) : Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:92)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:841)
	at org.apache.spark.executor.Executor$$anon$2.$anonfun$run$24(Executor.scala:870)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.executor.Executor$$anon$2.run(Executor.scala:870)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:255)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:259)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:220)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 13 more
2019-07-09 19:35:10,290   WARN --- [heartbeat-receiver-event-loop-thread]  org.apache.spark.rpc.netty.NettyRpcEnv(line:66) : Ignored message: HeartbeatResponse(false)
2019-07-09 19:35:10,421   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:35:10,715   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:40
2019-07-09 19:35:10,795   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:40) with 2 output partitions
2019-07-09 19:35:10,796   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:40)
2019-07-09 19:35:10,801   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:35:10,807   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:35:10,842   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[3] at flatMap at test.scala:32), which has no missing parents
2019-07-09 19:35:10,997   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 1426.3 MB)
2019-07-09 19:35:11,014   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 19:35:11,016   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1160 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:35:11,018   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:35:11,092   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at flatMap at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:35:11,095   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:35:11,320   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:35:11,331   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:35:11,371   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:35:11,371   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:35:11,955   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1254.
2019-07-09 19:35:12,146   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:35:12,460   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:35:12,471   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:35:12,473   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:35:12,548   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 19:35:12,548   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 19:35:12,610   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-6982fefc-2d93-4be5-a649-06e55a383939
2019-07-09 19:35:12,688   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:35:12,728   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:35:12,874   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @18697ms
2019-07-09 19:35:12,968   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:35:12,984   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @18807ms
2019-07-09 19:35:13,015   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@38b107eb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:35:13,015   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:35:13,062   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@a8c1f44{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,062   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4a335fa8{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@44828f6b{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,077   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,093   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,093   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,093   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,093   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,093   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,093   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,093   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,093   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/static,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@73ff4fae{/,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/api,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54227100{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:35:13,124   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:35:13,484   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:35:13,593   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1275.
2019-07-09 19:35:13,593   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1275
2019-07-09 19:35:13,593   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:35:13,640   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1275, None)
2019-07-09 19:35:13,640   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1275 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1275, None)
2019-07-09 19:35:13,640   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1275, None)
2019-07-09 19:35:13,640   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1275, None)
2019-07-09 19:35:13,924   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@e27ba81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:35:14,885   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:35:15,008   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:35:15,015   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1275 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:35:15,023   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:35:15,264   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:35:15,382   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:40
2019-07-09 19:35:15,426   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:40) with 2 output partitions
2019-07-09 19:35:15,428   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:40)
2019-07-09 19:35:15,430   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:35:15,439   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:35:15,455   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:32), which has no missing parents
2019-07-09 19:35:15,629   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 19:35:15,642   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 19:35:15,645   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1275 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 19:35:15,647   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:35:15,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:35:15,701   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:35:15,812   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:35:15,827   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:35:15,859   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:35:15,859   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:35:16,624   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 19:35:16,624   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 19:35:17,411   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_1 stored as bytes in memory (estimated size 3.7 MB, free 1422.5 MB)
2019-07-09 19:35:17,413   INFO --- [Executor task launch worker for task 0]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_0 stored as bytes in memory (estimated size 3.7 MB, free 1418.8 MB)
2019-07-09 19:35:17,417   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_1 in memory on fc-pc:1275 (size: 3.7 MB, free: 1422.7 MB)
2019-07-09 19:35:17,419   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3925891 bytes result sent via BlockManager)
2019-07-09 19:35:17,484   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_0 in memory on fc-pc:1275 (size: 3.7 MB, free: 1419.0 MB)
2019-07-09 19:35:17,485   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 3929679 bytes result sent via BlockManager)
2019-07-09 19:35:17,539   INFO --- [task-result-getter-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:1275 after 69 ms (0 ms spent in bootstraps)
2019-07-09 19:35:18,147   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2318 ms on localhost (executor driver) (1/2)
2019-07-09 19:35:18,162   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2370 ms on localhost (executor driver) (2/2)
2019-07-09 19:35:18,166   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:35:18,170   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:40) finished in 2.560 s
2019-07-09 19:35:18,180   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_0 on fc-pc:1275 in memory (size: 3.7 MB, free: 1422.7 MB)
2019-07-09 19:35:18,182   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_1 on fc-pc:1275 in memory (size: 3.7 MB, free: 1426.5 MB)
2019-07-09 19:35:18,191   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:40, took 2.807650 s
2019-07-09 19:35:22,198   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@38b107eb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:35:22,200   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:35:22,232   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:35:22,328   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:35:22,329   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:35:22,334   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:35:22,342   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:35:22,381   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:35:22,381   INFO --- [main]  WordCount$(line:49) : complete!
2019-07-09 19:35:22,391   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:35:22,393   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-bb0d8dfe-9eb0-4ce5-b453-77a6007a3c79
2019-07-09 19:37:03,796   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:37:05,095   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:37:05,205   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:37:05,205   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:37:05,205   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:37:05,205   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:37:05,205   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:37:10,489   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1313.
2019-07-09 19:37:10,572   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:37:10,706   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:37:10,713   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:37:10,715   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:37:10,744   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-f0d6062e-7df5-4231-986c-d8ebbc2c38f8
2019-07-09 19:37:10,800   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:37:10,834   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:37:11,019   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @18654ms
2019-07-09 19:37:11,133   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:37:11,158   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @18795ms
2019-07-09 19:37:11,198   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:37:11,201   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:37:11,281   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,283   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,285   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,287   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,291   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,293   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,296   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,297   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,299   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,302   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,309   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,312   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,314   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,316   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,318   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,320   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,322   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,324   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,340   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,341   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,345   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,347   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,352   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:37:11,354   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:37:11,748   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:37:11,993   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1334.
2019-07-09 19:37:11,997   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1334
2019-07-09 19:37:12,002   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:37:12,269   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1334, None)
2019-07-09 19:37:12,278   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1334 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1334, None)
2019-07-09 19:37:12,287   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1334, None)
2019-07-09 19:37:12,290   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1334, None)
2019-07-09 19:37:12,620   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:37:13,546   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:37:14,111   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:37:14,117   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1334 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:37:14,122   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:37:14,287   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:37:14,408   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:40
2019-07-09 19:37:14,455   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:40) with 2 output partitions
2019-07-09 19:37:14,456   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:40)
2019-07-09 19:37:14,457   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:37:14,462   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:37:14,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[3] at map at test.scala:32), which has no missing parents
2019-07-09 19:37:14,559   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 1426.3 MB)
2019-07-09 19:37:14,574   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 19:37:14,576   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1334 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:37:14,578   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:37:14,613   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:37:14,616   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:37:14,748   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:37:14,757   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:37:14,783   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:37:14,783   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:37:15,578   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 19:37:15,578   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 19:37:16,186   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_1 stored as bytes in memory (estimated size 3.3 MB, free 1419.7 MB)
2019-07-09 19:37:16,186   INFO --- [Executor task launch worker for task 0]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_0 stored as bytes in memory (estimated size 3.3 MB, free 1419.7 MB)
2019-07-09 19:37:16,186   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_1 in memory on fc-pc:1334 (size: 3.3 MB, free: 1423.2 MB)
2019-07-09 19:37:16,186   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3419052 bytes result sent via BlockManager)
2019-07-09 19:37:16,201   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_0 in memory on fc-pc:1334 (size: 3.3 MB, free: 1420.0 MB)
2019-07-09 19:37:16,201   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 3421759 bytes result sent via BlockManager)
2019-07-09 19:37:16,295   INFO --- [task-result-getter-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:1334 after 64 ms (0 ms spent in bootstraps)
2019-07-09 19:37:17,053   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2352 ms on localhost (executor driver) (1/2)
2019-07-09 19:37:17,064   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2308 ms on localhost (executor driver) (2/2)
2019-07-09 19:37:17,085   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_0 on fc-pc:1334 in memory (size: 3.3 MB, free: 1423.2 MB)
2019-07-09 19:37:17,086   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:40) finished in 2.533 s
2019-07-09 19:37:17,127   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:40, took 2.718594 s
2019-07-09 19:37:17,135   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:37:17,142   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_1 on fc-pc:1334 in memory (size: 3.3 MB, free: 1426.5 MB)
2019-07-09 19:37:18,985   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:37:18,985   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:37:19,001   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:37:19,110   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:37:19,110   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:37:19,110   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:37:19,126   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:37:19,141   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:37:19,141   INFO --- [main]  WordCount$(line:49) : complete!
2019-07-09 19:37:19,141   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:37:19,141   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-f2adc487-c5ad-46f6-8546-0abf58a92586
2019-07-09 19:38:45,143   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:38:45,807   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:38:45,927   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:38:45,930   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:38:45,932   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:38:45,933   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:38:45,933   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:38:49,795   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1368.
2019-07-09 19:38:49,836   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:38:49,870   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:38:49,874   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:38:49,876   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:38:49,898   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-8b2f5d73-7ca9-4db8-9e29-ac57dee36168
2019-07-09 19:38:49,940   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:38:49,965   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:38:50,110   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16374ms
2019-07-09 19:38:50,200   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:38:50,220   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16487ms
2019-07-09 19:38:50,252   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:38:50,253   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:38:50,297   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,298   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,300   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,302   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,306   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,308   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,310   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,312   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,313   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,315   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,324   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,327   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,329   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,338   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,340   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,342   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,344   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,359   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,360   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,363   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,365   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,371   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:38:50,375   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:38:50,720   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:38:50,813   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1389.
2019-07-09 19:38:50,813   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1389
2019-07-09 19:38:50,813   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:38:50,845   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1389, None)
2019-07-09 19:38:50,860   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1389 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1389, None)
2019-07-09 19:38:50,860   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1389, None)
2019-07-09 19:38:50,860   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1389, None)
2019-07-09 19:38:51,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:38:52,131   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:38:52,272   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:38:52,280   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1389 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:38:52,290   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:38:52,483   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:38:52,610   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:40
2019-07-09 19:38:52,656   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:40) with 2 output partitions
2019-07-09 19:38:52,657   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:40)
2019-07-09 19:38:52,658   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:38:52,663   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:38:52,681   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMap at test.scala:32), which has no missing parents
2019-07-09 19:38:52,793   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.3 KB, free 1426.3 MB)
2019-07-09 19:38:52,812   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KB, free 1426.3 MB)
2019-07-09 19:38:52,814   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1389 (size: 2.4 KB, free: 1426.5 MB)
2019-07-09 19:38:52,816   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:38:52,856   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMap at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:38:52,856   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:38:52,966   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:38:52,981   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 19:38:53,012   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:38:53,012   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:38:53,762   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 19:38:53,762   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 19:38:54,169   INFO --- [Executor task launch worker for task 0]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_0 stored as bytes in memory (estimated size 2.4 MB, free 1423.8 MB)
2019-07-09 19:38:54,169   INFO --- [Executor task launch worker for task 1]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_1 stored as bytes in memory (estimated size 2.4 MB, free 1421.4 MB)
2019-07-09 19:38:54,169   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_0 in memory on fc-pc:1389 (size: 2.4 MB, free: 1424.1 MB)
2019-07-09 19:38:54,184   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_1 in memory on fc-pc:1389 (size: 2.4 MB, free: 1421.6 MB)
2019-07-09 19:38:54,184   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 2533731 bytes result sent via BlockManager)
2019-07-09 19:38:54,184   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 2533557 bytes result sent via BlockManager)
2019-07-09 19:38:54,278   INFO --- [task-result-getter-1]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:1389 after 57 ms (0 ms spent in bootstraps)
2019-07-09 19:38:54,591   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1657 ms on localhost (executor driver) (1/2)
2019-07-09 19:38:54,622   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_0 on fc-pc:1389 in memory (size: 2.4 MB, free: 1424.1 MB)
2019-07-09 19:38:54,622   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1641 ms on localhost (executor driver) (2/2)
2019-07-09 19:38:54,637   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:38:54,637   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_1 on fc-pc:1389 in memory (size: 2.4 MB, free: 1426.5 MB)
2019-07-09 19:38:54,653   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:40) finished in 1.873 s
2019-07-09 19:38:54,669   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:40, took 2.065238 s
2019-07-09 19:38:55,086   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:38:55,086   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:38:55,102   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:38:55,196   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:38:55,196   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:38:55,196   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:38:55,211   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:38:55,227   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:38:55,227   INFO --- [main]  WordCount$(line:49) : complete!
2019-07-09 19:38:55,227   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:38:55,227   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-29ebdb82-b0ab-4807-a0f7-9a7c02b076d1
2019-07-09 19:39:49,661   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:39:50,267   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:39:50,376   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:39:50,376   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:39:50,376   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:39:50,376   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:39:50,376   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:39:54,251   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1419.
2019-07-09 19:39:54,282   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:39:54,313   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:39:54,313   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:39:54,313   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:39:54,344   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-04b07c30-2cf1-4214-a03e-0efe4bfcb474
2019-07-09 19:39:54,376   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:39:54,407   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:39:54,548   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16337ms
2019-07-09 19:39:54,641   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:39:54,657   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16438ms
2019-07-09 19:39:54,688   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:39:54,688   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:39:54,719   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,719   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,719   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,719   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,735   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,751   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,751   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,751   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,751   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,751   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,751   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,751   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,766   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,766   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:39:54,766   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:39:55,032   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:39:55,141   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1440.
2019-07-09 19:39:55,141   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1440
2019-07-09 19:39:55,141   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:39:55,173   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1440, None)
2019-07-09 19:39:55,173   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1440 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1440, None)
2019-07-09 19:39:55,188   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1440, None)
2019-07-09 19:39:55,188   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1440, None)
2019-07-09 19:39:55,454   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:39:56,499   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:39:56,664   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:39:56,672   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1440 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:39:56,682   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:39:56,863   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:39:56,988   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:40
2019-07-09 19:39:57,035   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:40) with 2 output partitions
2019-07-09 19:39:57,035   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:40)
2019-07-09 19:39:57,035   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:39:57,035   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:39:57,051   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[3] at map at test.scala:32), which has no missing parents
2019-07-09 19:39:57,191   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 1426.3 MB)
2019-07-09 19:39:57,207   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 19:39:57,222   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1440 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:39:57,222   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:39:57,269   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:39:57,269   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:39:57,379   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:39:57,379   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:39:57,410   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:39:57,410   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:39:58,254   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:39:58,254   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:39:58,347   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 3768 bytes result sent to driver
2019-07-09 19:39:58,347   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3624 bytes result sent to driver
2019-07-09 19:39:58,363   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1016 ms on localhost (executor driver) (1/2)
2019-07-09 19:39:58,379   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1000 ms on localhost (executor driver) (2/2)
2019-07-09 19:39:58,379   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:39:58,379   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:40) finished in 1.188 s
2019-07-09 19:39:58,410   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:40, took 1.423479 s
2019-07-09 19:39:58,426   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:39:58,441   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:39:58,457   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:39:58,566   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:39:58,566   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:39:58,566   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:39:58,582   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:39:58,597   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:39:58,597   INFO --- [main]  WordCount$(line:49) : complete!
2019-07-09 19:39:58,597   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:39:58,597   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-f0417af8-5814-466c-8f32-3f761d7d4174
2019-07-09 19:40:59,912   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:41:00,552   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:41:00,662   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:41:00,662   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:41:00,662   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:41:00,662   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:41:00,662   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:41:04,610   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1472.
2019-07-09 19:41:04,641   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:41:04,672   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:41:04,688   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:41:04,688   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:41:04,704   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-b4f64759-057b-42c9-9a4c-bd850f268b9c
2019-07-09 19:41:04,735   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:41:04,766   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:41:04,907   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16704ms
2019-07-09 19:41:05,001   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:41:05,016   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16816ms
2019-07-09 19:41:05,047   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:41:05,047   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:41:05,094   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,094   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,126   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,126   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,126   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,126   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,141   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,141   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,141   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,141   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,157   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,157   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,157   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,157   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,157   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,157   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,157   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,172   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,172   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,172   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,172   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,188   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:41:05,188   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:41:05,485   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:41:05,610   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1493.
2019-07-09 19:41:05,610   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1493
2019-07-09 19:41:05,625   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:41:05,657   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1493, None)
2019-07-09 19:41:05,657   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1493 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1493, None)
2019-07-09 19:41:05,672   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1493, None)
2019-07-09 19:41:05,672   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1493, None)
2019-07-09 19:41:05,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:41:06,992   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:41:07,176   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:41:07,183   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1493 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:41:07,193   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:41:07,410   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:41:07,590   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:40
2019-07-09 19:41:07,660   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:40) with 2 output partitions
2019-07-09 19:41:07,661   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:40)
2019-07-09 19:41:07,662   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:41:07,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:41:07,694   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[3] at map at test.scala:32), which has no missing parents
2019-07-09 19:41:07,797   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 1426.3 MB)
2019-07-09 19:41:07,833   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 19:41:07,833   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1493 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:41:07,833   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:41:07,880   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:41:07,880   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:41:08,005   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:41:08,020   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:41:08,052   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:41:08,052   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:41:08,864   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:41:08,895   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:41:08,978   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 3943 bytes result sent to driver
2019-07-09 19:41:08,979   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3807 bytes result sent to driver
2019-07-09 19:41:08,996   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1018 ms on localhost (executor driver) (1/2)
2019-07-09 19:41:09,001   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 981 ms on localhost (executor driver) (2/2)
2019-07-09 19:41:09,004   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:41:09,021   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:40) finished in 1.241 s
2019-07-09 19:41:09,039   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:40, took 1.448970 s
2019-07-09 19:41:09,065   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:41:09,071   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:41:09,127   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:41:09,206   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:41:09,207   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:41:09,220   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:41:09,226   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:41:09,244   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:41:09,244   INFO --- [main]  WordCount$(line:49) : complete!
2019-07-09 19:41:09,250   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:41:09,251   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-df2f6c39-875b-407a-b1b2-c124e977d016
2019-07-09 19:41:57,422   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:41:58,110   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:41:58,204   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:41:58,204   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:41:58,204   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:41:58,219   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:41:58,219   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:42:02,617   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1523.
2019-07-09 19:42:02,657   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:42:02,689   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:42:02,693   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:42:02,694   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:42:02,713   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-e7b19e3c-5fb6-42bd-949e-8489afca07a4
2019-07-09 19:42:02,754   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:42:02,778   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:42:02,920   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16978ms
2019-07-09 19:42:03,010   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:42:03,030   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17088ms
2019-07-09 19:42:03,062   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@77be8c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:42:03,063   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:42:03,109   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,110   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,112   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,118   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,119   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,122   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,124   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,125   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,129   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,138   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,143   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,148   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,150   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,152   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,154   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,155   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,158   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,160   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,163   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,183   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,184   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,200   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,203   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,210   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:42:03,214   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:42:03,638   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:42:03,754   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1549.
2019-07-09 19:42:03,755   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1549
2019-07-09 19:42:03,759   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:42:03,798   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1549, None)
2019-07-09 19:42:03,798   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1549 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1549, None)
2019-07-09 19:42:03,798   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1549, None)
2019-07-09 19:42:03,798   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1549, None)
2019-07-09 19:42:04,063   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:42:05,303   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:42:05,578   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:42:05,585   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1549 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:42:05,592   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:31
2019-07-09 19:42:05,767   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:42:05,936   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:40
2019-07-09 19:42:05,978   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:40) with 2 output partitions
2019-07-09 19:42:05,979   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:40)
2019-07-09 19:42:05,980   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:42:05,984   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:42:05,997   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:32), which has no missing parents
2019-07-09 19:42:06,061   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 19:42:06,071   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 19:42:06,074   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1549 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 19:42:06,076   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:42:06,115   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:32) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:42:06,121   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:42:06,417   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:42:06,427   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:42:06,458   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:42:06,458   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:42:07,516   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:42:07,531   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:42:07,641   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3974 bytes result sent to driver
2019-07-09 19:42:07,641   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 4082 bytes result sent to driver
2019-07-09 19:42:07,656   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1230 ms on localhost (executor driver) (1/2)
2019-07-09 19:42:07,656   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1353 ms on localhost (executor driver) (2/2)
2019-07-09 19:42:07,656   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:42:07,672   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:40) finished in 1.622 s
2019-07-09 19:42:07,688   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:40, took 1.752911 s
2019-07-09 19:42:07,719   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@77be8c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:42:07,735   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:42:07,750   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:42:07,781   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:42:07,797   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:42:07,797   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:42:07,813   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:42:07,828   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:42:07,828   INFO --- [main]  WordCount$(line:49) : complete!
2019-07-09 19:42:07,828   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:42:07,828   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-d2743e4c-9c2d-41b9-a0c7-75a0d9cba421
2019-07-09 19:43:47,302   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:43:47,960   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:43:48,066   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:43:48,070   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:43:48,071   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:43:48,072   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:43:48,074   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:43:51,997   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1585.
2019-07-09 19:43:52,042   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:43:52,075   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:43:52,079   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:43:52,080   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:43:52,099   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-a3bd353d-19e7-4a5a-8139-3181d118ce62
2019-07-09 19:43:52,140   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:43:52,164   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:43:52,310   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16394ms
2019-07-09 19:43:52,400   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:43:52,419   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16504ms
2019-07-09 19:43:52,454   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@45d7ef20{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:43:52,454   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:43:52,513   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,515   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,521   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,524   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,527   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,532   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,534   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,536   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,538   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,541   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,544   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,551   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,554   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,556   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,557   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,558   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,559   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,562   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,564   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,577   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,578   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,581   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,584   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,588   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:43:52,591   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:43:52,846   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:43:52,939   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1606.
2019-07-09 19:43:52,939   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1606
2019-07-09 19:43:52,939   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:43:52,986   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1606, None)
2019-07-09 19:43:52,986   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1606 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1606, None)
2019-07-09 19:43:52,986   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1606, None)
2019-07-09 19:43:52,986   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1606, None)
2019-07-09 19:43:53,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:43:54,276   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:43:54,427   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:43:54,435   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1606 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:43:54,445   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 19:43:54,663   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:43:54,780   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 19:43:54,825   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 19:43:54,826   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 19:43:54,827   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:43:54,832   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:43:54,845   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[3] at flatMap at test.scala:34), which has no missing parents
2019-07-09 19:43:54,970   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.5 KB, free 1426.3 MB)
2019-07-09 19:43:54,993   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 19:43:54,999   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1606 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:43:55,004   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:43:55,045   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at flatMap at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:43:55,048   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:43:55,164   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:43:55,173   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:43:55,202   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:43:55,202   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:43:56,017   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:43:56,017   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:43:56,114   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 3557 bytes result sent to driver
2019-07-09 19:43:56,115   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3448 bytes result sent to driver
2019-07-09 19:43:56,130   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 996 ms on localhost (executor driver) (1/2)
2019-07-09 19:43:56,137   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 966 ms on localhost (executor driver) (2/2)
2019-07-09 19:43:56,139   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:43:56,155   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.201 s
2019-07-09 19:43:56,171   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.390514 s
2019-07-09 19:43:56,194   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@45d7ef20{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:43:56,194   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:43:56,241   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:43:56,303   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:43:56,303   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:43:56,319   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:43:56,334   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:43:56,350   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:43:56,350   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 19:43:56,350   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:43:56,366   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-a598d3ac-8176-460d-9e9e-309b783f4794
2019-07-09 19:45:54,684   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:45:55,282   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:45:55,376   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:45:55,391   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:45:55,391   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:45:55,391   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:45:55,391   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:45:59,297   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1639.
2019-07-09 19:45:59,344   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:45:59,375   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:45:59,375   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:45:59,375   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:45:59,391   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-46d5aa5a-93c2-4830-b867-2cca3c568aef
2019-07-09 19:45:59,438   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:45:59,454   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:45:59,594   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16331ms
2019-07-09 19:45:59,688   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:45:59,704   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16434ms
2019-07-09 19:45:59,735   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:45:59,735   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,782   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,797   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,797   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,797   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,797   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,805   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,807   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,809   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,812   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,822   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,823   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,825   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,831   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:45:59,834   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:46:00,139   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:46:00,265   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1660.
2019-07-09 19:46:00,266   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1660
2019-07-09 19:46:00,269   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:46:00,311   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1660, None)
2019-07-09 19:46:00,318   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1660 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1660, None)
2019-07-09 19:46:00,323   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1660, None)
2019-07-09 19:46:00,325   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1660, None)
2019-07-09 19:46:00,630   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:01,662   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:46:01,834   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:46:01,834   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1660 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:46:01,850   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 19:46:02,037   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:46:02,162   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 19:46:02,209   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 19:46:02,209   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 19:46:02,209   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:46:02,225   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:46:02,240   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34), which has no missing parents
2019-07-09 19:46:02,350   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 19:46:02,365   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 19:46:02,365   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1660 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 19:46:02,365   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:46:02,412   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:46:02,412   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:46:02,568   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:46:02,568   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:46:02,600   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:46:02,600   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:46:03,725   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:46:03,725   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:46:03,803   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 4487 bytes result sent to driver
2019-07-09 19:46:03,803   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 4590 bytes result sent to driver
2019-07-09 19:46:03,834   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1250 ms on localhost (executor driver) (1/2)
2019-07-09 19:46:03,834   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1328 ms on localhost (executor driver) (2/2)
2019-07-09 19:46:03,834   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:46:03,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.516 s
2019-07-09 19:46:03,865   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.707816 s
2019-07-09 19:46:03,896   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:46:03,896   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:46:03,912   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:46:03,959   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:46:03,959   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:46:03,959   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:46:03,959   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:46:03,975   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:46:03,975   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 19:46:03,975   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:46:03,990   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-d8691444-3124-461c-b8ff-f966ff8dc739
2019-07-09 19:46:49,571   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:46:50,157   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:46:50,266   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:46:50,266   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:46:50,266   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:46:50,266   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:46:50,266   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:46:54,116   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1688.
2019-07-09 19:46:54,147   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:46:54,178   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:46:54,178   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:46:54,178   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:46:54,194   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-bbc5a88f-8259-47de-8552-7b2d2377154a
2019-07-09 19:46:54,241   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:46:54,272   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:46:54,397   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16248ms
2019-07-09 19:46:54,491   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:46:54,507   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16355ms
2019-07-09 19:46:54,538   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@6d69c3bf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:46:54,538   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:46:54,585   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,585   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,585   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,585   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,585   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,585   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,585   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,585   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,585   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,585   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,600   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,616   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,616   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,616   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,632   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,632   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:46:54,632   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:46:54,921   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:46:55,030   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1709.
2019-07-09 19:46:55,030   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1709
2019-07-09 19:46:55,046   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:46:55,077   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1709, None)
2019-07-09 19:46:55,077   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1709 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1709, None)
2019-07-09 19:46:55,093   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1709, None)
2019-07-09 19:46:55,093   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1709, None)
2019-07-09 19:46:55,343   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:46:56,381   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:46:56,559   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:46:56,566   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1709 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:46:56,575   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 19:46:56,758   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:46:56,861   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 19:46:56,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 19:46:56,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 19:46:56,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:46:56,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:46:56,924   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at flatMap at test.scala:34), which has no missing parents
2019-07-09 19:46:57,033   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 19:46:57,049   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 19:46:57,049   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1709 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:46:57,049   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:46:57,111   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at flatMap at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:46:57,111   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:46:57,236   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:46:57,252   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:46:57,283   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:46:57,283   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:46:58,080   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:46:58,080   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:46:58,174   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3485 bytes result sent to driver
2019-07-09 19:46:58,174   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 3629 bytes result sent to driver
2019-07-09 19:46:58,189   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 937 ms on localhost (executor driver) (1/2)
2019-07-09 19:46:58,189   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 984 ms on localhost (executor driver) (2/2)
2019-07-09 19:46:58,189   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:46:58,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.188 s
2019-07-09 19:46:58,220   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.361183 s
2019-07-09 19:46:58,252   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@6d69c3bf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:46:58,267   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:46:58,283   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:46:58,345   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:46:58,345   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:46:58,345   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:46:58,361   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:46:58,377   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:46:58,377   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 19:46:58,377   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:46:58,377   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-4097ae52-90e4-4087-a01e-2efe5653f9bc
2019-07-09 19:51:09,683   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:51:10,277   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:51:10,386   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:51:10,386   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:51:10,386   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:51:10,386   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:51:10,386   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:51:14,277   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1905.
2019-07-09 19:51:14,323   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:51:14,355   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:51:14,355   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:51:14,355   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:51:14,370   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-da5f44d8-1f2c-4433-8f6d-4f25c2e077aa
2019-07-09 19:51:14,417   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:51:14,433   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:51:14,589   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16309ms
2019-07-09 19:51:14,683   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:51:14,698   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16416ms
2019-07-09 19:51:14,745   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:51:14,745   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:51:14,777   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,777   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,777   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,777   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,792   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,808   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,823   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,823   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,823   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,839   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,839   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:51:14,839   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:51:15,089   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:51:15,198   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1926.
2019-07-09 19:51:15,214   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1926
2019-07-09 19:51:15,214   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:51:15,245   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1926, None)
2019-07-09 19:51:15,245   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1926 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1926, None)
2019-07-09 19:51:15,261   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1926, None)
2019-07-09 19:51:15,261   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1926, None)
2019-07-09 19:51:15,511   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:51:16,417   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:51:16,558   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:51:16,573   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1926 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:51:16,573   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 19:51:16,776   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:51:16,901   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 19:51:16,948   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 19:51:16,948   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 19:51:16,948   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:51:16,948   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:51:16,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34), which has no missing parents
2019-07-09 19:51:17,136   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 19:51:17,151   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 19:51:17,167   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:1926 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 19:51:17,167   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:51:17,198   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:51:17,198   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:51:17,323   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:51:17,339   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:51:17,370   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:51:17,370   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:51:18,151   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:51:18,151   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:51:18,245   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 4022 bytes result sent to driver
2019-07-09 19:51:18,245   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3918 bytes result sent to driver
2019-07-09 19:51:18,261   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 922 ms on localhost (executor driver) (1/2)
2019-07-09 19:51:18,261   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 985 ms on localhost (executor driver) (2/2)
2019-07-09 19:51:18,261   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:51:18,276   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.218 s
2019-07-09 19:51:18,292   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.398055 s
2019-07-09 19:51:18,323   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:51:18,323   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:51:18,355   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:51:18,386   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:51:18,386   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:51:18,401   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:51:18,401   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:51:18,433   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:51:18,433   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 19:51:18,433   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:51:18,433   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-cfb4a08e-84d9-450c-9218-c09ce1fad993
2019-07-09 19:52:51,900   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:52:52,509   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:52:52,638   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:52:52,638   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:52:52,638   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:52:52,638   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:52:52,638   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:52:56,396   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 1966.
2019-07-09 19:52:56,427   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:52:56,458   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:52:56,474   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:52:56,474   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:52:56,490   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-9dbc989a-b13d-4089-b832-bb0232e5fbe2
2019-07-09 19:52:56,537   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:52:56,568   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:52:56,708   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16263ms
2019-07-09 19:52:56,802   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:52:56,818   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16372ms
2019-07-09 19:52:56,849   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:52:56,849   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:52:56,896   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,896   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,896   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,896   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,896   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,896   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,896   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,927   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,927   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,927   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,927   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,943   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,943   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,943   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,943   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,943   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:52:56,958   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:52:57,240   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:52:57,349   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1987.
2019-07-09 19:52:57,349   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:1987
2019-07-09 19:52:57,349   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:52:57,380   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 1987, None)
2019-07-09 19:52:57,396   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:1987 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 1987, None)
2019-07-09 19:52:57,396   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 1987, None)
2019-07-09 19:52:57,396   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 1987, None)
2019-07-09 19:52:57,677   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:52:58,755   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:52:59,005   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:52:59,021   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:1987 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:52:59,021   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 19:52:59,146   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:52:59,146   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:52:59,161   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:52:59,193   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:52:59,193   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:52:59,208   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:52:59,208   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:52:59,240   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:52:59,240   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 19:52:59,240   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:52:59,240   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-86965538-8071-45f5-b5b0-0a8b01b07b20
2019-07-09 19:55:09,716   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:55:10,326   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:55:10,419   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:55:10,419   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:55:10,419   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:55:10,419   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:55:10,419   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:55:14,247   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2021.
2019-07-09 19:55:14,294   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:55:14,325   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:55:14,325   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:55:14,325   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:55:14,341   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-2a9c7d9d-133f-48ef-82c6-b9982edbd9e5
2019-07-09 19:55:14,388   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:55:14,404   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:55:14,544   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16282ms
2019-07-09 19:55:14,638   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:55:14,654   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16393ms
2019-07-09 19:55:14,700   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1f3f0c9a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:55:14,700   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:55:14,732   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,732   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,747   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,747   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,747   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,747   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,747   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,747   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,747   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,747   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,747   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,763   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,763   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,763   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,763   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,763   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,763   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,779   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,779   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,779   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,794   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,794   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,794   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,794   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,794   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:55:14,810   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:55:15,044   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:55:15,138   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2042.
2019-07-09 19:55:15,154   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2042
2019-07-09 19:55:15,154   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:55:15,185   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2042, None)
2019-07-09 19:55:15,185   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2042 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2042, None)
2019-07-09 19:55:15,200   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2042, None)
2019-07-09 19:55:15,200   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2042, None)
2019-07-09 19:55:15,450   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:55:16,466   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:55:16,638   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:55:16,638   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2042 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:55:16,653   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 19:55:16,825   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:55:16,935   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 19:55:16,982   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 19:55:16,982   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 19:55:16,982   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:55:16,997   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:55:17,013   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34), which has no missing parents
2019-07-09 19:55:17,153   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 19:55:17,169   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 19:55:17,169   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2042 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 19:55:17,169   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:55:17,216   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:55:17,216   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:55:17,372   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:55:17,388   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:55:17,403   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:55:17,403   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:55:18,216   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:55:18,216   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:55:18,310   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3918 bytes result sent to driver
2019-07-09 19:55:18,310   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 4022 bytes result sent to driver
2019-07-09 19:55:18,325   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 937 ms on localhost (executor driver) (1/2)
2019-07-09 19:55:18,325   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 984 ms on localhost (executor driver) (2/2)
2019-07-09 19:55:18,341   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:55:18,341   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.203 s
2019-07-09 19:55:18,357   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.421685 s
2019-07-09 19:55:18,388   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1f3f0c9a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:55:18,388   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:55:18,419   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:55:18,466   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:55:18,466   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:55:18,482   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:55:18,482   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:55:18,513   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:55:18,513   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 19:55:18,528   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:55:18,528   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-9386854e-7224-4902-a74a-6e47a810421b
2019-07-09 19:55:58,315   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:55:59,117   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:55:59,296   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:55:59,301   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:55:59,302   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:55:59,303   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:55:59,305   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:56:03,693   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2089.
2019-07-09 19:56:03,728   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:56:03,759   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:56:03,759   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:56:03,759   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:56:03,812   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-266ca1a5-5d77-4596-942d-a2d250bbecb8
2019-07-09 19:56:03,861   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:56:03,919   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:56:04,109   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17964ms
2019-07-09 19:56:04,248   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:56:04,280   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @18141ms
2019-07-09 19:56:04,339   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@109a4414{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:56:04,340   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:56:04,391   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,392   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,393   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,395   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,397   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,398   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,398   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,398   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,398   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,398   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,398   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,398   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,398   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,413   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,413   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,413   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,413   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,413   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,413   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,413   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,429   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,429   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,429   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,445   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,445   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:56:04,445   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:56:04,933   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:56:05,098   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2117.
2019-07-09 19:56:05,101   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2117
2019-07-09 19:56:05,105   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:56:05,170   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2117, None)
2019-07-09 19:56:05,180   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2117 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2117, None)
2019-07-09 19:56:05,187   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2117, None)
2019-07-09 19:56:05,188   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2117, None)
2019-07-09 19:56:05,511   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:56:06,777   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:56:07,029   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:56:07,035   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2117 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:56:07,044   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 19:56:07,360   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:56:07,751   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 19:56:07,838   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 19:56:07,839   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 19:56:07,840   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:56:07,845   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:56:07,858   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at flatMap at test.scala:34), which has no missing parents
2019-07-09 19:56:08,020   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 19:56:08,045   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 19:56:08,045   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2117 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 19:56:08,045   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:56:08,107   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at flatMap at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:56:08,123   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:56:08,358   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:56:08,365   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:56:08,393   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:56:08,395   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:56:09,234   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:56:09,249   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:56:09,321   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3485 bytes result sent to driver
2019-07-09 19:56:09,321   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 3629 bytes result sent to driver
2019-07-09 19:56:09,337   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 973 ms on localhost (executor driver) (1/2)
2019-07-09 19:56:09,353   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1091 ms on localhost (executor driver) (2/2)
2019-07-09 19:56:09,353   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:56:09,368   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.387 s
2019-07-09 19:56:09,384   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.631064 s
2019-07-09 19:56:09,399   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@109a4414{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:56:09,415   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:56:09,431   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:56:09,516   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:56:09,516   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:56:09,531   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:56:09,531   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:56:09,563   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:56:09,563   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 19:56:09,576   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:56:09,578   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-9ac93497-dc22-40da-8cf0-7f5cb1f9fe63
2019-07-09 19:57:11,576   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:57:12,170   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:57:12,279   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:57:12,279   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:57:12,279   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:57:12,279   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:57:12,279   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:57:16,029   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2196.
2019-07-09 19:57:16,076   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:57:16,107   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:57:16,107   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:57:16,107   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:57:16,123   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-3a87e513-35cd-4e46-ab83-9da7e0c01f01
2019-07-09 19:57:16,170   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:57:16,185   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:57:16,326   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16546ms
2019-07-09 19:57:16,404   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:57:16,420   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16644ms
2019-07-09 19:57:16,451   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:57:16,451   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:57:16,482   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,482   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,482   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,482   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,482   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,482   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,482   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,498   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,498   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,498   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,498   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,498   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,498   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,498   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,513   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,513   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,513   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,513   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,513   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,513   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,529   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,529   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,529   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,529   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,545   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:57:16,545   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:57:16,810   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:57:16,920   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2217.
2019-07-09 19:57:16,920   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2217
2019-07-09 19:57:16,935   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:57:16,967   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2217, None)
2019-07-09 19:57:16,982   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2217 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2217, None)
2019-07-09 19:57:16,982   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2217, None)
2019-07-09 19:57:16,982   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2217, None)
2019-07-09 19:57:17,263   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:57:18,201   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:57:18,342   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:57:18,357   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2217 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:57:18,357   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 19:57:18,592   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:57:18,717   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 19:57:18,763   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 19:57:18,763   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 19:57:18,763   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:57:18,763   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:57:18,779   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34), which has no missing parents
2019-07-09 19:57:18,935   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 19:57:18,967   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 19:57:18,967   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2217 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 19:57:18,982   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:57:19,013   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:57:19,013   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:57:19,154   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:57:19,154   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:57:19,201   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:57:19,201   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:57:19,935   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:57:19,935   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:57:20,029   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 4082 bytes result sent to driver
2019-07-09 19:57:20,029   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3974 bytes result sent to driver
2019-07-09 19:57:20,045   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 922 ms on localhost (executor driver) (1/2)
2019-07-09 19:57:20,045   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 891 ms on localhost (executor driver) (2/2)
2019-07-09 19:57:20,060   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:57:20,076   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.156 s
2019-07-09 19:57:20,091   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.371437 s
2019-07-09 19:57:20,107   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:57:20,107   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:57:20,138   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:57:20,185   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:57:20,185   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:57:20,185   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:57:20,185   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:57:20,216   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:57:20,216   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 19:57:20,216   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:57:20,216   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-78a5a820-840b-44fa-8b3e-87d0108a84f7
2019-07-09 19:58:34,929   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:58:35,589   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:58:35,683   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:58:35,683   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:58:35,699   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:58:35,699   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:58:35,699   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:58:39,734   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2270.
2019-07-09 19:58:39,783   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:58:39,803   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:58:39,803   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:58:39,819   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:58:43,333   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-c37582e0-b894-483a-83e8-8b5b99cb9d6a
2019-07-09 19:58:43,389   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:58:43,401   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:58:43,542   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @19997ms
2019-07-09 19:58:43,853   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:58:43,869   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @20338ms
2019-07-09 19:58:43,931   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1e2bcadb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:58:43,931   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:58:44,009   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@71b3bc45{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,009   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@76f7d241{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,009   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4a335fa8{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,009   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,009   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,025   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,025   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,025   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,025   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@44828f6b{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,025   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,025   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,041   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,072   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/static,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,072   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@78fb9a67{/,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,087   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@73ff4fae{/api,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,087   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2611b9a3{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,087   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54227100{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:58:44,087   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:58:44,371   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:58:44,480   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2293.
2019-07-09 19:58:44,480   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2293
2019-07-09 19:58:44,480   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:58:44,512   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2293, None)
2019-07-09 19:58:44,527   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2293 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2293, None)
2019-07-09 19:58:44,527   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2293, None)
2019-07-09 19:58:44,527   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2293, None)
2019-07-09 19:58:44,793   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6548bb7d{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:58:45,762   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:58:45,919   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:58:45,926   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2293 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:58:45,926   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 19:58:46,139   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:58:46,264   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 19:58:46,311   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 19:58:46,311   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 19:58:46,311   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:58:46,311   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:58:46,326   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34), which has no missing parents
2019-07-09 19:58:46,444   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 19:58:46,465   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 19:58:46,471   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2293 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 19:58:46,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:58:46,553   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:58:46,558   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:58:46,810   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:58:46,813   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:58:46,845   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:58:46,845   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:58:47,721   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:58:47,721   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:58:47,799   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3977 bytes result sent to driver
2019-07-09 19:58:47,799   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 4046 bytes result sent to driver
2019-07-09 19:58:47,815   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1002 ms on localhost (executor driver) (1/2)
2019-07-09 19:58:47,830   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1126 ms on localhost (executor driver) (2/2)
2019-07-09 19:58:47,830   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:58:47,846   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.441 s
2019-07-09 19:58:47,862   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.592987 s
2019-07-09 19:58:47,893   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1e2bcadb{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:58:47,893   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:58:47,924   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:58:47,971   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:58:47,971   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:58:48,002   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:58:48,002   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:58:48,018   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:58:48,018   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 19:58:48,018   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:58:48,033   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-1e34674c-0df6-44db-8e1a-cc1ac96a32a0
2019-07-09 19:59:27,391   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 19:59:28,031   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 19:59:28,141   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 19:59:28,141   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 19:59:28,141   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 19:59:28,141   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 19:59:28,141   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 19:59:32,231   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2426.
2019-07-09 19:59:32,278   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 19:59:32,310   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 19:59:32,325   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 19:59:32,325   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 19:59:32,341   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-17da62e0-4550-4869-937e-5be9eeeddef9
2019-07-09 19:59:32,388   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 19:59:32,403   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 19:59:32,595   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17269ms
2019-07-09 19:59:32,673   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 19:59:32,705   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17377ms
2019-07-09 19:59:32,720   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:59:32,736   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 19:59:32,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,783   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,783   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,783   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,783   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,783   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,783   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,783   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,783   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,783   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,798   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,798   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,798   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,814   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,814   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 19:59:32,814   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 19:59:33,080   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 19:59:33,173   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2448.
2019-07-09 19:59:33,173   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2448
2019-07-09 19:59:33,192   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 19:59:33,250   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2448, None)
2019-07-09 19:59:33,257   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2448 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2448, None)
2019-07-09 19:59:33,263   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2448, None)
2019-07-09 19:59:33,264   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2448, None)
2019-07-09 19:59:33,670   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 19:59:34,953   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 19:59:35,072   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 19:59:35,087   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2448 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 19:59:35,087   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 19:59:35,275   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 19:59:35,384   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 19:59:35,431   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 19:59:35,431   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 19:59:35,431   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 19:59:35,447   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 19:59:35,462   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34), which has no missing parents
2019-07-09 19:59:35,556   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 19:59:35,572   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 19:59:35,587   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2448 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 19:59:35,587   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 19:59:35,665   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 19:59:35,665   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 19:59:35,775   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:59:35,790   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 19:59:35,837   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 19:59:35,822   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 19:59:36,665   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 19:59:36,665   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 19:59:36,743   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3894 bytes result sent to driver
2019-07-09 19:59:36,743   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 4053 bytes result sent to driver
2019-07-09 19:59:36,759   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 969 ms on localhost (executor driver) (1/2)
2019-07-09 19:59:36,775   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1032 ms on localhost (executor driver) (2/2)
2019-07-09 19:59:36,775   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 19:59:36,790   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.265 s
2019-07-09 19:59:36,806   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.420711 s
2019-07-09 19:59:36,837   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 19:59:36,837   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 19:59:36,868   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 19:59:36,915   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 19:59:36,915   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 19:59:36,915   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 19:59:36,931   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 19:59:36,946   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 19:59:36,946   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 19:59:36,946   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 19:59:36,946   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-62aada86-7b7d-4f9b-8d00-a7b6117cb927
2019-07-09 20:01:02,544   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:01:03,134   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:01:03,260   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:01:03,265   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:01:03,266   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:01:03,267   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:01:03,268   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:01:07,166   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2490.
2019-07-09 20:01:07,197   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:01:07,228   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:01:07,228   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:01:07,228   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:01:07,244   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-89d7ea82-ec91-4aa3-a57a-36154ee31e58
2019-07-09 20:01:07,291   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:01:07,306   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:01:07,447   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16821ms
2019-07-09 20:01:07,525   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:01:07,556   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16932ms
2019-07-09 20:01:07,588   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2a77d999{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:01:07,588   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:01:07,634   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,634   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,634   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,634   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,634   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,634   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,634   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,650   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,666   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,666   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,666   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,681   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,681   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,681   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:01:07,681   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:01:07,947   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:01:08,056   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2511.
2019-07-09 20:01:08,056   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2511
2019-07-09 20:01:08,056   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:01:08,088   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2511, None)
2019-07-09 20:01:08,103   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2511 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2511, None)
2019-07-09 20:01:08,103   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2511, None)
2019-07-09 20:01:08,103   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2511, None)
2019-07-09 20:01:08,405   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:01:09,431   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:01:09,603   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:01:09,619   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2511 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:01:09,619   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 20:01:09,806   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:01:09,915   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 20:01:09,962   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 20:01:09,962   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 20:01:09,962   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 20:01:09,962   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 20:01:09,994   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34), which has no missing parents
2019-07-09 20:01:10,087   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 20:01:10,103   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 20:01:10,103   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2511 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 20:01:10,103   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:01:10,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:01:10,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:01:10,259   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 20:01:10,259   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 20:01:10,290   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:01:10,290   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:01:11,072   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 20:01:11,072   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 20:01:11,165   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1568 bytes result sent to driver
2019-07-09 20:01:11,165   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1504 bytes result sent to driver
2019-07-09 20:01:11,181   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 922 ms on localhost (executor driver) (1/2)
2019-07-09 20:01:11,181   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 953 ms on localhost (executor driver) (2/2)
2019-07-09 20:01:11,197   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:01:11,197   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.125 s
2019-07-09 20:01:11,212   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.300451 s
2019-07-09 20:01:11,244   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2a77d999{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:01:11,244   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:01:11,259   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:01:11,337   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:01:11,337   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:01:11,353   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:01:11,353   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:01:11,384   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:01:11,384   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 20:01:11,400   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:01:11,400   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-0ab58699-d3ab-4dc1-ac8f-c0d8b3168284
2019-07-09 20:02:09,683   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:02:10,293   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:02:10,402   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:02:10,402   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:02:10,402   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:02:10,402   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:02:10,402   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:02:14,871   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2546.
2019-07-09 20:02:14,926   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:02:14,976   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:02:14,985   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:02:14,986   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:02:15,010   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-a6fe5b8f-fcad-4727-ab8d-35d6d607999e
2019-07-09 20:02:15,065   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:02:15,109   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:02:15,264   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17083ms
2019-07-09 20:02:15,342   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:02:15,358   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17187ms
2019-07-09 20:02:15,389   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:02:15,389   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:02:15,451   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,452   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,453   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,454   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,456   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,457   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,458   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,460   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,461   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,462   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,463   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,464   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,464   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,464   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,464   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,464   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,464   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,464   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,464   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,464   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,480   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,480   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,480   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,496   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,496   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:02:15,496   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:02:15,777   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:02:15,871   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2567.
2019-07-09 20:02:15,871   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2567
2019-07-09 20:02:15,871   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:02:15,917   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2567, None)
2019-07-09 20:02:15,917   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2567 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2567, None)
2019-07-09 20:02:15,933   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2567, None)
2019-07-09 20:02:15,933   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2567, None)
2019-07-09 20:02:16,183   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:02:17,121   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:02:17,261   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:02:17,261   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2567 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:02:17,277   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 20:02:17,480   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:02:17,620   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 20:02:17,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 20:02:17,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 20:02:17,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 20:02:17,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 20:02:17,714   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34), which has no missing parents
2019-07-09 20:02:17,808   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 20:02:17,855   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 20:02:17,855   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2567 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 20:02:17,855   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:02:17,902   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:02:17,902   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:02:18,027   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 20:02:18,042   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 20:02:18,074   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:02:18,074   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:02:18,901   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 20:02:18,901   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 20:02:18,995   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3894 bytes result sent to driver
2019-07-09 20:02:18,995   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 4010 bytes result sent to driver
2019-07-09 20:02:19,011   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 969 ms on localhost (executor driver) (1/2)
2019-07-09 20:02:19,011   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1016 ms on localhost (executor driver) (2/2)
2019-07-09 20:02:19,011   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:02:19,026   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.249 s
2019-07-09 20:02:19,042   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.420728 s
2019-07-09 20:02:19,073   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:02:19,073   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:02:19,104   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:02:19,167   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:02:19,167   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:02:19,183   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:02:19,183   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:02:19,198   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:02:19,198   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 20:02:19,214   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:02:19,214   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-8917a0bc-ef57-483b-b4e5-c9c5d46bbf9c
2019-07-09 20:04:01,479   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:04:02,151   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:04:02,292   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:04:02,292   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:04:02,292   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:04:02,292   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:04:02,292   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:04:06,385   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2651.
2019-07-09 20:04:06,432   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:04:06,463   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:04:06,479   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:04:06,479   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:04:06,495   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-4faf8bc5-7796-4134-9485-b47fcfc12290
2019-07-09 20:04:06,526   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:04:06,557   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:04:06,698   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16635ms
2019-07-09 20:04:06,791   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:04:06,807   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16738ms
2019-07-09 20:04:06,838   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@34d80665{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:04:06,838   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:04:06,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,885   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,916   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,916   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,916   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,916   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,932   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,932   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,932   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,932   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:04:06,948   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:04:07,291   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:04:07,385   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2672.
2019-07-09 20:04:07,385   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2672
2019-07-09 20:04:07,385   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:04:07,432   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2672, None)
2019-07-09 20:04:07,432   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2672 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2672, None)
2019-07-09 20:04:07,448   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2672, None)
2019-07-09 20:04:07,448   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2672, None)
2019-07-09 20:04:07,807   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:04:09,369   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:04:09,510   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:04:09,510   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2672 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:04:09,510   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:33
2019-07-09 20:04:09,698   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:04:09,823   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 20:04:09,869   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 20:04:09,869   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:42)
2019-07-09 20:04:09,869   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 20:04:09,869   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 20:04:09,885   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34), which has no missing parents
2019-07-09 20:04:09,963   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.7 KB, free 1426.3 MB)
2019-07-09 20:04:09,979   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 20:04:09,979   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2672 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 20:04:09,994   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:04:10,041   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:34) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:04:10,041   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:04:10,166   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7361 bytes)
2019-07-09 20:04:10,166   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7361 bytes)
2019-07-09 20:04:10,229   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:04:10,229   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:04:11,244   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 20:04:11,244   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 20:04:11,322   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 3902 bytes result sent to driver
2019-07-09 20:04:11,322   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 3780 bytes result sent to driver
2019-07-09 20:04:11,338   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1172 ms on localhost (executor driver) (1/2)
2019-07-09 20:04:11,338   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1234 ms on localhost (executor driver) (2/2)
2019-07-09 20:04:11,338   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:04:11,369   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:42) finished in 1.406 s
2019-07-09 20:04:11,385   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 1.565046 s
2019-07-09 20:04:11,416   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@34d80665{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:04:11,432   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:04:11,447   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:04:11,510   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:04:11,510   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:04:11,526   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:04:11,526   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:04:11,557   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:04:11,557   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 20:04:11,557   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:04:11,557   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-13ec8491-0514-47fd-94b2-adc3f53c1fb3
2019-07-09 20:06:09,681   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:06:10,291   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:06:10,400   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:06:10,400   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:06:10,400   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:06:10,400   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:06:10,400   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:06:14,275   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2712.
2019-07-09 20:06:14,322   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:06:14,353   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:06:14,353   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:06:14,353   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:06:14,525   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-28ff10da-4688-4267-9518-bda2f89a49be
2019-07-09 20:06:14,603   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:06:14,634   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:06:14,759   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16419ms
2019-07-09 20:06:14,837   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:06:14,853   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16518ms
2019-07-09 20:06:14,884   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@7328bf2d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:06:14,884   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:06:14,947   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,947   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,947   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,947   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,947   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,947   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,962   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,978   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,978   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,978   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,994   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,994   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,994   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 20:06:14,994   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:06:15,009   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:06:15,009   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:06:15,415   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:06:15,603   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2733.
2019-07-09 20:06:15,603   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2733
2019-07-09 20:06:15,603   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:06:15,665   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2733, None)
2019-07-09 20:06:15,681   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2733 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2733, None)
2019-07-09 20:06:15,697   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2733, None)
2019-07-09 20:06:15,697   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2733, None)
2019-07-09 20:06:16,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:06:17,244   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:06:17,400   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:06:17,415   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2733 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:06:17,415   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:34
2019-07-09 20:06:17,618   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:06:17,806   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:42
2019-07-09 20:06:17,868   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (groupBy at test.scala:36)
2019-07-09 20:06:17,884   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:42) with 2 output partitions
2019-07-09 20:06:17,884   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at test.scala:42)
2019-07-09 20:06:17,884   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-07-09 20:06:17,884   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-07-09 20:06:17,900   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:36), which has no missing parents
2019-07-09 20:06:18,025   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 6.4 KB, free 1426.3 MB)
2019-07-09 20:06:18,040   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:06:18,040   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2733 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:06:18,040   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:06:18,087   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:36) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:06:18,087   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:06:18,197   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:06:18,197   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:06:18,212   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:06:18,212   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:06:19,806   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 20:06:19,853   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 20:06:20,056   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 981 bytes result sent to driver
2019-07-09 20:06:20,056   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 981 bytes result sent to driver
2019-07-09 20:06:20,072   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1907 ms on localhost (executor driver) (1/2)
2019-07-09 20:06:20,072   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1875 ms on localhost (executor driver) (2/2)
2019-07-09 20:06:20,072   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:06:20,087   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (groupBy at test.scala:36) finished in 2.109 s
2019-07-09 20:06:20,087   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:06:20,087   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:06:20,087   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-07-09 20:06:20,087   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:06:20,103   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[6] at groupBy at test.scala:36), which has no missing parents
2019-07-09 20:06:20,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 7.4 KB, free 1426.3 MB)
2019-07-09 20:06:20,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 1426.3 MB)
2019-07-09 20:06:20,134   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:2733 (size: 3.7 KB, free: 1426.5 MB)
2019-07-09 20:06:20,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:06:20,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (ShuffledRDD[6] at groupBy at test.scala:36) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:06:20,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-07-09 20:06:20,150   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:06:20,150   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7141 bytes)
2019-07-09 20:06:20,150   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:06:20,150   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-07-09 20:06:20,212   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:06:20,212   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:06:20,212   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:06:20,212   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:06:20,290   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 7194 bytes result sent to driver
2019-07-09 20:06:20,290   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1830 bytes result sent to driver
2019-07-09 20:06:20,290   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 140 ms on localhost (executor driver) (1/2)
2019-07-09 20:06:20,306   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 156 ms on localhost (executor driver) (2/2)
2019-07-09 20:06:20,306   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:06:20,306   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at test.scala:42) finished in 0.188 s
2019-07-09 20:06:20,322   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:42, took 2.499562 s
2019-07-09 20:06:20,337   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@7328bf2d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:06:20,337   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:06:20,353   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:06:20,478   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:06:20,478   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:06:20,478   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:06:20,478   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:06:20,493   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:06:20,493   INFO --- [main]  WordCount$(line:51) : complete!
2019-07-09 20:06:20,509   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:06:20,509   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-ab18da01-f896-4bb0-916b-3564a678d9b0
2019-07-09 20:09:55,789   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:09:56,477   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:09:56,914   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:09:56,914   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:09:56,914   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:09:56,914   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:09:56,930   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:10:00,914   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2780.
2019-07-09 20:10:00,977   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:10:01,008   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:10:01,024   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:10:01,024   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:10:01,039   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-7dc34151-1e54-4bb6-91c3-7dc16078091b
2019-07-09 20:10:01,086   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:10:01,102   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:10:01,227   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16808ms
2019-07-09 20:10:01,321   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:10:01,336   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16910ms
2019-07-09 20:10:01,367   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@268ccd6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:10:01,367   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:10:01,414   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,414   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,414   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,430   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,430   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,430   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,430   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,430   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,430   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,430   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,430   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,446   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,461   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,461   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,461   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,477   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,477   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:10:01,477   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:10:01,758   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:10:01,852   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2802.
2019-07-09 20:10:01,852   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2802
2019-07-09 20:10:01,867   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:10:01,899   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2802, None)
2019-07-09 20:10:01,899   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2802 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2802, None)
2019-07-09 20:10:01,914   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2802, None)
2019-07-09 20:10:01,914   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2802, None)
2019-07-09 20:10:02,195   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:10:03,117   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:10:03,258   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:10:03,274   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2802 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:10:03,274   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:34
2019-07-09 20:10:03,508   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:10:03,742   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:38
2019-07-09 20:10:03,789   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (groupBy at test.scala:38)
2019-07-09 20:10:03,789   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:38) with 2 output partitions
2019-07-09 20:10:03,789   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at test.scala:38)
2019-07-09 20:10:03,805   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-07-09 20:10:03,805   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-07-09 20:10:03,820   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38), which has no missing parents
2019-07-09 20:10:03,945   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 6.4 KB, free 1426.3 MB)
2019-07-09 20:10:03,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:10:03,961   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2802 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:10:03,961   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:10:04,008   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:10:04,008   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:10:04,133   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:10:04,133   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:10:04,149   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:10:04,149   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:10:04,992   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 20:10:04,992   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 20:10:05,211   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 981 bytes result sent to driver
2019-07-09 20:10:05,211   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 981 bytes result sent to driver
2019-07-09 20:10:05,227   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1125 ms on localhost (executor driver) (1/2)
2019-07-09 20:10:05,242   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1109 ms on localhost (executor driver) (2/2)
2019-07-09 20:10:05,242   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:10:05,258   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (groupBy at test.scala:38) finished in 1.359 s
2019-07-09 20:10:05,258   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:10:05,258   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:10:05,258   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-07-09 20:10:05,258   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:10:05,274   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[6] at groupBy at test.scala:38), which has no missing parents
2019-07-09 20:10:05,289   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 7.4 KB, free 1426.3 MB)
2019-07-09 20:10:05,305   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.7 KB, free 1426.3 MB)
2019-07-09 20:10:05,305   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:2802 (size: 3.7 KB, free: 1426.5 MB)
2019-07-09 20:10:05,305   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:10:05,305   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (ShuffledRDD[6] at groupBy at test.scala:38) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:10:05,320   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-07-09 20:10:05,320   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:10:05,336   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7141 bytes)
2019-07-09 20:10:05,336   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:10:05,336   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-07-09 20:10:05,398   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:10:05,398   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:10:05,414   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 20:10:05,414   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 20:10:05,508   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1830 bytes result sent to driver
2019-07-09 20:10:05,508   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 7237 bytes result sent to driver
2019-07-09 20:10:05,508   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 188 ms on localhost (executor driver) (1/2)
2019-07-09 20:10:05,508   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 172 ms on localhost (executor driver) (2/2)
2019-07-09 20:10:05,523   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:10:05,523   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at test.scala:38) finished in 0.234 s
2019-07-09 20:10:05,539   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:38, took 1.798213 s
2019-07-09 20:10:05,602   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@268ccd6{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 1
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 11
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 12
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-07-09 20:10:05,617   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 6
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 18
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 22
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 17
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 7
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 23
2019-07-09 20:10:05,617   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 14
2019-07-09 20:10:05,648   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:2802 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:10:05,680   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:10:05,836   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:10:05,836   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:10:05,852   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:10:05,852   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:10:05,867   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:10:05,867   INFO --- [main]  WordCount$(line:54) : complete!
2019-07-09 20:10:05,883   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:10:05,883   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-bc27afc4-dc98-4acd-a7aa-61ae427d5fb7
2019-07-09 20:14:50,431   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:14:51,087   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:14:51,197   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:14:51,197   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:14:51,197   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:14:51,197   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:14:51,212   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:14:55,556   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2872.
2019-07-09 20:14:55,603   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:14:55,634   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:14:55,634   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:14:55,649   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:14:55,665   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-90785228-f93c-4697-92bb-a887452f4b70
2019-07-09 20:14:55,712   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:14:55,728   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:14:55,868   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17466ms
2019-07-09 20:14:55,946   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:14:55,978   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17577ms
2019-07-09 20:14:56,009   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:14:56,009   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:14:56,040   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,040   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,040   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,040   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,040   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,040   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,056   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,071   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,071   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,071   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,071   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,071   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,071   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,087   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,087   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,087   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:14:56,087   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:14:56,368   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:14:56,493   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2893.
2019-07-09 20:14:56,493   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2893
2019-07-09 20:14:56,493   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:14:56,540   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2893, None)
2019-07-09 20:14:56,540   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2893 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2893, None)
2019-07-09 20:14:56,540   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2893, None)
2019-07-09 20:14:56,540   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2893, None)
2019-07-09 20:14:56,790   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:14:57,837   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:14:58,056   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:14:58,056   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2893 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:14:58,071   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:34
2019-07-09 20:14:58,368   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:14:58,946   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: sortBy at test.scala:39
2019-07-09 20:14:59,009   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (groupBy at test.scala:38)
2019-07-09 20:14:59,009   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (sortBy at test.scala:39) with 2 output partitions
2019-07-09 20:14:59,009   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (sortBy at test.scala:39)
2019-07-09 20:14:59,009   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-07-09 20:14:59,024   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-07-09 20:14:59,024   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38), which has no missing parents
2019-07-09 20:14:59,196   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 6.4 KB, free 1426.3 MB)
2019-07-09 20:14:59,274   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:14:59,274   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2893 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:14:59,274   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:14:59,321   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:14:59,321   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:14:59,431   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:14:59,431   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:14:59,462   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:14:59,462   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:15:01,181   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 20:15:01,181   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 20:15:01,868   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 981 bytes result sent to driver
2019-07-09 20:15:01,868   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 981 bytes result sent to driver
2019-07-09 20:15:01,899   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2468 ms on localhost (executor driver) (1/2)
2019-07-09 20:15:01,899   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2500 ms on localhost (executor driver) (2/2)
2019-07-09 20:15:01,899   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:15:01,915   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (groupBy at test.scala:38) finished in 2.750 s
2019-07-09 20:15:01,915   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:15:01,915   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:15:01,915   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-07-09 20:15:01,915   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:15:01,931   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[9] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:15:01,977   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 8.9 KB, free 1426.3 MB)
2019-07-09 20:15:01,977   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1426.2 MB)
2019-07-09 20:15:01,977   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:2893 (size: 4.2 KB, free: 1426.5 MB)
2019-07-09 20:15:01,993   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:15:01,993   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:15:01,993   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-07-09 20:15:01,993   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:15:01,993   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7141 bytes)
2019-07-09 20:15:02,009   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:15:02,009   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-07-09 20:15:02,087   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:15:02,087   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:15:02,102   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:15:02,165   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 78 ms
2019-07-09 20:15:02,243   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 7466 bytes result sent to driver
2019-07-09 20:15:02,243   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 2083 bytes result sent to driver
2019-07-09 20:15:02,259   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 266 ms on localhost (executor driver) (1/2)
2019-07-09 20:15:02,259   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 266 ms on localhost (executor driver) (2/2)
2019-07-09 20:15:02,259   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:15:02,259   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (sortBy at test.scala:39) finished in 0.297 s
2019-07-09 20:15:02,274   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: sortBy at test.scala:39, took 3.322507 s
2019-07-09 20:15:02,321   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:45
2019-07-09 20:15:02,321   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:39)
2019-07-09 20:15:02,321   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at test.scala:45) with 2 output partitions
2019-07-09 20:15:02,321   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 4 (collect at test.scala:45)
2019-07-09 20:15:02,321   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 3)
2019-07-09 20:15:02,321   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 3)
2019-07-09 20:15:02,337   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 3 (MapPartitionsRDD[7] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:15:02,337   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 8.4 KB, free 1426.2 MB)
2019-07-09 20:15:02,368   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.3 KB, free 1426.2 MB)
2019-07-09 20:15:02,384   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:2893 (size: 4.3 KB, free: 1426.5 MB)
2019-07-09 20:15:02,384   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:15:02,399   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[7] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:15:02,399   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 2 tasks
2019-07-09 20:15:02,415   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:15:02,415   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 3.0 (TID 5, localhost, executor driver, partition 1, ANY, 7130 bytes)
2019-07-09 20:15:02,415   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 4)
2019-07-09 20:15:02,430   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 3.0 (TID 5)
2019-07-09 20:15:02,540   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:15:02,540   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:15:02,555   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:15:02,571   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 20:15:02,634   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 4). 1282 bytes result sent to driver
2019-07-09 20:15:02,634   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 4) in 219 ms on localhost (executor driver) (1/2)
2019-07-09 20:15:02,696   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 3.0 (TID 5). 1282 bytes result sent to driver
2019-07-09 20:15:02,696   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 3.0 (TID 5) in 281 ms on localhost (executor driver) (2/2)
2019-07-09 20:15:02,696   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-07-09 20:15:02,696   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 3 (sortBy at test.scala:39) finished in 0.359 s
2019-07-09 20:15:02,696   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:15:02,696   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:15:02,696   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 4)
2019-07-09 20:15:02,696   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:15:02,696   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 4 (MapPartitionsRDD[11] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:15:02,712   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 5.2 KB, free 1426.2 MB)
2019-07-09 20:15:02,712   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1426.2 MB)
2019-07-09 20:15:02,712   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:2893 (size: 2.9 KB, free: 1426.5 MB)
2019-07-09 20:15:02,712   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:15:02,712   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[11] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:15:02,712   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 4.0 with 2 tasks
2019-07-09 20:15:02,712   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 4.0 (TID 6, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:15:02,727   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 4.0 (TID 7, localhost, executor driver, partition 1, ANY, 7141 bytes)
2019-07-09 20:15:02,727   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 4.0 (TID 6)
2019-07-09 20:15:02,727   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 4.0 (TID 7)
2019-07-09 20:15:02,727   INFO --- [Executor task launch worker for task 6]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:15:02,727   INFO --- [Executor task launch worker for task 7]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:15:02,727   INFO --- [Executor task launch worker for task 6]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:15:02,727   INFO --- [Executor task launch worker for task 7]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:15:02,774   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 4.0 (TID 7). 6904 bytes result sent to driver
2019-07-09 20:15:02,774   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 4.0 (TID 6). 1997 bytes result sent to driver
2019-07-09 20:15:02,774   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 4.0 (TID 7) in 47 ms on localhost (executor driver) (1/2)
2019-07-09 20:15:02,774   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 4.0 (TID 6) in 62 ms on localhost (executor driver) (2/2)
2019-07-09 20:15:02,774   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2019-07-09 20:15:02,790   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 4 (collect at test.scala:45) finished in 0.094 s
2019-07-09 20:15:02,790   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at test.scala:45, took 0.466826 s
2019-07-09 20:15:02,805   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:15:02,821   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:15:02,837   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:15:03,009   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:15:03,009   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:15:03,024   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:15:03,024   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:15:03,040   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:15:03,040   INFO --- [main]  WordCount$(line:54) : complete!
2019-07-09 20:15:03,040   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:15:03,040   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-5611b60c-7fa4-4366-8407-ef3dfdb5b572
2019-07-09 20:17:49,943   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:17:50,553   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:17:50,662   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:17:50,662   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:17:50,662   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:17:50,662   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:17:50,662   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:17:54,474   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 2937.
2019-07-09 20:17:54,506   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:17:54,537   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:17:54,537   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:17:54,537   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:17:54,568   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-2593cbc8-bd42-4389-84ef-5db4e980c735
2019-07-09 20:17:54,615   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:17:54,631   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:17:54,771   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16221ms
2019-07-09 20:17:54,849   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:17:54,865   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16325ms
2019-07-09 20:17:54,912   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:17:54,912   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:17:54,943   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,943   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,959   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,974   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,974   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,974   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,974   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,974   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,974   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,974   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:17:54,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:17:55,005   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:17:55,005   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:17:55,271   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:17:55,365   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 2958.
2019-07-09 20:17:55,365   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:2958
2019-07-09 20:17:55,365   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:17:55,412   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 2958, None)
2019-07-09 20:17:55,412   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:2958 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 2958, None)
2019-07-09 20:17:55,412   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 2958, None)
2019-07-09 20:17:55,412   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 2958, None)
2019-07-09 20:17:55,677   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:17:56,630   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:17:56,787   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:17:56,802   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:2958 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:17:56,802   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:34
2019-07-09 20:17:57,021   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:17:57,318   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: sortBy at test.scala:39
2019-07-09 20:17:57,380   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (groupBy at test.scala:38)
2019-07-09 20:17:57,380   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (sortBy at test.scala:39) with 2 output partitions
2019-07-09 20:17:57,380   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (sortBy at test.scala:39)
2019-07-09 20:17:57,380   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-07-09 20:17:57,396   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-07-09 20:17:57,412   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38), which has no missing parents
2019-07-09 20:17:57,521   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 6.4 KB, free 1426.3 MB)
2019-07-09 20:17:57,568   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:17:57,568   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:2958 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:17:57,568   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:17:57,646   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:17:57,662   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:17:57,771   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:17:57,771   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:17:57,802   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:17:57,802   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:17:58,662   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:17:58,662   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:17:59,568   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 981 bytes result sent to driver
2019-07-09 20:17:59,568   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1024 bytes result sent to driver
2019-07-09 20:17:59,599   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1859 ms on localhost (executor driver) (1/2)
2019-07-09 20:17:59,599   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1828 ms on localhost (executor driver) (2/2)
2019-07-09 20:17:59,599   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:17:59,630   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (groupBy at test.scala:38) finished in 2.125 s
2019-07-09 20:17:59,630   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:17:59,630   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:17:59,630   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-07-09 20:17:59,646   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:17:59,646   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[9] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:17:59,693   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 8.9 KB, free 1426.3 MB)
2019-07-09 20:17:59,708   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1426.2 MB)
2019-07-09 20:17:59,708   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:2958 (size: 4.2 KB, free: 1426.5 MB)
2019-07-09 20:17:59,708   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:17:59,708   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:17:59,708   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-07-09 20:17:59,724   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:17:59,724   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7141 bytes)
2019-07-09 20:17:59,724   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:17:59,724   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-07-09 20:17:59,833   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:17:59,833   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:17:59,849   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 31 ms
2019-07-09 20:17:59,849   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 31 ms
2019-07-09 20:18:00,458   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 373147 bytes result sent to driver
2019-07-09 20:18:00,537   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 813 ms on localhost (executor driver) (1/2)
2019-07-09 20:18:00,912   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:2958 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:18:01,083   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_3 stored as bytes in memory (estimated size 5.2 MB, free 1421.1 MB)
2019-07-09 20:18:01,083   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_3 in memory on fc-pc:2958 (size: 5.2 MB, free: 1421.3 MB)
2019-07-09 20:18:01,083   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 5418761 bytes result sent via BlockManager)
2019-07-09 20:18:01,162   INFO --- [task-result-getter-3]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:2958 after 52 ms (0 ms spent in bootstraps)
2019-07-09 20:18:01,412   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 1688 ms on localhost (executor driver) (2/2)
2019-07-09 20:18:01,412   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:18:01,412   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (sortBy at test.scala:39) finished in 1.719 s
2019-07-09 20:18:01,412   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_3 on fc-pc:2958 in memory (size: 5.2 MB, free: 1426.5 MB)
2019-07-09 20:18:01,427   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: sortBy at test.scala:39, took 4.101895 s
2019-07-09 20:18:01,521   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:45
2019-07-09 20:18:01,521   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:39)
2019-07-09 20:18:01,521   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at test.scala:45) with 2 output partitions
2019-07-09 20:18:01,521   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 4 (collect at test.scala:45)
2019-07-09 20:18:01,521   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 3)
2019-07-09 20:18:01,521   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 3)
2019-07-09 20:18:01,521   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 3 (MapPartitionsRDD[7] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:18:01,552   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 369.0 KB, free 1425.9 MB)
2019-07-09 20:18:01,583   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 112.7 KB, free 1425.8 MB)
2019-07-09 20:18:01,583   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:2958 (size: 112.7 KB, free: 1426.4 MB)
2019-07-09 20:18:01,583   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:18:01,599   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[7] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:18:01,615   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 2 tasks
2019-07-09 20:18:01,615   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:18:01,615   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 3.0 (TID 5, localhost, executor driver, partition 1, ANY, 7130 bytes)
2019-07-09 20:18:01,615   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 4)
2019-07-09 20:18:01,615   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 3.0 (TID 5)
2019-07-09 20:18:01,693   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:18:01,693   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 20:18:01,693   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:18:01,693   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:18:01,958   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 4). 1282 bytes result sent to driver
2019-07-09 20:18:01,958   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 4) in 343 ms on localhost (executor driver) (1/2)
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 7
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 10
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 15
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 1
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 14
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 8
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 13
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-07-09 20:18:02,318   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 4
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 24
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 21
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 18
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 19
2019-07-09 20:18:02,333   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:2958 in memory (size: 4.2 KB, free: 1426.4 MB)
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 0
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 23
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 12
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 16
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 5
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 6
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 20
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 17
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 11
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 22
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 3
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 9
2019-07-09 20:18:02,333   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 2
2019-07-09 20:18:02,458   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 3.0 (TID 5). 1325 bytes result sent to driver
2019-07-09 20:18:02,458   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 3.0 (TID 5) in 843 ms on localhost (executor driver) (2/2)
2019-07-09 20:18:02,458   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-07-09 20:18:02,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 3 (sortBy at test.scala:39) finished in 0.937 s
2019-07-09 20:18:02,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:18:02,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:18:02,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 4)
2019-07-09 20:18:02,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:18:02,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 4 (MapPartitionsRDD[11] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:18:02,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 365.7 KB, free 1425.4 MB)
2019-07-09 20:18:02,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 111.2 KB, free 1425.3 MB)
2019-07-09 20:18:02,490   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:2958 (size: 111.2 KB, free: 1426.3 MB)
2019-07-09 20:18:02,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:18:02,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[11] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:18:02,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 4.0 with 2 tasks
2019-07-09 20:18:02,505   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 4.0 (TID 6, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:18:02,505   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 4.0 (TID 7, localhost, executor driver, partition 1, ANY, 7141 bytes)
2019-07-09 20:18:02,505   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 4.0 (TID 6)
2019-07-09 20:18:02,505   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 4.0 (TID 7)
2019-07-09 20:18:02,521   INFO --- [Executor task launch worker for task 6]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:18:02,521   INFO --- [Executor task launch worker for task 6]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:18:02,521   INFO --- [Executor task launch worker for task 7]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:18:02,521   INFO --- [Executor task launch worker for task 7]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:18:02,661   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 4.0 (TID 6). 745532 bytes result sent to driver
2019-07-09 20:18:02,677   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 4.0 (TID 6) in 172 ms on localhost (executor driver) (1/2)
2019-07-09 20:18:02,740   INFO --- [Executor task launch worker for task 7]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_7 stored as bytes in memory (estimated size 4.8 MB, free 1420.5 MB)
2019-07-09 20:18:02,755   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_7 in memory on fc-pc:2958 (size: 4.8 MB, free: 1421.4 MB)
2019-07-09 20:18:02,755   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 4.0 (TID 7). 5045773 bytes result sent via BlockManager)
2019-07-09 20:18:02,833   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 4.0 (TID 7) in 328 ms on localhost (executor driver) (2/2)
2019-07-09 20:18:02,833   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2019-07-09 20:18:02,849   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_7 on fc-pc:2958 in memory (size: 4.8 MB, free: 1426.3 MB)
2019-07-09 20:18:02,849   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 4 (collect at test.scala:45) finished in 0.375 s
2019-07-09 20:18:02,849   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at test.scala:45, took 1.326799 s
2019-07-09 20:18:03,193   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_4_piece0 on fc-pc:2958 in memory (size: 111.2 KB, free: 1426.4 MB)
2019-07-09 20:18:03,490   INFO --- [main]  org.apache.hadoop.conf.Configuration.deprecation(line:1129) : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-07-09 20:18:03,521   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 20:18:03,599   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-07-09 20:18:03,599   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions
2019-07-09 20:18:03,599   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 7 (runJob at SparkHadoopWriter.scala:78)
2019-07-09 20:18:03,599   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 6)
2019-07-09 20:18:03,599   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 20:18:03,615   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 7 (MapPartitionsRDD[12] at saveAsTextFile at test.scala:46), which has no missing parents
2019-07-09 20:18:03,755   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 428.0 KB, free 1425.4 MB)
2019-07-09 20:18:03,833   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 133.5 KB, free 1425.3 MB)
2019-07-09 20:18:03,849   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on fc-pc:2958 (size: 133.5 KB, free: 1426.2 MB)
2019-07-09 20:18:03,849   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:18:03,849   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[12] at saveAsTextFile at test.scala:46) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:18:03,849   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 7.0 with 2 tasks
2019-07-09 20:18:03,865   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 7.0 (TID 8, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:18:03,865   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 7.0 (TID 9, localhost, executor driver, partition 1, ANY, 7141 bytes)
2019-07-09 20:18:03,865   INFO --- [Executor task launch worker for task 8]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 7.0 (TID 8)
2019-07-09 20:18:03,865   INFO --- [Executor task launch worker for task 9]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 7.0 (TID 9)
2019-07-09 20:18:03,974   INFO --- [Executor task launch worker for task 9]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:18:03,974   INFO --- [Executor task launch worker for task 9]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:18:03,974   INFO --- [Executor task launch worker for task 8]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:18:03,974   INFO --- [Executor task launch worker for task 8]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:18:04,036   INFO --- [Executor task launch worker for task 8]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 20:18:04,099   INFO --- [Executor task launch worker for task 9]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 20:18:04,380   INFO --- [Executor task launch worker for task 8]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190709201803_0012_m_000000_0' to file:/D:/data/output/_temporary/0/task_20190709201803_0012_m_000000
2019-07-09 20:18:04,380   INFO --- [Executor task launch worker for task 8]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190709201803_0012_m_000000_0: Committed
2019-07-09 20:18:04,396   INFO --- [Executor task launch worker for task 8]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 7.0 (TID 8). 1422 bytes result sent to driver
2019-07-09 20:18:04,396   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 7.0 (TID 8) in 531 ms on localhost (executor driver) (1/2)
2019-07-09 20:18:04,443   INFO --- [Executor task launch worker for task 9]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190709201803_0012_m_000001_0' to file:/D:/data/output/_temporary/0/task_20190709201803_0012_m_000001
2019-07-09 20:18:04,443   INFO --- [Executor task launch worker for task 9]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190709201803_0012_m_000001_0: Committed
2019-07-09 20:18:04,443   INFO --- [Executor task launch worker for task 9]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 7.0 (TID 9). 1422 bytes result sent to driver
2019-07-09 20:18:04,443   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 7.0 (TID 9) in 578 ms on localhost (executor driver) (2/2)
2019-07-09 20:18:04,443   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 7.0, whose tasks have all completed, from pool 
2019-07-09 20:18:04,458   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 7 (runJob at SparkHadoopWriter.scala:78) finished in 0.828 s
2019-07-09 20:18:04,458   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: runJob at SparkHadoopWriter.scala:78, took 0.857084 s
2019-07-09 20:18:04,536   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190709201803_0012 committed.
2019-07-09 20:18:04,552   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:18:04,552   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:18:04,615   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:18:04,818   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:18:04,818   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:18:04,818   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:18:04,833   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:18:04,849   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:18:04,849   INFO --- [main]  WordCount$(line:54) : complete!
2019-07-09 20:18:04,849   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:18:04,849   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-fb89abbd-1bf9-4e5b-9f92-a6877ee50cd7
2019-07-09 20:22:48,561   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:22:49,217   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:22:49,326   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:22:49,326   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:22:49,326   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:22:49,326   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:22:49,326   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:22:53,123   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3014.
2019-07-09 20:22:53,170   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:22:53,201   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:22:53,201   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:22:53,201   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:22:53,217   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-b0454b63-6de5-4a2b-b423-c4253ba2e93d
2019-07-09 20:22:53,264   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:22:53,279   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:22:53,420   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16314ms
2019-07-09 20:22:53,498   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:22:53,514   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16422ms
2019-07-09 20:22:53,545   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@10fd85d1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:22:53,545   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:22:53,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,607   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,623   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,623   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,623   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,623   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,623   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,639   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:22:53,639   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:22:53,904   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:22:54,014   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3035.
2019-07-09 20:22:54,014   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3035
2019-07-09 20:22:54,014   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:22:54,061   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3035, None)
2019-07-09 20:22:54,061   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3035 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3035, None)
2019-07-09 20:22:54,061   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3035, None)
2019-07-09 20:22:54,076   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3035, None)
2019-07-09 20:22:54,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:22:55,217   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:22:55,357   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:22:55,357   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3035 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:22:55,373   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:34
2019-07-09 20:22:55,607   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:22:55,873   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: sortBy at test.scala:39
2019-07-09 20:22:55,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (groupBy at test.scala:38)
2019-07-09 20:22:55,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (sortBy at test.scala:39) with 2 output partitions
2019-07-09 20:22:55,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (sortBy at test.scala:39)
2019-07-09 20:22:55,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-07-09 20:22:55,951   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-07-09 20:22:55,967   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38), which has no missing parents
2019-07-09 20:22:56,061   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 6.4 KB, free 1426.3 MB)
2019-07-09 20:22:56,076   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:22:56,076   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3035 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:22:56,076   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:22:56,123   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:22:56,123   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:22:56,248   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:22:56,248   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:22:56,279   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:22:56,279   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:22:57,232   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:22:57,232   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:22:58,310   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1024 bytes result sent to driver
2019-07-09 20:22:58,326   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1024 bytes result sent to driver
2019-07-09 20:22:58,342   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2094 ms on localhost (executor driver) (1/2)
2019-07-09 20:22:58,342   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2125 ms on localhost (executor driver) (2/2)
2019-07-09 20:22:58,342   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:22:58,373   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (groupBy at test.scala:38) finished in 2.312 s
2019-07-09 20:22:58,373   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:22:58,373   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:22:58,373   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-07-09 20:22:58,373   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:22:58,373   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[9] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:22:58,404   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 8.9 KB, free 1426.3 MB)
2019-07-09 20:22:58,404   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1426.2 MB)
2019-07-09 20:22:58,404   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3035 (size: 4.2 KB, free: 1426.5 MB)
2019-07-09 20:22:58,420   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:22:58,420   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:22:58,420   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-07-09 20:22:58,420   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:22:58,420   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7141 bytes)
2019-07-09 20:22:58,435   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:22:58,435   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-07-09 20:22:58,529   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:22:58,529   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:22:58,529   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:22:58,545   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 31 ms
2019-07-09 20:22:59,170   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 373147 bytes result sent to driver
2019-07-09 20:22:59,217   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 797 ms on localhost (executor driver) (1/2)
2019-07-09 20:22:59,529   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3035 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:22:59,732   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_3 stored as bytes in memory (estimated size 5.2 MB, free 1421.1 MB)
2019-07-09 20:22:59,732   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_3 in memory on fc-pc:3035 (size: 5.2 MB, free: 1421.3 MB)
2019-07-09 20:22:59,732   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 5418761 bytes result sent via BlockManager)
2019-07-09 20:22:59,842   INFO --- [task-result-getter-3]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:3035 after 77 ms (0 ms spent in bootstraps)
2019-07-09 20:23:00,107   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 1687 ms on localhost (executor driver) (2/2)
2019-07-09 20:23:00,123   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:23:00,123   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (sortBy at test.scala:39) finished in 1.719 s
2019-07-09 20:23:00,123   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_3 on fc-pc:3035 in memory (size: 5.2 MB, free: 1426.5 MB)
2019-07-09 20:23:00,138   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: sortBy at test.scala:39, took 4.252237 s
2019-07-09 20:23:00,185   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:48
2019-07-09 20:23:00,185   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:39)
2019-07-09 20:23:00,201   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 12 (map at test.scala:40)
2019-07-09 20:23:00,201   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at test.scala:48) with 1 output partitions
2019-07-09 20:23:00,201   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (collect at test.scala:48)
2019-07-09 20:23:00,201   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 4)
2019-07-09 20:23:00,201   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 4)
2019-07-09 20:23:00,201   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 3 (MapPartitionsRDD[7] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:23:00,248   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 369.0 KB, free 1425.9 MB)
2019-07-09 20:23:00,263   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 112.7 KB, free 1425.8 MB)
2019-07-09 20:23:00,263   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3035 (size: 112.7 KB, free: 1426.4 MB)
2019-07-09 20:23:00,263   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:23:00,263   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[7] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:23:00,263   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 2 tasks
2019-07-09 20:23:00,263   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:23:00,279   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 3.0 (TID 5, localhost, executor driver, partition 1, ANY, 7130 bytes)
2019-07-09 20:23:00,279   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 4)
2019-07-09 20:23:00,279   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 3.0 (TID 5)
2019-07-09 20:23:00,310   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:23:00,310   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:23:00,373   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:23:00,373   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:23:00,654   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 4). 1282 bytes result sent to driver
2019-07-09 20:23:00,670   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 4) in 407 ms on localhost (executor driver) (1/2)
2019-07-09 20:23:01,045   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 3.0 (TID 5). 1325 bytes result sent to driver
2019-07-09 20:23:01,045   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 3.0 (TID 5) in 766 ms on localhost (executor driver) (2/2)
2019-07-09 20:23:01,045   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-07-09 20:23:01,045   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 3 (sortBy at test.scala:39) finished in 0.844 s
2019-07-09 20:23:01,045   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:23:01,045   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:23:01,045   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 5, ShuffleMapStage 4)
2019-07-09 20:23:01,045   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:23:01,045   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at map at test.scala:40), which has no missing parents
2019-07-09 20:23:01,060   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 365.9 KB, free 1425.4 MB)
2019-07-09 20:23:01,076   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 111.4 KB, free 1425.3 MB)
2019-07-09 20:23:01,076   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:3035 (size: 111.4 KB, free: 1426.3 MB)
2019-07-09 20:23:01,076   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:23:01,076   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at map at test.scala:40) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:23:01,076   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 4.0 with 2 tasks
2019-07-09 20:23:01,076   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 4.0 (TID 6, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:23:01,076   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 4.0 (TID 7, localhost, executor driver, partition 1, ANY, 7130 bytes)
2019-07-09 20:23:01,076   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 4.0 (TID 6)
2019-07-09 20:23:01,076   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 4.0 (TID 7)
2019-07-09 20:23:01,092   INFO --- [Executor task launch worker for task 7]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:23:01,092   INFO --- [Executor task launch worker for task 7]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:23:01,092   INFO --- [Executor task launch worker for task 6]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:23:01,092   INFO --- [Executor task launch worker for task 6]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:23:01,326   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 4.0 (TID 6). 1281 bytes result sent to driver
2019-07-09 20:23:01,326   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 4.0 (TID 6) in 250 ms on localhost (executor driver) (1/2)
2019-07-09 20:23:01,467   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 4.0 (TID 7). 1281 bytes result sent to driver
2019-07-09 20:23:01,467   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 4.0 (TID 7) in 391 ms on localhost (executor driver) (2/2)
2019-07-09 20:23:01,467   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2019-07-09 20:23:01,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 4 (map at test.scala:40) finished in 0.422 s
2019-07-09 20:23:01,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:23:01,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:23:01,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 5)
2019-07-09 20:23:01,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:23:01,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (ShuffledRDD[13] at reduceByKey at test.scala:41), which has no missing parents
2019-07-09 20:23:01,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 3.5 KB, free 1425.3 MB)
2019-07-09 20:23:01,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1425.3 MB)
2019-07-09 20:23:01,592   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on fc-pc:3035 (size: 2.1 KB, free: 1426.3 MB)
2019-07-09 20:23:01,592   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:23:01,592   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[13] at reduceByKey at test.scala:41) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:23:01,592   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-07-09 20:23:01,592   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 8, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:23:01,607   INFO --- [Executor task launch worker for task 8]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 8)
2019-07-09 20:23:01,607   INFO --- [Executor task launch worker for task 8]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:23:01,607   INFO --- [Executor task launch worker for task 8]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:23:01,857   INFO --- [Executor task launch worker for task 8]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_8 stored as bytes in memory (estimated size 5.5 MB, free 1419.8 MB)
2019-07-09 20:23:01,857   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_8 in memory on fc-pc:3035 (size: 5.5 MB, free: 1420.7 MB)
2019-07-09 20:23:01,857   INFO --- [Executor task launch worker for task 8]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 8). 5789931 bytes result sent via BlockManager)
2019-07-09 20:23:01,920   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_4_piece0 on fc-pc:3035 in memory (size: 111.4 KB, free: 1420.8 MB)
2019-07-09 20:23:01,998   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 8) in 406 ms on localhost (executor driver) (1/1)
2019-07-09 20:23:01,998   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-07-09 20:23:01,998   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (collect at test.scala:48) finished in 0.531 s
2019-07-09 20:23:01,998   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_8 on fc-pc:3035 in memory (size: 5.5 MB, free: 1426.4 MB)
2019-07-09 20:23:01,998   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at test.scala:48, took 1.812311 s
2019-07-09 20:23:02,435   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-07-09 20:23:02,435   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@10fd85d1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:23:02,451   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:23:02,467   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:23:02,779   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:23:02,779   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:23:02,779   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:23:02,779   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:23:02,795   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:23:02,795   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:23:02,795   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-e21759ea-dca0-46e2-8b98-9c21687f462f
2019-07-09 20:23:43,730   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:23:44,371   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:23:44,465   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:23:44,480   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:23:44,480   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:23:44,480   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:23:44,480   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:23:48,324   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3068.
2019-07-09 20:23:48,355   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:23:48,386   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:23:48,386   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:23:48,386   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:23:48,402   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-1ea01cea-3c19-4329-ad38-c9b75627c396
2019-07-09 20:23:48,449   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:23:48,465   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:23:48,621   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16388ms
2019-07-09 20:23:48,699   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:23:48,715   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16491ms
2019-07-09 20:23:48,761   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@5f082d1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:23:48,761   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:23:48,824   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,824   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,824   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,824   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,824   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,839   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,839   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,839   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,855   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,855   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,855   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,886   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,902   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,902   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,902   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,918   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,918   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:23:48,918   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:23:49,152   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:23:49,261   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3089.
2019-07-09 20:23:49,261   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3089
2019-07-09 20:23:49,261   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:23:49,293   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3089, None)
2019-07-09 20:23:49,308   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3089 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3089, None)
2019-07-09 20:23:49,308   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3089, None)
2019-07-09 20:23:49,308   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3089, None)
2019-07-09 20:23:49,558   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:23:50,543   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:23:50,683   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:23:50,699   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3089 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:23:50,699   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:34
2019-07-09 20:23:50,902   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:23:51,183   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: sortBy at test.scala:39
2019-07-09 20:23:51,230   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (groupBy at test.scala:38)
2019-07-09 20:23:51,230   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (sortBy at test.scala:39) with 2 output partitions
2019-07-09 20:23:51,246   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (sortBy at test.scala:39)
2019-07-09 20:23:51,246   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-07-09 20:23:51,246   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-07-09 20:23:51,261   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38), which has no missing parents
2019-07-09 20:23:51,355   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 6.4 KB, free 1426.3 MB)
2019-07-09 20:23:51,386   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:23:51,386   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3089 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:23:51,386   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:23:51,449   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:23:51,449   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:23:51,574   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:23:51,589   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:23:51,636   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:23:51,652   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:23:52,464   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:23:52,464   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:23:53,308   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 981 bytes result sent to driver
2019-07-09 20:23:53,308   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 981 bytes result sent to driver
2019-07-09 20:23:53,324   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1781 ms on localhost (executor driver) (1/2)
2019-07-09 20:23:53,339   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1750 ms on localhost (executor driver) (2/2)
2019-07-09 20:23:53,339   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:23:53,355   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (groupBy at test.scala:38) finished in 2.016 s
2019-07-09 20:23:53,355   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:23:53,355   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:23:53,355   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-07-09 20:23:53,355   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:23:53,371   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[9] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:23:53,386   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 8.9 KB, free 1426.3 MB)
2019-07-09 20:23:53,386   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1426.2 MB)
2019-07-09 20:23:53,402   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3089 (size: 4.2 KB, free: 1426.5 MB)
2019-07-09 20:23:53,402   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:23:53,402   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:23:53,402   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-07-09 20:23:53,417   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:23:53,417   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7141 bytes)
2019-07-09 20:23:53,417   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-07-09 20:23:53,417   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:23:53,511   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:23:53,511   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:23:53,511   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:23:53,511   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:23:54,121   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 373147 bytes result sent to driver
2019-07-09 20:23:54,183   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 766 ms on localhost (executor driver) (1/2)
2019-07-09 20:23:54,667   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_3 stored as bytes in memory (estimated size 5.2 MB, free 1421.1 MB)
2019-07-09 20:23:54,667   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_3 in memory on fc-pc:3089 (size: 5.2 MB, free: 1421.3 MB)
2019-07-09 20:23:54,667   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 5418761 bytes result sent via BlockManager)
2019-07-09 20:23:54,761   INFO --- [task-result-getter-3]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:3089 after 51 ms (0 ms spent in bootstraps)
2019-07-09 20:23:54,996   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 1579 ms on localhost (executor driver) (2/2)
2019-07-09 20:23:54,996   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:23:55,011   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (sortBy at test.scala:39) finished in 1.625 s
2019-07-09 20:23:55,011   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_3 on fc-pc:3089 in memory (size: 5.2 MB, free: 1426.5 MB)
2019-07-09 20:23:55,027   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: sortBy at test.scala:39, took 3.841562 s
2019-07-09 20:23:55,074   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:48
2019-07-09 20:23:55,089   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:39)
2019-07-09 20:23:55,089   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 12 (map at test.scala:40)
2019-07-09 20:23:55,089   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at test.scala:48) with 1 output partitions
2019-07-09 20:23:55,089   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (collect at test.scala:48)
2019-07-09 20:23:55,089   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 4)
2019-07-09 20:23:55,089   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 4)
2019-07-09 20:23:55,089   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 3 (MapPartitionsRDD[7] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:23:55,152   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 369.0 KB, free 1425.9 MB)
2019-07-09 20:23:55,152   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 112.7 KB, free 1425.8 MB)
2019-07-09 20:23:55,152   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3089 (size: 112.7 KB, free: 1426.4 MB)
2019-07-09 20:23:55,152   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:23:55,152   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[7] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:23:55,152   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 2 tasks
2019-07-09 20:23:55,167   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:23:55,167   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 3.0 (TID 5, localhost, executor driver, partition 1, ANY, 7130 bytes)
2019-07-09 20:23:55,167   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 4)
2019-07-09 20:23:55,167   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 3.0 (TID 5)
2019-07-09 20:23:55,246   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:23:55,246   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:23:55,246   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:23:55,246   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:23:55,449   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 4). 1282 bytes result sent to driver
2019-07-09 20:23:55,449   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 4) in 282 ms on localhost (executor driver) (1/2)
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 21
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 10
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 7
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-07-09 20:23:55,730   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 3
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 18
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 5
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 19
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 4
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 15
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 11
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 12
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 9
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 20
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 13
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 1
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 8
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 16
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 14
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 23
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 17
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 22
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 6
2019-07-09 20:23:55,746   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-07-09 20:23:55,777   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:3089 in memory (size: 4.2 KB, free: 1426.4 MB)
2019-07-09 20:23:55,792   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-07-09 20:23:55,792   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 2
2019-07-09 20:23:55,792   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-07-09 20:23:55,792   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 0
2019-07-09 20:23:55,792   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-09 20:23:55,792   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 24
2019-07-09 20:23:55,808   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3089 in memory (size: 3.3 KB, free: 1426.4 MB)
2019-07-09 20:23:55,933   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 3.0 (TID 5). 1325 bytes result sent to driver
2019-07-09 20:23:55,949   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 3.0 (TID 5) in 782 ms on localhost (executor driver) (2/2)
2019-07-09 20:23:55,949   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-07-09 20:23:55,949   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 3 (sortBy at test.scala:39) finished in 0.844 s
2019-07-09 20:23:55,949   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:23:55,949   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:23:55,949   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 5, ShuffleMapStage 4)
2019-07-09 20:23:55,949   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:23:55,949   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at map at test.scala:40), which has no missing parents
2019-07-09 20:23:55,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 365.9 KB, free 1425.4 MB)
2019-07-09 20:23:55,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 111.4 KB, free 1425.3 MB)
2019-07-09 20:23:55,964   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:3089 (size: 111.4 KB, free: 1426.3 MB)
2019-07-09 20:23:55,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:23:55,980   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at map at test.scala:40) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:23:55,980   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 4.0 with 2 tasks
2019-07-09 20:23:55,980   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 4.0 (TID 6, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:23:55,980   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 4.0 (TID 7, localhost, executor driver, partition 1, ANY, 7130 bytes)
2019-07-09 20:23:55,980   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 4.0 (TID 7)
2019-07-09 20:23:55,980   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 4.0 (TID 6)
2019-07-09 20:23:55,995   INFO --- [Executor task launch worker for task 6]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:23:56,011   INFO --- [Executor task launch worker for task 6]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 20:23:56,011   INFO --- [Executor task launch worker for task 7]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:23:56,011   INFO --- [Executor task launch worker for task 7]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:23:56,183   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 4.0 (TID 6). 1281 bytes result sent to driver
2019-07-09 20:23:56,183   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 4.0 (TID 6) in 203 ms on localhost (executor driver) (1/2)
2019-07-09 20:23:56,292   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 4.0 (TID 7). 1281 bytes result sent to driver
2019-07-09 20:23:56,292   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 4.0 (TID 7) in 312 ms on localhost (executor driver) (2/2)
2019-07-09 20:23:56,292   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2019-07-09 20:23:56,292   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 4 (map at test.scala:40) finished in 0.343 s
2019-07-09 20:23:56,292   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:23:56,292   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:23:56,292   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 5)
2019-07-09 20:23:56,292   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:23:56,292   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (ShuffledRDD[13] at reduceByKey at test.scala:41), which has no missing parents
2019-07-09 20:23:56,308   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 3.5 KB, free 1425.3 MB)
2019-07-09 20:23:56,308   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1425.3 MB)
2019-07-09 20:23:56,308   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on fc-pc:3089 (size: 2.1 KB, free: 1426.3 MB)
2019-07-09 20:23:56,308   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:23:56,308   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[13] at reduceByKey at test.scala:41) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:23:56,308   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-07-09 20:23:56,308   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 8, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:23:56,308   INFO --- [Executor task launch worker for task 8]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 8)
2019-07-09 20:23:56,324   INFO --- [Executor task launch worker for task 8]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:23:56,324   INFO --- [Executor task launch worker for task 8]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:23:56,527   INFO --- [Executor task launch worker for task 8]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_8 stored as bytes in memory (estimated size 5.5 MB, free 1419.8 MB)
2019-07-09 20:23:56,527   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_8 in memory on fc-pc:3089 (size: 5.5 MB, free: 1420.7 MB)
2019-07-09 20:23:56,527   INFO --- [Executor task launch worker for task 8]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 8). 5790017 bytes result sent via BlockManager)
2019-07-09 20:23:56,605   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_4_piece0 on fc-pc:3089 in memory (size: 111.4 KB, free: 1420.8 MB)
2019-07-09 20:23:56,667   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 8) in 359 ms on localhost (executor driver) (1/1)
2019-07-09 20:23:56,667   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-07-09 20:23:56,667   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_8 on fc-pc:3089 in memory (size: 5.5 MB, free: 1426.4 MB)
2019-07-09 20:23:56,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (collect at test.scala:48) finished in 0.375 s
2019-07-09 20:23:56,667   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at test.scala:48, took 1.591127 s
2019-07-09 20:23:57,214   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-07-09 20:23:57,214   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@5f082d1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:23:57,230   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:23:57,245   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:23:57,527   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:23:57,527   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:23:57,527   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:23:57,527   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:23:57,542   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:23:57,542   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:23:57,542   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-9405cf11-6383-468b-b885-6ccfab6e33a4
2019-07-09 20:24:27,256   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:24:27,881   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:24:27,991   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:24:27,991   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:24:27,991   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:24:27,991   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:24:27,991   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:24:31,834   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3116.
2019-07-09 20:24:31,866   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:24:31,912   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:24:31,912   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:24:31,912   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:24:31,928   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-be412c3e-ae35-41cc-af9f-60b59176302b
2019-07-09 20:24:31,975   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:24:31,991   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:24:32,116   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16492ms
2019-07-09 20:24:32,194   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:24:32,225   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16590ms
2019-07-09 20:24:32,241   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:24:32,256   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:24:32,287   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,287   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,287   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,287   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,287   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,287   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,303   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,319   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,319   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,319   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,334   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,334   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,334   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,334   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,334   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:24:32,334   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:24:32,631   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:24:32,725   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3138.
2019-07-09 20:24:32,741   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3138
2019-07-09 20:24:32,741   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:24:32,772   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3138, None)
2019-07-09 20:24:32,787   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3138 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3138, None)
2019-07-09 20:24:32,787   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3138, None)
2019-07-09 20:24:32,787   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3138, None)
2019-07-09 20:24:33,053   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:24:33,819   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:24:33,991   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:24:33,991   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3138 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:24:34,006   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:34
2019-07-09 20:24:34,209   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:24:34,444   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: sortBy at test.scala:39
2019-07-09 20:24:34,491   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (groupBy at test.scala:38)
2019-07-09 20:24:34,506   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (sortBy at test.scala:39) with 2 output partitions
2019-07-09 20:24:34,506   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (sortBy at test.scala:39)
2019-07-09 20:24:34,506   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-07-09 20:24:34,506   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-07-09 20:24:34,537   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38), which has no missing parents
2019-07-09 20:24:34,678   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 6.4 KB, free 1426.3 MB)
2019-07-09 20:24:34,694   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:24:34,694   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3138 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:24:34,694   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:24:34,741   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at groupBy at test.scala:38) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:24:34,741   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:24:34,850   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:24:34,850   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:24:34,881   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:24:34,881   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:24:35,787   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:24:35,787   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:24:36,678   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 981 bytes result sent to driver
2019-07-09 20:24:36,678   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 981 bytes result sent to driver
2019-07-09 20:24:36,694   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1844 ms on localhost (executor driver) (1/2)
2019-07-09 20:24:36,694   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1875 ms on localhost (executor driver) (2/2)
2019-07-09 20:24:36,694   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:24:36,709   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (groupBy at test.scala:38) finished in 2.062 s
2019-07-09 20:24:36,709   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:24:36,725   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:24:36,725   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-07-09 20:24:36,725   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:24:36,725   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[9] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:24:36,756   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 8.9 KB, free 1426.3 MB)
2019-07-09 20:24:36,756   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1426.2 MB)
2019-07-09 20:24:36,756   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3138 (size: 4.2 KB, free: 1426.5 MB)
2019-07-09 20:24:36,756   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:24:36,772   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:24:36,772   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-07-09 20:24:36,772   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:24:36,772   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 7141 bytes)
2019-07-09 20:24:36,772   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:24:36,772   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-07-09 20:24:36,865   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:24:36,865   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:24:36,865   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:24:36,865   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:24:37,537   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 373147 bytes result sent to driver
2019-07-09 20:24:37,584   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 812 ms on localhost (executor driver) (1/2)
2019-07-09 20:24:37,959   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3138 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:24:38,069   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_3 stored as bytes in memory (estimated size 5.2 MB, free 1421.1 MB)
2019-07-09 20:24:38,069   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_3 in memory on fc-pc:3138 (size: 5.2 MB, free: 1421.3 MB)
2019-07-09 20:24:38,084   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 5418761 bytes result sent via BlockManager)
2019-07-09 20:24:38,178   INFO --- [task-result-getter-3]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:3138 after 61 ms (0 ms spent in bootstraps)
2019-07-09 20:24:38,428   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 1656 ms on localhost (executor driver) (2/2)
2019-07-09 20:24:38,428   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:24:38,428   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (sortBy at test.scala:39) finished in 1.672 s
2019-07-09 20:24:38,444   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_3 on fc-pc:3138 in memory (size: 5.2 MB, free: 1426.5 MB)
2019-07-09 20:24:38,444   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: sortBy at test.scala:39, took 4.003529 s
2019-07-09 20:24:38,506   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:48
2019-07-09 20:24:38,522   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:39)
2019-07-09 20:24:38,522   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 12 (map at test.scala:40)
2019-07-09 20:24:38,522   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at test.scala:48) with 1 output partitions
2019-07-09 20:24:38,522   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (collect at test.scala:48)
2019-07-09 20:24:38,522   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 4)
2019-07-09 20:24:38,522   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 4)
2019-07-09 20:24:38,522   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 3 (MapPartitionsRDD[7] at sortBy at test.scala:39), which has no missing parents
2019-07-09 20:24:38,615   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 369.0 KB, free 1425.9 MB)
2019-07-09 20:24:38,615   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 112.7 KB, free 1425.8 MB)
2019-07-09 20:24:38,631   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3138 (size: 112.7 KB, free: 1426.4 MB)
2019-07-09 20:24:38,631   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:24:38,631   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[7] at sortBy at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:24:38,631   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 2 tasks
2019-07-09 20:24:38,631   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 4, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:24:38,631   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 3.0 (TID 5, localhost, executor driver, partition 1, ANY, 7130 bytes)
2019-07-09 20:24:38,631   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 4)
2019-07-09 20:24:38,631   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 3.0 (TID 5)
2019-07-09 20:24:38,709   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:24:38,709   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:24:38,725   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:24:38,725   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:24:39,022   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 4). 1282 bytes result sent to driver
2019-07-09 20:24:39,022   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 4) in 391 ms on localhost (executor driver) (1/2)
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 24
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-07-09 20:24:39,397   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-07-09 20:24:39,397   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:3138 in memory (size: 4.2 KB, free: 1426.4 MB)
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 14
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 16
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 7
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 11
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 23
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 20
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 12
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 13
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 19
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 2
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 5
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 17
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 15
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 4
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 10
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 0
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 18
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 21
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 9
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 8
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 6
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 3
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 22
2019-07-09 20:24:39,412   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 1
2019-07-09 20:24:39,490   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 3.0 (TID 5). 1325 bytes result sent to driver
2019-07-09 20:24:39,490   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 3.0 (TID 5) in 859 ms on localhost (executor driver) (2/2)
2019-07-09 20:24:39,490   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-07-09 20:24:39,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 3 (sortBy at test.scala:39) finished in 0.953 s
2019-07-09 20:24:39,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:24:39,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:24:39,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 5, ShuffleMapStage 4)
2019-07-09 20:24:39,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:24:39,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at map at test.scala:40), which has no missing parents
2019-07-09 20:24:39,506   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 365.9 KB, free 1425.4 MB)
2019-07-09 20:24:39,506   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 111.4 KB, free 1425.3 MB)
2019-07-09 20:24:39,506   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:3138 (size: 111.4 KB, free: 1426.3 MB)
2019-07-09 20:24:39,506   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:24:39,522   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at map at test.scala:40) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:24:39,522   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 4.0 with 2 tasks
2019-07-09 20:24:39,522   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 4.0 (TID 6, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:24:39,522   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 4.0 (TID 7, localhost, executor driver, partition 1, ANY, 7130 bytes)
2019-07-09 20:24:39,522   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 4.0 (TID 6)
2019-07-09 20:24:39,522   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 4.0 (TID 7)
2019-07-09 20:24:39,537   INFO --- [Executor task launch worker for task 6]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:24:39,537   INFO --- [Executor task launch worker for task 7]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:24:39,537   INFO --- [Executor task launch worker for task 6]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:24:39,537   INFO --- [Executor task launch worker for task 7]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:24:39,725   INFO --- [Executor task launch worker for task 6]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 4.0 (TID 6). 1281 bytes result sent to driver
2019-07-09 20:24:39,725   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 4.0 (TID 6) in 203 ms on localhost (executor driver) (1/2)
2019-07-09 20:24:39,834   INFO --- [Executor task launch worker for task 7]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 4.0 (TID 7). 1281 bytes result sent to driver
2019-07-09 20:24:39,834   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 4.0 (TID 7) in 312 ms on localhost (executor driver) (2/2)
2019-07-09 20:24:39,834   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 4.0, whose tasks have all completed, from pool 
2019-07-09 20:24:39,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 4 (map at test.scala:40) finished in 0.344 s
2019-07-09 20:24:39,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:24:39,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:24:39,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 5)
2019-07-09 20:24:39,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:24:39,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (ShuffledRDD[13] at reduceByKey at test.scala:41), which has no missing parents
2019-07-09 20:24:39,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 3.5 KB, free 1425.3 MB)
2019-07-09 20:24:39,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1425.3 MB)
2019-07-09 20:24:39,850   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on fc-pc:3138 (size: 2.1 KB, free: 1426.3 MB)
2019-07-09 20:24:39,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:24:39,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[13] at reduceByKey at test.scala:41) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:24:39,850   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-07-09 20:24:39,850   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 8, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:24:39,850   INFO --- [Executor task launch worker for task 8]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 8)
2019-07-09 20:24:39,865   INFO --- [Executor task launch worker for task 8]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:24:39,865   INFO --- [Executor task launch worker for task 8]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:24:40,068   INFO --- [Executor task launch worker for task 8]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_8 stored as bytes in memory (estimated size 5.5 MB, free 1419.8 MB)
2019-07-09 20:24:40,084   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_8 in memory on fc-pc:3138 (size: 5.5 MB, free: 1420.7 MB)
2019-07-09 20:24:40,084   INFO --- [Executor task launch worker for task 8]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 8). 5790017 bytes result sent via BlockManager)
2019-07-09 20:24:40,178   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_4_piece0 on fc-pc:3138 in memory (size: 111.4 KB, free: 1420.8 MB)
2019-07-09 20:24:40,193   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 8) in 343 ms on localhost (executor driver) (1/1)
2019-07-09 20:24:40,193   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-07-09 20:24:40,193   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_8 on fc-pc:3138 in memory (size: 5.5 MB, free: 1426.4 MB)
2019-07-09 20:24:40,193   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (collect at test.scala:48) finished in 0.343 s
2019-07-09 20:24:40,193   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at test.scala:48, took 1.689412 s
2019-07-09 20:24:40,865   INFO --- [main]  org.apache.hadoop.conf.Configuration.deprecation(line:1129) : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-07-09 20:24:40,881   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 20:24:40,912   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-07-09 20:24:40,928   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-07-09 20:24:40,928   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 9 (runJob at SparkHadoopWriter.scala:78)
2019-07-09 20:24:40,928   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 8)
2019-07-09 20:24:40,928   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 20:24:40,928   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 9 (MapPartitionsRDD[14] at saveAsTextFile at test.scala:49), which has no missing parents
2019-07-09 20:24:40,959   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_6 stored as values in memory (estimated size 66.4 KB, free 1425.7 MB)
2019-07-09 20:24:40,959   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_6_piece0 stored as bytes in memory (estimated size 24.0 KB, free 1425.7 MB)
2019-07-09 20:24:40,975   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_6_piece0 in memory on fc-pc:3138 (size: 24.0 KB, free: 1426.3 MB)
2019-07-09 20:24:40,975   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 6 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:24:40,975   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[14] at saveAsTextFile at test.scala:49) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:24:40,975   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 9.0 with 1 tasks
2019-07-09 20:24:40,975   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:24:40,975   INFO --- [Executor task launch worker for task 9]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 9.0 (TID 9)
2019-07-09 20:24:41,053   INFO --- [Executor task launch worker for task 9]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:24:41,053   INFO --- [Executor task launch worker for task 9]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:24:41,178   INFO --- [Executor task launch worker for task 9]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 20:24:41,537   INFO --- [Executor task launch worker for task 9]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190709202440_0014_m_000000_0' to file:/D:/data/output/_temporary/0/task_20190709202440_0014_m_000000
2019-07-09 20:24:41,537   INFO --- [Executor task launch worker for task 9]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190709202440_0014_m_000000_0: Committed
2019-07-09 20:24:41,537   INFO --- [Executor task launch worker for task 9]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 9.0 (TID 9). 1465 bytes result sent to driver
2019-07-09 20:24:41,537   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 9.0 (TID 9) in 562 ms on localhost (executor driver) (1/1)
2019-07-09 20:24:41,537   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 9.0, whose tasks have all completed, from pool 
2019-07-09 20:24:41,537   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 9 (runJob at SparkHadoopWriter.scala:78) finished in 0.609 s
2019-07-09 20:24:41,537   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: runJob at SparkHadoopWriter.scala:78, took 0.628554 s
2019-07-09 20:24:41,600   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190709202440_0014 committed.
2019-07-09 20:24:41,615   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:24:41,615   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:24:41,631   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:24:41,865   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:24:41,865   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:24:41,865   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:24:41,865   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:24:41,881   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:24:41,881   INFO --- [main]  WordCount$(line:57) : complete!
2019-07-09 20:24:41,881   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:24:41,897   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-61a754b4-fede-4170-a1b7-4182aa181cb8
2019-07-09 20:27:32,311   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:27:32,999   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:27:33,108   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:27:33,124   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:27:33,124   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:27:33,124   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:27:33,124   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:27:37,030   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3196.
2019-07-09 20:27:37,061   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:27:37,108   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:27:37,123   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:27:37,123   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:27:37,139   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-47afcb11-dce1-4de5-8873-79ab261471b3
2019-07-09 20:27:37,202   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:27:37,217   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:27:37,373   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16527ms
2019-07-09 20:27:37,452   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:27:37,483   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16639ms
2019-07-09 20:27:37,514   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:27:37,514   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:27:37,545   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,545   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,577   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,577   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,577   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,577   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,577   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,577   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,608   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,608   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,608   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,623   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,623   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:27:37,623   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:27:37,920   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:27:38,045   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3218.
2019-07-09 20:27:38,045   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3218
2019-07-09 20:27:38,045   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:27:38,077   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3218, None)
2019-07-09 20:27:38,092   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3218 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3218, None)
2019-07-09 20:27:38,092   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3218, None)
2019-07-09 20:27:38,092   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3218, None)
2019-07-09 20:27:38,358   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:27:39,717   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:27:40,233   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:27:40,248   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3218 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:27:40,248   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:27:40,811   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:49
2019-07-09 20:27:40,889   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:27:40,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:41)
2019-07-09 20:27:40,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:49) with 1 output partitions
2019-07-09 20:27:40,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at test.scala:49)
2019-07-09 20:27:40,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-07-09 20:27:40,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-07-09 20:27:40,951   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41), which has no missing parents
2019-07-09 20:27:41,139   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 1426.3 MB)
2019-07-09 20:27:41,155   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.1 KB, free 1426.3 MB)
2019-07-09 20:27:41,155   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3218 (size: 3.1 KB, free: 1426.5 MB)
2019-07-09 20:27:41,155   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:27:41,201   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:27:41,201   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:27:41,311   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:27:41,326   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:27:41,342   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:27:41,342   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:27:42,420   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:27:42,420   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:27:43,842   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:27:43,858   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 20:27:43,873   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2578 ms on localhost (executor driver) (1/2)
2019-07-09 20:27:43,873   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2547 ms on localhost (executor driver) (2/2)
2019-07-09 20:27:43,873   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:27:43,889   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:41) finished in 2.813 s
2019-07-09 20:27:43,889   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:27:43,889   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:27:43,889   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-07-09 20:27:43,889   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:27:43,904   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[6] at reduceByKey at test.scala:42), which has no missing parents
2019-07-09 20:27:43,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 3.5 KB, free 1426.3 MB)
2019-07-09 20:27:43,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1426.3 MB)
2019-07-09 20:27:43,936   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3218 (size: 2.1 KB, free: 1426.5 MB)
2019-07-09 20:27:43,936   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:27:43,951   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[6] at reduceByKey at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:27:43,951   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:27:43,951   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:27:43,951   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:27:43,998   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:27:44,014   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 31 ms
2019-07-09 20:27:45,404   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_2 stored as bytes in memory (estimated size 5.2 MB, free 1421.0 MB)
2019-07-09 20:27:45,404   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_2 in memory on fc-pc:3218 (size: 5.2 MB, free: 1421.3 MB)
2019-07-09 20:27:45,404   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 5471539 bytes result sent via BlockManager)
2019-07-09 20:27:45,436   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3218 in memory (size: 3.1 KB, free: 1421.3 MB)
2019-07-09 20:27:45,483   INFO --- [task-result-getter-2]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:3218 after 42 ms (0 ms spent in bootstraps)
2019-07-09 20:27:45,717   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 1766 ms on localhost (executor driver) (1/1)
2019-07-09 20:27:45,717   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:27:45,733   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at test.scala:49) finished in 1.781 s
2019-07-09 20:27:45,733   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_2 on fc-pc:3218 in memory (size: 5.2 MB, free: 1426.5 MB)
2019-07-09 20:27:45,733   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:49, took 4.928031 s
2019-07-09 20:27:46,686   INFO --- [main]  org.apache.hadoop.conf.Configuration.deprecation(line:1129) : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-07-09 20:27:46,686   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 20:27:46,748   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-07-09 20:27:46,748   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-07-09 20:27:46,764   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:78)
2019-07-09 20:27:46,764   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 2)
2019-07-09 20:27:46,764   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 20:27:46,764   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at test.scala:50), which has no missing parents
2019-07-09 20:27:46,811   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 66.4 KB, free 1426.2 MB)
2019-07-09 20:27:46,811   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 24.0 KB, free 1426.2 MB)
2019-07-09 20:27:46,826   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3218 (size: 24.0 KB, free: 1426.5 MB)
2019-07-09 20:27:46,826   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:27:46,826   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at saveAsTextFile at test.scala:50) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:27:46,826   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 3.0 with 1 tasks
2019-07-09 20:27:46,826   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:27:46,826   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 3.0 (TID 3)
2019-07-09 20:27:47,029   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:27:47,029   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-07-09 20:27:47,342   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-07-09 20:27:47,357   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:3218 in memory (size: 2.1 KB, free: 1426.5 MB)
2019-07-09 20:27:47,357   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-07-09 20:27:47,357   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-07-09 20:27:47,357   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-07-09 20:27:47,357   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-07-09 20:27:47,357   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-09 20:27:47,357   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-09 20:27:47,357   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-07-09 20:27:47,357   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-07-09 20:27:47,514   INFO --- [Executor task launch worker for task 3]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 20:27:47,857   INFO --- [Executor task launch worker for task 3]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190709202746_0007_m_000000_0' to file:/D:/data/output/_temporary/0/task_20190709202746_0007_m_000000
2019-07-09 20:27:47,857   INFO --- [Executor task launch worker for task 3]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190709202746_0007_m_000000_0: Committed
2019-07-09 20:27:47,857   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 3.0 (TID 3). 1465 bytes result sent to driver
2019-07-09 20:27:47,857   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 3.0 (TID 3) in 1031 ms on localhost (executor driver) (1/1)
2019-07-09 20:27:47,857   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 3.0, whose tasks have all completed, from pool 
2019-07-09 20:27:47,857   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 3 (runJob at SparkHadoopWriter.scala:78) finished in 1.078 s
2019-07-09 20:27:47,857   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: runJob at SparkHadoopWriter.scala:78, took 1.113437 s
2019-07-09 20:27:47,904   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190709202746_0007 committed.
2019-07-09 20:27:47,920   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:27:47,936   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:27:47,951   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:27:48,061   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:27:48,076   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:27:48,076   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:27:48,076   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:27:48,092   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:27:48,092   INFO --- [main]  WordCount$(line:58) : complete!
2019-07-09 20:27:48,107   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:27:48,107   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-a4ab6a51-cd9b-4a69-9a2e-a3b16934a534
2019-07-09 20:32:23,389   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:32:23,998   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:32:24,108   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:32:24,108   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:32:24,108   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:32:24,108   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:32:24,108   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:32:28,030   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3288.
2019-07-09 20:32:28,092   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:32:28,139   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:32:28,139   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:32:28,139   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:32:28,170   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-2617f095-33cf-4146-ac20-cb3a1997de6e
2019-07-09 20:32:28,201   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:32:28,217   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:32:28,373   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16594ms
2019-07-09 20:32:28,498   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:32:28,545   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16761ms
2019-07-09 20:32:28,608   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@364b4dab{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:32:28,608   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:32:28,686   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,686   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,686   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,686   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,686   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,686   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,686   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,686   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,701   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,701   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,701   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,701   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,701   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,701   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,701   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,717   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,717   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,717   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,717   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,717   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,733   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,733   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,748   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,748   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,748   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:32:28,764   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:32:29,154   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:32:29,326   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3310.
2019-07-09 20:32:29,326   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3310
2019-07-09 20:32:29,342   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:32:29,404   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3310, None)
2019-07-09 20:32:29,404   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3310 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3310, None)
2019-07-09 20:32:29,420   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3310, None)
2019-07-09 20:32:29,420   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3310, None)
2019-07-09 20:32:29,764   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:32:30,811   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:32:30,983   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:32:30,998   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3310 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:32:30,998   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:32:31,404   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:50
2019-07-09 20:32:31,545   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:32:31,639   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:41)
2019-07-09 20:32:31,654   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:50) with 1 output partitions
2019-07-09 20:32:31,654   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at test.scala:50)
2019-07-09 20:32:31,654   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-07-09 20:32:31,670   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-07-09 20:32:31,686   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41), which has no missing parents
2019-07-09 20:32:31,842   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 20:32:31,858   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:32:31,873   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3310 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:32:31,873   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:32:31,920   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:32:31,920   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:32:32,061   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:32:32,076   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:32:32,123   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:32:32,123   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:32:32,998   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:32:32,998   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:32:34,420   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 20:32:34,420   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:32:34,436   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2422 ms on localhost (executor driver) (1/2)
2019-07-09 20:32:34,436   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2360 ms on localhost (executor driver) (2/2)
2019-07-09 20:32:34,451   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:32:34,451   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:41) finished in 2.656 s
2019-07-09 20:32:34,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:32:34,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:32:34,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-07-09 20:32:34,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:32:34,467   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[6] at reduceByKey at test.scala:42), which has no missing parents
2019-07-09 20:32:34,482   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 3.8 KB, free 1426.3 MB)
2019-07-09 20:32:34,498   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1426.3 MB)
2019-07-09 20:32:34,498   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3310 (size: 2.2 KB, free: 1426.5 MB)
2019-07-09 20:32:34,498   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:32:34,498   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[6] at reduceByKey at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:32:34,498   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:32:34,514   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:32:34,514   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:32:34,545   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:32:34,545   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:32:35,139   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3310 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:32:35,420   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_2 stored as bytes in memory (estimated size 5.2 MB, free 1421.0 MB)
2019-07-09 20:32:35,420   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_2 in memory on fc-pc:3310 (size: 5.2 MB, free: 1421.3 MB)
2019-07-09 20:32:35,420   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 5471625 bytes result sent via BlockManager)
2019-07-09 20:32:35,498   INFO --- [task-result-getter-2]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:3310 after 43 ms (0 ms spent in bootstraps)
2019-07-09 20:32:35,764   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 1250 ms on localhost (executor driver) (1/1)
2019-07-09 20:32:35,764   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:32:35,779   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at test.scala:50) finished in 1.282 s
2019-07-09 20:32:35,779   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_2 on fc-pc:3310 in memory (size: 5.2 MB, free: 1426.5 MB)
2019-07-09 20:32:35,779   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:50, took 4.380696 s
2019-07-09 20:32:36,842   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-09 20:32:36,873   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-09 20:32:36,889   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-07-09 20:32:36,889   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-07-09 20:32:36,889   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-07-09 20:32:36,889   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-07-09 20:32:36,904   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-07-09 20:32:36,904   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-07-09 20:32:36,904   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-07-09 20:32:36,904   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-07-09 20:32:36,904   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-07-09 20:32:36,904   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-07-09 20:32:36,920   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-07-09 20:32:36,920   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-07-09 20:32:36,935   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-07-09 20:32:36,951   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-07-09 20:32:36,951   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-07-09 20:32:36,967   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-07-09 20:32:36,982   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-07-09 20:32:36,982   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-07-09 20:32:36,998   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-07-09 20:32:37,014   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-09 20:32:37,014   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-07-09 20:32:37,014   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-09 20:32:37,014   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-07-09 20:32:37,014   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:3310 in memory (size: 2.2 KB, free: 1426.5 MB)
2019-07-09 20:32:37,029   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@364b4dab{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:32:37,029   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:32:37,076   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:32:37,217   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:32:37,217   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:32:37,217   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:32:37,217   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:32:37,248   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:32:37,248   INFO --- [main]  WordCount$(line:59) : complete!
2019-07-09 20:32:37,264   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:32:37,264   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-d5b6bfbe-bfe6-4e30-822f-c8427768a470
2019-07-09 20:34:21,341   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:34:21,966   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:34:22,059   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:34:22,075   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:34:22,075   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:34:22,075   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:34:22,075   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:34:25,856   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3352.
2019-07-09 20:34:25,887   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:34:25,919   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:34:25,919   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:34:25,919   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:34:25,950   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-1c1f5888-dfa0-48c0-af82-c719ea787a87
2019-07-09 20:34:25,981   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:34:26,012   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:34:26,137   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16438ms
2019-07-09 20:34:26,216   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:34:26,231   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16540ms
2019-07-09 20:34:26,262   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:34:26,262   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:34:26,309   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,309   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,309   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,309   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,309   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,325   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,341   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,341   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,341   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,341   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,341   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,341   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,356   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,356   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,356   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,356   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:34:26,356   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:34:26,637   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:34:26,731   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3373.
2019-07-09 20:34:26,731   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3373
2019-07-09 20:34:26,747   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:34:26,778   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3373, None)
2019-07-09 20:34:26,778   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3373 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3373, None)
2019-07-09 20:34:26,794   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3373, None)
2019-07-09 20:34:26,794   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3373, None)
2019-07-09 20:34:27,044   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:34:28,012   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:34:28,137   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:34:28,137   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3373 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:34:28,153   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:34:28,434   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:50
2019-07-09 20:34:28,512   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:34:28,590   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:41)
2019-07-09 20:34:28,590   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:50) with 1 output partitions
2019-07-09 20:34:28,606   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (collect at test.scala:50)
2019-07-09 20:34:28,606   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 0)
2019-07-09 20:34:28,606   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 0)
2019-07-09 20:34:28,622   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41), which has no missing parents
2019-07-09 20:34:28,794   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 20:34:28,809   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:34:28,809   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3373 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:34:28,809   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:34:28,856   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:34:28,856   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:34:28,997   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:34:28,997   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:34:29,044   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:34:29,044   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:34:29,919   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:34:29,919   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:34:30,965   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 20:34:30,965   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:34:30,981   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2016 ms on localhost (executor driver) (1/2)
2019-07-09 20:34:30,981   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1984 ms on localhost (executor driver) (2/2)
2019-07-09 20:34:30,981   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:34:31,012   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:41) finished in 2.282 s
2019-07-09 20:34:31,012   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:34:31,012   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:34:31,012   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 1)
2019-07-09 20:34:31,012   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:34:31,012   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (ShuffledRDD[6] at reduceByKey at test.scala:42), which has no missing parents
2019-07-09 20:34:31,044   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 3.8 KB, free 1426.3 MB)
2019-07-09 20:34:31,044   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1426.3 MB)
2019-07-09 20:34:31,044   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3373 (size: 2.2 KB, free: 1426.5 MB)
2019-07-09 20:34:31,044   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:34:31,059   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[6] at reduceByKey at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:34:31,059   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:34:31,059   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:34:31,059   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:34:31,106   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:34:31,106   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 20:34:31,481   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 555998 bytes result sent to driver
2019-07-09 20:34:31,512   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 453 ms on localhost (executor driver) (1/1)
2019-07-09 20:34:31,512   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:34:31,512   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (collect at test.scala:50) finished in 0.468 s
2019-07-09 20:34:31,528   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:50, took 3.093821 s
2019-07-09 20:34:32,059   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:34:32,075   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:34:32,090   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:34:32,200   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:34:32,200   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:34:32,215   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:34:32,231   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:34:32,247   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:34:32,247   INFO --- [main]  WordCount$(line:59) : complete!
2019-07-09 20:34:32,262   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:34:32,262   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-fed0f49a-b242-4bcb-a780-b349d7d4fbc2
2019-07-09 20:36:56,453   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:36:57,141   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:36:57,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:36:57,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:36:57,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:36:57,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:36:57,313   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:37:01,203   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3411.
2019-07-09 20:37:01,234   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:37:01,266   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:37:01,281   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:37:01,281   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:37:01,297   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-af40380d-5b39-4786-87da-11732cbec4d5
2019-07-09 20:37:01,328   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:37:01,359   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:37:01,500   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16442ms
2019-07-09 20:37:01,578   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:37:01,594   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16548ms
2019-07-09 20:37:01,641   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@5262b823{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:37:01,641   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:37:01,672   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,672   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,672   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,672   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,672   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,672   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,703   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,703   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,703   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,703   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,703   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,719   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,719   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,719   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:37:01,719   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:37:02,000   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:37:02,094   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3432.
2019-07-09 20:37:02,094   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3432
2019-07-09 20:37:02,094   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:37:02,141   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3432, None)
2019-07-09 20:37:02,141   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3432 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3432, None)
2019-07-09 20:37:02,156   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3432, None)
2019-07-09 20:37:02,156   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3432, None)
2019-07-09 20:37:02,406   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:37:03,437   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:37:03,609   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:37:03,625   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3432 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:37:03,625   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:37:03,969   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:51
2019-07-09 20:37:04,062   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:37:04,094   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:41)
2019-07-09 20:37:04,109   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:43)
2019-07-09 20:37:04,109   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:51) with 1 output partitions
2019-07-09 20:37:04,109   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:51)
2019-07-09 20:37:04,109   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 20:37:04,109   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 20:37:04,125   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41), which has no missing parents
2019-07-09 20:37:04,281   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 20:37:04,281   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:37:04,297   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3432 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:37:04,297   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:37:04,328   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:37:04,328   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:37:04,437   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:37:04,437   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:37:04,453   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:37:04,453   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:37:05,359   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:37:05,359   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:37:06,469   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:37:06,469   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 20:37:06,484   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2078 ms on localhost (executor driver) (1/2)
2019-07-09 20:37:06,500   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2063 ms on localhost (executor driver) (2/2)
2019-07-09 20:37:06,500   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:37:06,515   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:41) finished in 2.312 s
2019-07-09 20:37:06,515   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:37:06,515   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:37:06,515   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 20:37:06,515   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:37:06,531   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43), which has no missing parents
2019-07-09 20:37:06,547   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 20:37:06,547   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1426.3 MB)
2019-07-09 20:37:06,562   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3432 (size: 2.7 KB, free: 1426.5 MB)
2019-07-09 20:37:06,562   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:37:06,562   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:37:06,562   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:37:06,578   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:37:06,578   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:37:06,672   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:37:06,672   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 20:37:07,172   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 20:37:07,172   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 594 ms on localhost (executor driver) (1/1)
2019-07-09 20:37:07,172   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:37:07,172   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:43) finished in 0.625 s
2019-07-09 20:37:07,172   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:37:07,172   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:37:07,172   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 20:37:07,172   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:37:07,172   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:43), which has no missing parents
2019-07-09 20:37:07,187   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.4 KB, free 1426.2 MB)
2019-07-09 20:37:07,187   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 20:37:07,187   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3432 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:37:07,187   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:37:07,203   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:37:07,203   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 20:37:07,203   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:37:07,203   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 20:37:07,219   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:37:07,219   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:37:07,515   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 602586 bytes result sent to driver
2019-07-09 20:37:07,562   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3432 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:37:07,562   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 359 ms on localhost (executor driver) (1/1)
2019-07-09 20:37:07,562   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:51) finished in 0.375 s
2019-07-09 20:37:07,578   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 20:37:07,594   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:51, took 3.623888 s
2019-07-09 20:37:07,625   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:3432 in memory (size: 2.7 KB, free: 1426.5 MB)
2019-07-09 20:37:08,140   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@5262b823{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:37:08,140   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:37:08,156   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:37:08,328   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:37:08,328   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:37:08,328   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:37:08,328   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:37:08,359   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:37:08,359   INFO --- [main]  WordCount$(line:60) : complete!
2019-07-09 20:37:08,375   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:37:08,375   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-f2226810-5184-43e0-92d9-c4614cea73e1
2019-07-09 20:38:02,256   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:38:02,865   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:38:02,975   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:38:02,975   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:38:02,975   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:38:02,975   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:38:02,975   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:38:06,896   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3463.
2019-07-09 20:38:06,928   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:38:06,959   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:38:06,959   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:38:06,974   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:38:06,990   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-3789fe23-bb27-4a61-aac3-63e72cc8ba97
2019-07-09 20:38:07,021   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:38:07,053   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:38:07,209   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16354ms
2019-07-09 20:38:07,287   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:38:07,303   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16458ms
2019-07-09 20:38:07,349   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@6d69c3bf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:38:07,349   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:38:07,396   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,396   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,396   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,396   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,396   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,396   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,396   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,412   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,412   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,412   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,412   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,412   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,412   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,412   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,412   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,412   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,428   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
