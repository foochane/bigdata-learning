2019-07-09 20:38:07,428   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,428   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,428   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,443   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,506   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,506   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,521   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,521   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:38:07,521   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:38:07,818   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:38:07,928   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3484.
2019-07-09 20:38:07,928   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3484
2019-07-09 20:38:07,928   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:38:07,974   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3484, None)
2019-07-09 20:38:07,974   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3484 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3484, None)
2019-07-09 20:38:07,990   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3484, None)
2019-07-09 20:38:07,990   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3484, None)
2019-07-09 20:38:08,256   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:38:09,287   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:38:09,428   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:38:09,428   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3484 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:38:09,443   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:38:09,803   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:50
2019-07-09 20:38:09,896   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:38:09,928   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:40)
2019-07-09 20:38:09,928   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:42)
2019-07-09 20:38:09,943   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:50) with 1 output partitions
2019-07-09 20:38:09,943   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:50)
2019-07-09 20:38:09,943   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 20:38:09,943   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 20:38:09,959   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40), which has no missing parents
2019-07-09 20:38:10,115   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 20:38:10,131   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:38:10,131   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3484 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:38:10,131   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:38:10,177   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:38:10,177   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:38:10,287   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:38:10,287   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:38:10,318   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:38:10,318   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:38:11,209   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:38:11,209   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:38:12,302   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 20:38:12,302   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:38:12,318   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2031 ms on localhost (executor driver) (1/2)
2019-07-09 20:38:12,318   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2062 ms on localhost (executor driver) (2/2)
2019-07-09 20:38:12,334   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:38:12,349   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:40) finished in 2.297 s
2019-07-09 20:38:12,349   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:38:12,349   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:38:12,349   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 20:38:12,365   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:38:12,365   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:38:12,381   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 20:38:12,396   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 20:38:12,396   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3484 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 20:38:12,396   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:38:12,396   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:38:12,396   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:38:12,412   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:38:12,412   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:38:12,459   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:38:12,459   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 20:38:12,896   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 20:38:12,896   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 484 ms on localhost (executor driver) (1/1)
2019-07-09 20:38:12,896   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:38:12,912   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:42) finished in 0.531 s
2019-07-09 20:38:12,912   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:38:12,912   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:38:12,912   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 20:38:12,912   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:38:12,912   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:38:12,912   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 20:38:12,927   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 20:38:12,927   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3484 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:38:12,927   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:38:12,927   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:38:12,927   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 20:38:12,927   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:38:12,927   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 20:38:12,943   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:38:12,943   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:38:13,146   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3484 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:38:13,177   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:3484 in memory (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 20:38:13,240   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 602586 bytes result sent to driver
2019-07-09 20:38:13,271   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 344 ms on localhost (executor driver) (1/1)
2019-07-09 20:38:13,271   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:50) finished in 0.359 s
2019-07-09 20:38:13,271   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 20:38:13,287   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:50, took 3.483641 s
2019-07-09 20:38:13,755   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@6d69c3bf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:38:13,755   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:38:13,787   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:38:13,927   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:38:13,927   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:38:13,927   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:38:13,943   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:38:13,959   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:38:13,959   INFO --- [main]  WordCount$(line:59) : complete!
2019-07-09 20:38:13,974   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:38:13,974   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-8515818b-85e8-466f-9e68-99837bea4e3e
2019-07-09 20:39:51,274   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:39:51,915   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:39:52,024   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:39:52,024   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:39:52,024   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:39:52,024   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:39:52,024   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:39:55,821   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3514.
2019-07-09 20:39:55,868   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:39:55,899   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:39:55,899   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:39:55,899   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:39:55,915   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-5f1a8980-6896-45f7-8906-b8e6f47774b5
2019-07-09 20:39:55,961   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:39:55,977   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:39:56,118   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16135ms
2019-07-09 20:39:56,196   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:39:56,211   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16241ms
2019-07-09 20:39:56,258   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2447f7a2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:39:56,258   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:39:56,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,289   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,305   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,352   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:39:56,352   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:39:56,618   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:39:56,727   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3535.
2019-07-09 20:39:56,727   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3535
2019-07-09 20:39:56,727   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:39:56,774   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3535, None)
2019-07-09 20:39:56,774   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3535 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3535, None)
2019-07-09 20:39:56,774   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3535, None)
2019-07-09 20:39:56,774   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3535, None)
2019-07-09 20:39:57,039   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:39:57,993   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:39:58,133   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:39:58,133   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3535 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:39:58,149   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:39:58,461   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:50
2019-07-09 20:39:58,571   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:39:58,618   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:40)
2019-07-09 20:39:58,618   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:42)
2019-07-09 20:39:58,633   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:50) with 1 output partitions
2019-07-09 20:39:58,633   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:50)
2019-07-09 20:39:58,633   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 20:39:58,633   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 20:39:58,649   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40), which has no missing parents
2019-07-09 20:39:58,836   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 20:39:58,868   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:39:58,868   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3535 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:39:58,883   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:39:58,930   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:39:58,930   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:39:59,039   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:39:59,039   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:39:59,071   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:39:59,071   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:39:59,883   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:2629+2629
2019-07-09 20:39:59,883   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+2629
2019-07-09 20:40:00,117   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:40:00,117   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 20:40:00,133   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1125 ms on localhost (executor driver) (1/2)
2019-07-09 20:40:00,133   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1094 ms on localhost (executor driver) (2/2)
2019-07-09 20:40:00,149   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:40:00,164   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:40) finished in 1.391 s
2019-07-09 20:40:00,164   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:40:00,164   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:40:00,164   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 20:40:00,164   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:40:00,180   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:40:00,196   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 20:40:00,196   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 20:40:00,211   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3535 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 20:40:00,211   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:40:00,227   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:40:00,227   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:40:00,227   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:40:00,227   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:40:00,289   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:40:00,289   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:40:00,383   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 20:40:00,399   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 172 ms on localhost (executor driver) (1/1)
2019-07-09 20:40:00,399   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:40:00,399   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:42) finished in 0.203 s
2019-07-09 20:40:00,399   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:40:00,399   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:40:00,399   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 20:40:00,399   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:40:00,399   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:40:00,399   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 20:40:00,414   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 20:40:00,414   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3535 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:40:00,414   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:40:00,414   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:40:00,414   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 20:40:00,430   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:40:00,430   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 20:40:00,446   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:40:00,446   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:40:00,477   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 2414 bytes result sent to driver
2019-07-09 20:40:00,477   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 47 ms on localhost (executor driver) (1/1)
2019-07-09 20:40:00,492   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:50) finished in 0.093 s
2019-07-09 20:40:00,492   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 20:40:00,508   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:50, took 2.034064 s
2019-07-09 20:40:00,524   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-07-09 20:40:00,524   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-09 20:40:00,539   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2447f7a2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:40:00,555   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:40:00,586   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on fc-pc:3535 in memory (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:40:00,649   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:40:00,774   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:40:00,774   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:40:00,774   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:40:00,774   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:40:00,805   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:40:00,805   INFO --- [main]  WordCount$(line:59) : complete!
2019-07-09 20:40:00,805   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:40:00,805   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-93459fa1-546f-4e45-9e47-9c1af102cd4f
2019-07-09 20:42:07,442   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:42:08,052   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:42:08,161   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:42:08,161   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:42:08,177   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:42:08,177   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:42:08,177   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:42:11,958   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3575.
2019-07-09 20:42:11,989   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:42:12,036   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:42:12,036   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:42:12,036   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:42:12,052   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-f1a504b8-3eb2-49c7-af53-991558612e0a
2019-07-09 20:42:12,099   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:42:12,114   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:42:12,255   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16325ms
2019-07-09 20:42:12,333   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:42:12,349   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16429ms
2019-07-09 20:42:12,380   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@34d80665{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:42:12,380   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,442   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,458   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,458   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,458   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,458   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,458   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:42:12,458   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:42:12,817   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:42:12,942   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3596.
2019-07-09 20:42:12,942   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3596
2019-07-09 20:42:12,958   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:42:12,989   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3596, None)
2019-07-09 20:42:12,989   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3596 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3596, None)
2019-07-09 20:42:13,005   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3596, None)
2019-07-09 20:42:13,005   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3596, None)
2019-07-09 20:42:13,255   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:42:14,239   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:42:14,380   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:42:14,380   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3596 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:42:14,395   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:42:14,755   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:50
2019-07-09 20:42:14,880   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:42:14,942   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:40)
2019-07-09 20:42:14,958   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:42)
2019-07-09 20:42:14,958   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:50) with 1 output partitions
2019-07-09 20:42:14,973   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:50)
2019-07-09 20:42:14,973   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 20:42:14,973   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 20:42:15,020   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40), which has no missing parents
2019-07-09 20:42:15,145   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 20:42:15,145   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:42:15,145   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3596 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:42:15,161   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:42:15,192   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:42:15,192   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:42:15,333   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:42:15,333   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:42:15,364   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:42:15,364   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:42:16,333   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+21
2019-07-09 20:42:16,333   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:21+21
2019-07-09 20:42:16,520   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 20:42:16,520   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:42:16,536   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1203 ms on localhost (executor driver) (1/2)
2019-07-09 20:42:16,536   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1234 ms on localhost (executor driver) (2/2)
2019-07-09 20:42:16,536   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:42:16,567   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:40) finished in 1.484 s
2019-07-09 20:42:16,567   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:42:16,567   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:42:16,567   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 20:42:16,583   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:42:16,583   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:42:16,630   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 20:42:16,661   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 20:42:16,661   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3596 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 20:42:16,661   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:42:16,677   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:42:16,677   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:42:16,677   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:42:16,677   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:42:16,739   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:42:16,739   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 20:42:16,833   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 20:42:16,833   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 156 ms on localhost (executor driver) (1/1)
2019-07-09 20:42:16,833   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:42) finished in 0.203 s
2019-07-09 20:42:16,833   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:42:16,833   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:42:16,833   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 20:42:16,833   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:42:16,833   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:42:16,848   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:42:16,848   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 20:42:16,848   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 20:42:16,848   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3596 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:42:16,848   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:42:16,864   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:42:16,864   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 20:42:16,864   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:42:16,864   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 20:42:16,864   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:42:16,864   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:42:16,895   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1403 bytes result sent to driver
2019-07-09 20:42:16,895   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 31 ms on localhost (executor driver) (1/1)
2019-07-09 20:42:16,895   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 20:42:16,895   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:50) finished in 0.047 s
2019-07-09 20:42:16,895   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:50, took 2.140314 s
2019-07-09 20:42:16,927   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@34d80665{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:42:16,927   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:42:16,958   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:42:17,161   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:42:17,161   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:42:17,176   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:42:17,176   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:42:17,192   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:42:17,192   INFO --- [main]  WordCount$(line:59) : complete!
2019-07-09 20:42:17,192   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:42:17,192   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-4a06f191-eebb-42c1-87bd-8403d30e43d1
2019-07-09 20:43:33,181   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:43:33,806   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:43:33,915   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:43:33,915   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:43:33,915   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:43:33,931   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:43:33,931   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:43:37,759   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3630.
2019-07-09 20:43:37,806   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:43:37,837   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:43:37,837   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:43:37,837   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:43:37,853   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-d6953403-dff3-4edf-a2a2-2788892736b5
2019-07-09 20:43:37,900   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:43:37,915   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:43:38,040   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16240ms
2019-07-09 20:43:38,134   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:43:38,150   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16346ms
2019-07-09 20:43:38,181   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:43:38,181   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:43:38,228   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,228   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,228   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,228   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,228   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,228   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,228   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,243   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,259   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,259   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,259   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,259   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,275   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,275   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,275   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,290   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:43:38,290   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:43:38,524   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:43:38,649   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3651.
2019-07-09 20:43:38,649   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3651
2019-07-09 20:43:38,649   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:43:38,696   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3651, None)
2019-07-09 20:43:38,696   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3651 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3651, None)
2019-07-09 20:43:38,712   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3651, None)
2019-07-09 20:43:38,712   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3651, None)
2019-07-09 20:43:38,978   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:43:39,868   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:43:40,009   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:43:40,009   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3651 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:43:40,009   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:43:40,353   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:50
2019-07-09 20:43:40,462   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:43:40,493   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:40)
2019-07-09 20:43:40,493   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:42)
2019-07-09 20:43:40,509   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:50) with 1 output partitions
2019-07-09 20:43:40,509   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:50)
2019-07-09 20:43:40,509   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 20:43:40,509   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 20:43:40,524   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40), which has no missing parents
2019-07-09 20:43:40,743   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 20:43:40,759   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:43:40,759   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3651 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:43:40,759   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:43:40,790   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:43:40,790   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:43:40,899   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:43:40,899   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:43:40,931   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:43:40,931   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:43:41,774   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:21+21
2019-07-09 20:43:41,774   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+21
2019-07-09 20:43:41,962   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1152 bytes result sent to driver
2019-07-09 20:43:41,962   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
2019-07-09 20:43:41,977   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1109 ms on localhost (executor driver) (1/2)
2019-07-09 20:43:42,009   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1110 ms on localhost (executor driver) (2/2)
2019-07-09 20:43:42,024   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:40) finished in 1.328 s
2019-07-09 20:43:42,024   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:43:42,024   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:43:42,024   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 20:43:42,024   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:43:42,040   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:43:42,040   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:43:42,056   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 20:43:42,071   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 20:43:42,071   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3651 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 20:43:42,071   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:43:42,071   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:43:42,071   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:43:42,087   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:43:42,087   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:43:42,134   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:43:42,149   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:43:42,227   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 20:43:42,227   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 140 ms on localhost (executor driver) (1/1)
2019-07-09 20:43:42,227   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:43:42,227   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:42) finished in 0.187 s
2019-07-09 20:43:42,227   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:43:42,243   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:43:42,243   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 20:43:42,243   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:43:42,243   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:43:42,243   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 20:43:42,259   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 20:43:42,290   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3651 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:43:42,290   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:43:42,290   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:43:42,290   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 20:43:42,290   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:43:42,290   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 20:43:42,321   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:43:42,321   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:43:42,321   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3651 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:43:42,337   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1446 bytes result sent to driver
2019-07-09 20:43:42,337   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 47 ms on localhost (executor driver) (1/1)
2019-07-09 20:43:42,337   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 20:43:42,352   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:50) finished in 0.109 s
2019-07-09 20:43:42,352   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:50, took 1.988586 s
2019-07-09 20:43:42,368   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:43:42,384   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:43:42,399   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:43:42,540   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:43:42,540   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:43:42,540   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:43:42,540   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:43:42,556   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:43:42,556   INFO --- [main]  WordCount$(line:59) : complete!
2019-07-09 20:43:42,571   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:43:42,571   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-47e8e195-e586-4b3c-be5e-98ae8f94e65b
2019-07-09 20:44:17,213   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:44:17,822   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:44:17,916   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:44:17,931   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:44:17,931   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:44:17,931   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:44:17,931   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:44:21,713   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3676.
2019-07-09 20:44:21,744   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:44:21,775   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:44:21,775   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:44:21,775   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:44:21,806   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-4009d1ec-eaa4-4a04-830b-72917065a3c1
2019-07-09 20:44:21,838   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:44:21,853   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:44:21,994   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16265ms
2019-07-09 20:44:22,072   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:44:22,088   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16369ms
2019-07-09 20:44:22,119   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:44:22,119   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:44:22,166   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,166   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,166   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,166   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,166   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,166   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,166   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,166   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,166   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,166   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,166   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,181   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,181   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,181   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,181   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,181   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,181   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,181   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,181   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,181   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,197   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,197   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,197   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,197   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,197   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:44:22,197   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:44:22,462   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:44:22,572   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3697.
2019-07-09 20:44:22,572   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3697
2019-07-09 20:44:22,572   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:44:22,619   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3697, None)
2019-07-09 20:44:22,619   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3697 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3697, None)
2019-07-09 20:44:22,634   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3697, None)
2019-07-09 20:44:22,634   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3697, None)
2019-07-09 20:44:22,884   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:44:23,775   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:44:23,900   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:44:23,900   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3697 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:44:23,916   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:44:24,259   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:50
2019-07-09 20:44:24,337   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:44:24,369   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:40)
2019-07-09 20:44:24,384   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:42)
2019-07-09 20:44:24,384   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:50) with 1 output partitions
2019-07-09 20:44:24,384   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:50)
2019-07-09 20:44:24,384   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 20:44:24,400   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 20:44:24,400   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40), which has no missing parents
2019-07-09 20:44:24,572   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 20:44:24,603   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:44:24,603   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3697 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:44:24,603   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:44:24,650   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:44:24,650   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:44:24,759   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:44:24,759   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:44:24,806   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:44:24,806   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:44:25,666   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:21+21
2019-07-09 20:44:25,666   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+21
2019-07-09 20:44:25,869   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 20:44:25,869   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:44:25,900   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1141 ms on localhost (executor driver) (1/2)
2019-07-09 20:44:25,900   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1172 ms on localhost (executor driver) (2/2)
2019-07-09 20:44:25,900   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:44:25,915   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:40) finished in 1.421 s
2019-07-09 20:44:25,931   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:44:25,931   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:44:25,931   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 20:44:25,931   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:44:25,931   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:44:25,962   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 20:44:25,962   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 20:44:25,962   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3697 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 20:44:25,978   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:44:25,978   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:44:25,978   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:44:25,978   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:44:25,978   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:44:26,040   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:44:26,040   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:44:26,119   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 20:44:26,119   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 141 ms on localhost (executor driver) (1/1)
2019-07-09 20:44:26,119   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:44:26,119   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:42) finished in 0.172 s
2019-07-09 20:44:26,119   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:44:26,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:44:26,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 20:44:26,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:44:26,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:44:26,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 20:44:26,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 20:44:26,150   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3697 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:44:26,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:44:26,165   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:44:26,165   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 20:44:26,165   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:44:26,165   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 20:44:26,197   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:44:26,197   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:44:26,212   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3697 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:44:26,228   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1446 bytes result sent to driver
2019-07-09 20:44:26,228   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 63 ms on localhost (executor driver) (1/1)
2019-07-09 20:44:26,228   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:50) finished in 0.094 s
2019-07-09 20:44:26,244   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 20:44:26,244   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:50, took 1.994910 s
2019-07-09 20:44:26,275   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:44:26,275   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:44:26,290   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:44:26,431   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:44:26,431   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:44:26,447   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:44:26,447   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:44:26,478   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:44:26,478   INFO --- [main]  WordCount$(line:59) : complete!
2019-07-09 20:44:26,478   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:44:26,478   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-c3d57228-6a7b-48f4-b5e3-e19271dfb763
2019-07-09 20:50:26,974   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:50:27,646   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:50:27,755   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:50:27,755   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:50:27,755   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:50:27,755   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:50:27,755   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:50:31,583   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3849.
2019-07-09 20:50:31,630   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:50:31,662   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:50:31,662   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:50:31,662   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:50:31,693   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-132b7df8-f319-4859-bd7c-c3364037a7ae
2019-07-09 20:50:31,724   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:50:31,755   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:50:31,912   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16430ms
2019-07-09 20:50:31,990   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:50:32,021   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16540ms
2019-07-09 20:50:32,052   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:50:32,052   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:50:32,083   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,083   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,099   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,099   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,099   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,099   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,099   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,099   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,099   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,099   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,099   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,099   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,115   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,115   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,115   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,115   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,115   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,115   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,115   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,115   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,130   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,130   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,130   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,146   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,146   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:50:32,146   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:50:32,427   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:50:32,521   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3870.
2019-07-09 20:50:32,521   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3870
2019-07-09 20:50:32,536   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:50:32,568   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3870, None)
2019-07-09 20:50:32,583   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3870 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3870, None)
2019-07-09 20:50:32,583   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3870, None)
2019-07-09 20:50:32,583   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3870, None)
2019-07-09 20:50:32,849   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:50:33,786   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:50:34,255   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:50:34,255   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3870 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:50:34,255   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:50:34,599   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:50
2019-07-09 20:50:34,677   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:50:34,724   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:40)
2019-07-09 20:50:34,724   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:42)
2019-07-09 20:50:34,724   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:50) with 1 output partitions
2019-07-09 20:50:34,724   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:50)
2019-07-09 20:50:34,724   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 20:50:34,740   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 20:50:34,740   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40), which has no missing parents
2019-07-09 20:50:34,896   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 1426.3 MB)
2019-07-09 20:50:34,927   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.1 KB, free 1426.3 MB)
2019-07-09 20:50:34,927   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3870 (size: 3.1 KB, free: 1426.5 MB)
2019-07-09 20:50:34,927   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:50:34,974   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:50:34,974   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:50:35,083   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:50:35,083   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:50:35,130   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:50:35,130   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:50:35,958   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:21+21
2019-07-09 20:50:35,974   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+21
2019-07-09 20:50:36,177   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 20:50:36,177   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:50:36,193   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1141 ms on localhost (executor driver) (1/2)
2019-07-09 20:50:36,208   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1125 ms on localhost (executor driver) (2/2)
2019-07-09 20:50:36,208   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:50:36,224   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:40) finished in 1.391 s
2019-07-09 20:50:36,224   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:50:36,224   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:50:36,224   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 20:50:36,224   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:50:36,239   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:50:36,274   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 1426.3 MB)
2019-07-09 20:50:36,285   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 20:50:36,285   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3870 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 20:50:36,285   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:50:36,285   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:50:36,285   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:50:36,301   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:50:36,301   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:50:36,347   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:50:36,363   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 20:50:36,472   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3870 in memory (size: 3.1 KB, free: 1426.5 MB)
2019-07-09 20:50:36,504   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1324 bytes result sent to driver
2019-07-09 20:50:36,504   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 203 ms on localhost (executor driver) (1/1)
2019-07-09 20:50:36,504   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:50:36,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:42) finished in 0.241 s
2019-07-09 20:50:36,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:50:36,513   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:50:36,513   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 20:50:36,513   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:50:36,514   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:50:36,521   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.3 MB)
2019-07-09 20:50:36,526   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 20:50:36,527   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3870 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:50:36,528   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:50:36,531   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:50:36,532   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 20:50:36,535   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:50:36,535   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 20:50:36,537   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:50:36,537   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:50:36,568   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1397 bytes result sent to driver
2019-07-09 20:50:36,568   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 35 ms on localhost (executor driver) (1/1)
2019-07-09 20:50:36,568   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 20:50:36,568   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:50) finished in 0.050 s
2019-07-09 20:50:36,584   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:50, took 1.988827 s
2019-07-09 20:50:36,615   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:50:36,615   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:50:36,631   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:50:36,756   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:50:36,756   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:50:36,772   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:50:36,772   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:50:36,787   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:50:36,787   INFO --- [main]  WordCount$(line:59) : complete!
2019-07-09 20:50:36,787   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:50:36,787   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-bc541bcc-a758-4848-bfe2-16fa51611d54
2019-07-09 20:51:12,379   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:51:13,004   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:51:13,098   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:51:13,113   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:51:13,113   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:51:13,113   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:51:13,113   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:51:16,894   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3897.
2019-07-09 20:51:16,941   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:51:16,973   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:51:16,973   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:51:16,973   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:51:16,988   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-5455c427-4e6c-4241-b16a-7440cab2d106
2019-07-09 20:51:17,035   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:51:17,051   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:51:17,207   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16422ms
2019-07-09 20:51:17,285   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:51:17,316   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16526ms
2019-07-09 20:51:17,348   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:51:17,348   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:51:17,379   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,379   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,379   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,379   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,379   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,379   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,394   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,410   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,410   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,410   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,426   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,426   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,426   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,426   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,426   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:51:17,441   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:51:17,738   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:51:17,832   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3918.
2019-07-09 20:51:17,832   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3918
2019-07-09 20:51:17,832   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:51:17,863   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3918, None)
2019-07-09 20:51:17,879   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3918 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3918, None)
2019-07-09 20:51:17,879   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3918, None)
2019-07-09 20:51:17,879   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3918, None)
2019-07-09 20:51:18,129   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:51:19,082   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:51:19,269   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:51:19,285   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3918 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:51:19,285   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:51:19,613   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:50
2019-07-09 20:51:19,691   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:51:19,722   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:40)
2019-07-09 20:51:19,722   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:42)
2019-07-09 20:51:19,738   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:50) with 1 output partitions
2019-07-09 20:51:19,738   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:50)
2019-07-09 20:51:19,738   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 20:51:19,738   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 20:51:19,754   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40), which has no missing parents
2019-07-09 20:51:19,926   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.7 KB, free 1426.3 MB)
2019-07-09 20:51:19,941   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.1 KB, free 1426.3 MB)
2019-07-09 20:51:19,941   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3918 (size: 3.1 KB, free: 1426.5 MB)
2019-07-09 20:51:19,941   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:51:20,004   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:51:20,004   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:51:20,144   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:51:20,160   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:51:20,176   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:51:20,191   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:51:21,051   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:51:21,051   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:51:22,394   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
2019-07-09 20:51:22,394   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1152 bytes result sent to driver
2019-07-09 20:51:22,426   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2329 ms on localhost (executor driver) (1/2)
2019-07-09 20:51:22,426   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2282 ms on localhost (executor driver) (2/2)
2019-07-09 20:51:22,441   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:51:22,457   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:40) finished in 2.610 s
2019-07-09 20:51:22,457   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:51:22,457   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:51:22,457   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 20:51:22,457   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:51:22,472   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:51:22,488   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.6 KB, free 1426.3 MB)
2019-07-09 20:51:22,488   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KB, free 1426.3 MB)
2019-07-09 20:51:22,504   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3918 (size: 2.6 KB, free: 1426.5 MB)
2019-07-09 20:51:22,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:51:22,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:51:22,504   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:51:22,504   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:51:22,504   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:51:22,566   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:51:22,566   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:51:23,207   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3918 in memory (size: 3.1 KB, free: 1426.5 MB)
2019-07-09 20:51:23,504   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1324 bytes result sent to driver
2019-07-09 20:51:23,519   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 1015 ms on localhost (executor driver) (1/1)
2019-07-09 20:51:23,519   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:51:23,519   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:42) finished in 1.031 s
2019-07-09 20:51:23,519   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:51:23,519   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:51:23,519   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 20:51:23,519   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:51:23,519   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:51:23,519   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.3 MB)
2019-07-09 20:51:23,535   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-09 20:51:23,535   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3918 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:51:23,535   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:51:23,535   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:51:23,535   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 20:51:23,535   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:51:23,535   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 20:51:23,550   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:51:23,550   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:51:24,113   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block taskresult_3 stored as bytes in memory (estimated size 5.4 MB, free 1420.8 MB)
2019-07-09 20:51:24,113   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added taskresult_3 in memory on fc-pc:3918 (size: 5.4 MB, free: 1421.0 MB)
2019-07-09 20:51:24,113   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 5689489 bytes result sent via BlockManager)
2019-07-09 20:51:24,207   INFO --- [task-result-getter-3]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to fc-pc/192.168.233.1:3918 after 58 ms (0 ms spent in bootstraps)
2019-07-09 20:51:24,769   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:3918 in memory (size: 2.6 KB, free: 1421.1 MB)
2019-07-09 20:51:24,785   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 1250 ms on localhost (executor driver) (1/1)
2019-07-09 20:51:24,785   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 20:51:24,785   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:50) finished in 1.266 s
2019-07-09 20:51:24,785   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed taskresult_3 on fc-pc:3918 in memory (size: 5.4 MB, free: 1426.5 MB)
2019-07-09 20:51:24,800   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:50, took 5.190065 s
2019-07-09 20:51:25,722   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:51:25,722   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:51:25,754   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:51:25,925   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:51:25,925   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:51:25,925   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:51:25,925   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:51:25,941   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:51:25,941   INFO --- [main]  WordCount$(line:59) : complete!
2019-07-09 20:51:25,941   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:51:25,957   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-f354e0a0-1da2-43ef-bb50-5f967288a5cb
2019-07-09 20:53:25,547   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:53:26,219   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:53:26,312   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:53:26,328   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:53:26,328   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:53:26,328   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:53:26,328   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:53:30,328   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3953.
2019-07-09 20:53:30,359   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:53:30,390   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:53:30,390   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:53:30,390   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:53:30,422   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-86621868-8381-44e4-8e9d-b4ec56ce1070
2019-07-09 20:53:30,453   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:53:30,484   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:53:30,625   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16500ms
2019-07-09 20:53:30,719   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:53:30,734   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16608ms
2019-07-09 20:53:30,812   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@75c3975b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:53:30,812   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:53:30,906   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:53:30,906   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:53:30,906   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:53:30,922   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:53:30,922   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:53:30,922   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:53:30,922   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:53:30,937   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:53:30,937   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:53:30,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:53:30,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,000   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,015   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,015   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,015   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,015   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,015   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,015   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,015   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,015   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,031   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,031   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,031   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,031   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,047   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:53:31,047   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:53:31,250   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:53:31,375   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3974.
2019-07-09 20:53:31,375   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3974
2019-07-09 20:53:31,375   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:53:31,437   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3974, None)
2019-07-09 20:53:31,469   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3974 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3974, None)
2019-07-09 20:53:31,515   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3974, None)
2019-07-09 20:53:31,515   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3974, None)
2019-07-09 20:53:31,859   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:53:32,969   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:53:33,094   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:53:33,109   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3974 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:53:33,109   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:36
2019-07-09 20:53:33,437   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:50
2019-07-09 20:53:33,547   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:53:33,593   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:40)
2019-07-09 20:53:33,593   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:42)
2019-07-09 20:53:33,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:50) with 1 output partitions
2019-07-09 20:53:33,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:50)
2019-07-09 20:53:33,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 20:53:33,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 20:53:33,640   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40), which has no missing parents
2019-07-09 20:53:33,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 20:53:33,875   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:53:33,875   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3974 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:53:33,875   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:53:33,906   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:40) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:53:33,906   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:53:34,015   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:53:34,031   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7350 bytes)
2019-07-09 20:53:34,047   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:53:34,062   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:53:35,078   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:21+21
2019-07-09 20:53:35,078   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/word.txt:0+21
2019-07-09 20:53:35,297   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 20:53:35,297   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:53:35,312   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1281 ms on localhost (executor driver) (1/2)
2019-07-09 20:53:35,328   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1344 ms on localhost (executor driver) (2/2)
2019-07-09 20:53:35,328   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:53:35,343   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:40) finished in 1.531 s
2019-07-09 20:53:35,343   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:53:35,343   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:53:35,343   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 20:53:35,343   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:53:35,359   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:53:35,375   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 20:53:35,390   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 20:53:35,390   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3974 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 20:53:35,390   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:53:35,406   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:53:35,406   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:53:35,406   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:53:35,406   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:53:35,468   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:53:35,468   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:53:35,593   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 20:53:35,593   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 187 ms on localhost (executor driver) (1/1)
2019-07-09 20:53:35,593   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:53:35,593   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:42) finished in 0.218 s
2019-07-09 20:53:35,593   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:53:35,593   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:53:35,593   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 20:53:35,593   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:53:35,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42), which has no missing parents
2019-07-09 20:53:35,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 20:53:35,640   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 20:53:35,640   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3974 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:53:35,640   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:53:35,640   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:53:35,640   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 20:53:35,656   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:53:35,656   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 20:53:35,672   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:53:35,672   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:53:35,687   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3974 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:53:35,703   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1446 bytes result sent to driver
2019-07-09 20:53:35,718   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 78 ms on localhost (executor driver) (1/1)
2019-07-09 20:53:35,718   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:50) finished in 0.109 s
2019-07-09 20:53:35,718   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 20:53:35,734   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:50, took 2.282021 s
2019-07-09 20:53:35,750   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@75c3975b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:53:35,750   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:53:35,781   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:53:35,937   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:53:35,937   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:53:35,953   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:53:35,953   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:53:35,984   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:53:35,984   INFO --- [main]  WordCount$(line:59) : complete!
2019-07-09 20:53:36,000   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:53:36,000   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-47500432-5eff-46b4-b58f-8369eabd639c
2019-07-09 20:54:32,566   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:54:33,222   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:54:33,347   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:54:33,347   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:54:33,347   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:54:33,347   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:54:33,347   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:54:37,237   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 4002.
2019-07-09 20:54:37,268   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:54:37,300   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:54:37,315   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:54:37,315   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:54:37,331   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-1ad076da-863a-494c-8cc8-3d1679db6896
2019-07-09 20:54:37,378   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:54:37,393   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:54:37,534   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16786ms
2019-07-09 20:54:37,643   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:54:37,675   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16917ms
2019-07-09 20:54:37,690   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@75c3975b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:54:37,690   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:54:37,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,737   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,753   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,753   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,753   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,753   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,753   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,753   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,753   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,753   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,753   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,768   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,768   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,768   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,768   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,784   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:54:37,784   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:54:38,050   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:54:38,159   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4023.
2019-07-09 20:54:38,159   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:4023
2019-07-09 20:54:38,175   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:54:38,206   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 4023, None)
2019-07-09 20:54:38,206   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:4023 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 4023, None)
2019-07-09 20:54:38,222   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 4023, None)
2019-07-09 20:54:38,222   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 4023, None)
2019-07-09 20:54:38,472   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:54:39,425   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:54:39,581   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:54:39,597   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:4023 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:54:39,597   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:35
2019-07-09 20:54:39,972   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:49
2019-07-09 20:54:40,065   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:54:40,112   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:39)
2019-07-09 20:54:40,112   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:41)
2019-07-09 20:54:40,128   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:49) with 1 output partitions
2019-07-09 20:54:40,128   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:49)
2019-07-09 20:54:40,128   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 20:54:40,128   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 20:54:40,143   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:39), which has no missing parents
2019-07-09 20:54:40,284   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 20:54:40,315   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:54:40,315   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:4023 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:54:40,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:54:40,362   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:54:40,362   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:54:40,472   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:54:40,472   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:54:40,503   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:54:40,503   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:54:41,362   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:54:41,362   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:54:42,284   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:54:42,284   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 20:54:42,300   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1860 ms on localhost (executor driver) (1/2)
2019-07-09 20:54:42,300   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1828 ms on localhost (executor driver) (2/2)
2019-07-09 20:54:42,315   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:54:42,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:39) finished in 2.109 s
2019-07-09 20:54:42,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:54:42,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:54:42,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 20:54:42,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:54:42,331   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:41), which has no missing parents
2019-07-09 20:54:42,362   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 20:54:42,362   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 20:54:42,362   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:4023 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 20:54:42,378   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:54:42,378   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:41) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:54:42,378   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:54:42,378   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:54:42,378   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:54:42,440   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:54:42,440   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 20:54:42,878   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 20:54:42,893   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 515 ms on localhost (executor driver) (1/1)
2019-07-09 20:54:42,893   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:54:42,893   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:41) finished in 0.547 s
2019-07-09 20:54:42,893   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:54:42,893   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:54:42,893   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 20:54:42,893   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:54:42,893   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:41), which has no missing parents
2019-07-09 20:54:42,893   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 20:54:42,909   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 20:54:42,909   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:4023 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:54:42,909   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:54:42,909   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:41) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:54:42,909   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 20:54:42,909   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:54:42,909   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 20:54:42,925   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:54:42,925   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:54:43,190   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 602500 bytes result sent to driver
2019-07-09 20:54:43,237   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:4023 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:54:43,253   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:4023 in memory (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 20:54:43,253   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 344 ms on localhost (executor driver) (1/1)
2019-07-09 20:54:43,253   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 20:54:43,253   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:49) finished in 0.360 s
2019-07-09 20:54:43,268   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:49, took 3.288332 s
2019-07-09 20:54:43,550   INFO --- [main]  org.apache.hadoop.conf.Configuration.deprecation(line:1129) : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-07-09 20:54:43,565   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 20:54:43,643   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-07-09 20:54:43,643   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-07-09 20:54:43,643   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (runJob at SparkHadoopWriter.scala:78)
2019-07-09 20:54:43,643   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 4)
2019-07-09 20:54:43,643   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 20:54:43,643   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (MapPartitionsRDD[10] at saveAsTextFile at test.scala:50), which has no missing parents
2019-07-09 20:54:43,675   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 66.8 KB, free 1426.2 MB)
2019-07-09 20:54:43,675   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.2 KB, free 1426.2 MB)
2019-07-09 20:54:43,675   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:4023 (size: 24.2 KB, free: 1426.5 MB)
2019-07-09 20:54:43,675   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:54:43,690   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at saveAsTextFile at test.scala:50) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:54:43,690   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-07-09 20:54:43,690   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:54:43,690   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 4)
2019-07-09 20:54:43,768   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:54:43,768   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:54:43,909   INFO --- [Executor task launch worker for task 4]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 20:54:44,065   INFO --- [Executor task launch worker for task 4]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190709205443_0010_m_000000_0' to file:/D:/data/output/_temporary/0/task_20190709205443_0010_m_000000
2019-07-09 20:54:44,065   INFO --- [Executor task launch worker for task 4]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190709205443_0010_m_000000_0: Committed
2019-07-09 20:54:44,081   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 4). 1465 bytes result sent to driver
2019-07-09 20:54:44,081   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 4) in 391 ms on localhost (executor driver) (1/1)
2019-07-09 20:54:44,081   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-07-09 20:54:44,081   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (runJob at SparkHadoopWriter.scala:78) finished in 0.438 s
2019-07-09 20:54:44,081   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: runJob at SparkHadoopWriter.scala:78, took 0.440556 s
2019-07-09 20:54:44,143   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190709205443_0010 committed.
2019-07-09 20:54:44,174   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@75c3975b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:54:44,174   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:54:44,206   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:54:44,346   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:54:44,346   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:54:44,346   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:54:44,346   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:54:44,362   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:54:44,362   INFO --- [main]  WordCount$(line:58) : complete!
2019-07-09 20:54:44,362   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:54:44,362   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-00a129d1-365f-4c2c-94f1-21c4df78e82a
2019-07-09 20:59:09,596   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:59:10,221   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:59:10,393   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:59:10,393   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:59:10,393   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:59:10,393   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:59:10,393   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:59:14,424   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 4064.
2019-07-09 20:59:14,471   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:59:14,502   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:59:14,502   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:59:14,502   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:59:14,533   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-ccf41214-e7c7-4c12-a3a6-6ed7a18480e8
2019-07-09 20:59:14,580   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:59:14,611   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:59:14,752   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16810ms
2019-07-09 20:59:14,830   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:59:14,846   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16912ms
2019-07-09 20:59:14,893   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@34d80665{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:59:14,893   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:59:14,924   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,924   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,955   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,955   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,955   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,955   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,955   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,955   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,955   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,955   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,971   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,971   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,971   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,986   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,986   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:59:14,986   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:59:15,221   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:59:15,315   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4085.
2019-07-09 20:59:15,330   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:4085
2019-07-09 20:59:15,330   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:59:15,361   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 4085, None)
2019-07-09 20:59:15,361   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:4085 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 4085, None)
2019-07-09 20:59:15,361   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 4085, None)
2019-07-09 20:59:15,377   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 4085, None)
2019-07-09 20:59:15,643   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:16,502   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 20:59:16,658   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 20:59:16,658   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:4085 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 20:59:16,674   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:35
2019-07-09 20:59:16,971   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: count at test.scala:45
2019-07-09 20:59:17,064   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 20:59:17,158   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:41)
2019-07-09 20:59:17,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:43)
2019-07-09 20:59:17,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (count at test.scala:45) with 1 output partitions
2019-07-09 20:59:17,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (count at test.scala:45)
2019-07-09 20:59:17,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 20:59:17,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 20:59:17,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41), which has no missing parents
2019-07-09 20:59:17,361   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 20:59:17,377   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 20:59:17,377   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:4085 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:59:17,377   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:59:17,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 20:59:17,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 20:59:17,533   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:59:17,549   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 20:59:17,611   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 20:59:17,643   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 20:59:18,455   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 20:59:18,455   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 20:59:19,596   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
2019-07-09 20:59:19,611   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 20:59:19,642   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2093 ms on localhost (executor driver) (1/2)
2019-07-09 20:59:19,642   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2140 ms on localhost (executor driver) (2/2)
2019-07-09 20:59:19,642   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 20:59:19,658   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:41) finished in 2.359 s
2019-07-09 20:59:19,658   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:59:19,658   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:59:19,674   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 20:59:19,674   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:59:19,674   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43), which has no missing parents
2019-07-09 20:59:19,705   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 20:59:19,705   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 20:59:19,705   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:4085 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 20:59:19,721   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:59:19,721   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:59:19,721   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 20:59:19,721   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 20:59:19,721   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 20:59:19,783   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 20:59:19,799   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 20:59:20,174   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 20:59:20,174   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 453 ms on localhost (executor driver) (1/1)
2019-07-09 20:59:20,174   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 20:59:20,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:43) finished in 0.485 s
2019-07-09 20:59:20,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 20:59:20,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 20:59:20,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 20:59:20,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 20:59:20,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:43), which has no missing parents
2019-07-09 20:59:20,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.4 KB, free 1426.2 MB)
2019-07-09 20:59:20,189   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 20:59:20,189   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:4085 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:59:20,189   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:59:20,189   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:59:20,189   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 20:59:20,189   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:59:20,189   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 20:59:20,205   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:59:20,205   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:59:20,361   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1053 bytes result sent to driver
2019-07-09 20:59:20,361   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 172 ms on localhost (executor driver) (1/1)
2019-07-09 20:59:20,361   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (count at test.scala:45) finished in 0.187 s
2019-07-09 20:59:20,377   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 20:59:20,377   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: count at test.scala:45, took 3.403325 s
2019-07-09 20:59:20,408   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:51
2019-07-09 20:59:20,408   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (collect at test.scala:51) with 1 output partitions
2019-07-09 20:59:20,408   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (collect at test.scala:51)
2019-07-09 20:59:20,408   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 4)
2019-07-09 20:59:20,408   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 20:59:20,408   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (MapPartitionsRDD[9] at sortBy at test.scala:43), which has no missing parents
2019-07-09 20:59:20,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 20:59:20,439   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 20:59:20,455   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:4085 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:59:20,455   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-07-09 20:59:20,455   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[9] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 20:59:20,455   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 55
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 4
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 50
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 69
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 63
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 14
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 2
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 24
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-09 20:59:20,455   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 66
2019-07-09 20:59:20,471   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-07-09 20:59:20,471   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-07-09 20:59:20,471   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-07-09 20:59:20,471   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 62
2019-07-09 20:59:20,471   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-07-09 20:59:20,471   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 19
2019-07-09 20:59:20,471   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 3
2019-07-09 20:59:20,471   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 71
2019-07-09 20:59:20,471   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 20:59:20,471   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 4)
2019-07-09 20:59:20,486   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 20:59:20,486   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 20:59:20,549   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:4085 in memory (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 20:59:20,596   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 65
2019-07-09 20:59:20,596   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 21
2019-07-09 20:59:20,596   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 68
2019-07-09 20:59:20,611   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-07-09 20:59:20,611   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 61
2019-07-09 20:59:20,611   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 16
2019-07-09 20:59:20,611   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 23
2019-07-09 20:59:20,611   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-07-09 20:59:20,611   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-07-09 20:59:20,611   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 60
2019-07-09 20:59:20,611   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-07-09 20:59:20,611   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-07-09 20:59:20,611   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 70
2019-07-09 20:59:20,611   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-07-09 20:59:20,611   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on fc-pc:4085 in memory (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 20
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 6
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 73
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 10
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 64
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 54
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 8
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 11
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-07-09 20:59:20,627   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:4085 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 20:59:20,627   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 0
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 22
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 67
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 58
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 12
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 1
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 17
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 72
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 5
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 13
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 74
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 18
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 7
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 15
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 9
2019-07-09 20:59:20,642   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-07-09 20:59:20,752   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 4). 602543 bytes result sent to driver
2019-07-09 20:59:20,783   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 4) in 312 ms on localhost (executor driver) (1/1)
2019-07-09 20:59:20,783   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-07-09 20:59:20,783   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (collect at test.scala:51) finished in 0.359 s
2019-07-09 20:59:20,799   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: collect at test.scala:51, took 0.384759 s
2019-07-09 20:59:21,189   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-07-09 20:59:21,205   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@34d80665{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:59:21,205   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 20:59:21,236   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 20:59:21,455   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 20:59:21,455   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 20:59:21,471   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 20:59:21,471   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 20:59:21,486   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 20:59:21,486   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 20:59:21,486   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-8db24112-901a-4b46-95dc-b77be8b54170
2019-07-09 20:59:53,870   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 20:59:54,886   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 20:59:54,995   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 20:59:54,995   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 20:59:54,995   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 20:59:54,995   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 20:59:54,995   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 20:59:58,855   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 4112.
2019-07-09 20:59:58,917   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 20:59:58,948   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 20:59:58,948   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 20:59:58,948   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 20:59:58,980   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-11e70baa-c888-4c03-923f-c813cb7ab11f
2019-07-09 20:59:59,011   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 20:59:59,042   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 20:59:59,167   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17149ms
2019-07-09 20:59:59,245   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 20:59:59,276   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17247ms
2019-07-09 20:59:59,292   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 20:59:59,292   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,339   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,355   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,355   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,355   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,355   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,355   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,355   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,355   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,370   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,370   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,370   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,370   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,370   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 20:59:59,370   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 20:59:59,683   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 20:59:59,792   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4133.
2019-07-09 20:59:59,792   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:4133
2019-07-09 20:59:59,792   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 20:59:59,839   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 4133, None)
2019-07-09 20:59:59,839   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:4133 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 4133, None)
2019-07-09 20:59:59,839   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 4133, None)
2019-07-09 20:59:59,839   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 4133, None)
2019-07-09 21:00:00,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 21:00:01,011   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 21:00:01,151   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 21:00:01,151   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:4133 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 21:00:01,167   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:35
2019-07-09 21:00:01,495   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:51
2019-07-09 21:00:01,589   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 21:00:01,636   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:41)
2019-07-09 21:00:01,651   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:43)
2019-07-09 21:00:01,651   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:51) with 1 output partitions
2019-07-09 21:00:01,651   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:51)
2019-07-09 21:00:01,651   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 21:00:01,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 21:00:01,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41), which has no missing parents
2019-07-09 21:00:01,855   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 21:00:01,886   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 21:00:01,886   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:4133 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 21:00:01,886   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:00:01,917   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 21:00:01,917   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 21:00:02,026   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 21:00:02,026   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 21:00:02,058   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 21:00:02,058   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 21:00:02,964   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 21:00:02,964   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 21:00:04,026   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 21:00:04,042   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 21:00:04,042   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2016 ms on localhost (executor driver) (1/2)
2019-07-09 21:00:04,058   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2063 ms on localhost (executor driver) (2/2)
2019-07-09 21:00:04,058   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 21:00:04,073   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:41) finished in 2.250 s
2019-07-09 21:00:04,073   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 21:00:04,073   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 21:00:04,073   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 21:00:04,073   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 21:00:04,089   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43), which has no missing parents
2019-07-09 21:00:04,104   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 21:00:04,120   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 21:00:04,120   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:4133 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:00:04,136   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:00:04,136   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:00:04,136   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 21:00:04,136   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 21:00:04,136   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 21:00:04,198   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 21:00:04,198   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-09 21:00:04,667   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 21:00:04,667   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 531 ms on localhost (executor driver) (1/1)
2019-07-09 21:00:04,667   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 21:00:04,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:43) finished in 0.563 s
2019-07-09 21:00:04,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 21:00:04,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 21:00:04,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 21:00:04,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 21:00:04,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:43), which has no missing parents
2019-07-09 21:00:04,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 21:00:04,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 21:00:04,698   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:4133 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 21:00:04,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:00:04,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:00:04,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 21:00:04,698   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 21:00:04,698   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 21:00:04,714   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 21:00:04,714   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 21:00:04,979   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 602543 bytes result sent to driver
2019-07-09 21:00:05,042   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 344 ms on localhost (executor driver) (1/1)
2019-07-09 21:00:05,042   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:4133 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 21:00:05,042   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:51) finished in 0.359 s
2019-07-09 21:00:05,042   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 21:00:05,058   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:51, took 3.564994 s
2019-07-09 21:00:05,089   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:4133 in memory (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:00:05,433   INFO --- [main]  org.apache.hadoop.conf.Configuration.deprecation(line:1129) : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-07-09 21:00:05,448   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 21:00:05,495   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-07-09 21:00:05,495   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-07-09 21:00:05,495   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (runJob at SparkHadoopWriter.scala:78)
2019-07-09 21:00:05,495   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 4)
2019-07-09 21:00:05,495   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 21:00:05,495   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (MapPartitionsRDD[10] at saveAsTextFile at test.scala:52), which has no missing parents
2019-07-09 21:00:05,526   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 66.8 KB, free 1426.2 MB)
2019-07-09 21:00:05,526   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.2 KB, free 1426.2 MB)
2019-07-09 21:00:05,542   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:4133 (size: 24.2 KB, free: 1426.5 MB)
2019-07-09 21:00:05,542   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:00:05,542   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at saveAsTextFile at test.scala:52) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:00:05,542   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-07-09 21:00:05,542   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 21:00:05,542   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 4)
2019-07-09 21:00:05,683   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 21:00:05,683   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 21:00:05,823   INFO --- [Executor task launch worker for task 4]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 21:00:06,011   INFO --- [Executor task launch worker for task 4]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190709210005_0010_m_000000_0' to file:/D:/data/output/_temporary/0/task_20190709210005_0010_m_000000
2019-07-09 21:00:06,011   INFO --- [Executor task launch worker for task 4]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190709210005_0010_m_000000_0: Committed
2019-07-09 21:00:06,026   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 4). 1465 bytes result sent to driver
2019-07-09 21:00:06,026   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 4) in 484 ms on localhost (executor driver) (1/1)
2019-07-09 21:00:06,026   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-07-09 21:00:06,026   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (runJob at SparkHadoopWriter.scala:78) finished in 0.531 s
2019-07-09 21:00:06,026   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: runJob at SparkHadoopWriter.scala:78, took 0.541273 s
2019-07-09 21:00:06,073   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190709210005_0010 committed.
2019-07-09 21:00:06,104   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: count at test.scala:54
2019-07-09 21:00:06,104   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (count at test.scala:54) with 1 output partitions
2019-07-09 21:00:06,104   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 8 (count at test.scala:54)
2019-07-09 21:00:06,104   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 7)
2019-07-09 21:00:06,104   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 21:00:06,104   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 8 (MapPartitionsRDD[9] at sortBy at test.scala:43), which has no missing parents
2019-07-09 21:00:06,120   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 4.4 KB, free 1426.2 MB)
2019-07-09 21:00:06,120   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 21:00:06,151   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on fc-pc:4133 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 21:00:06,151   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:00:06,151   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[9] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:00:06,151   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 8.0 with 1 tasks
2019-07-09 21:00:06,151   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 8.0 (TID 5, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 21:00:06,151   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 8.0 (TID 5)
2019-07-09 21:00:06,151   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 21:00:06,151   INFO --- [Executor task launch worker for task 5]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 21:00:06,292   INFO --- [Executor task launch worker for task 5]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 8.0 (TID 5). 1053 bytes result sent to driver
2019-07-09 21:00:06,292   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 8.0 (TID 5) in 141 ms on localhost (executor driver) (1/1)
2019-07-09 21:00:06,292   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 8.0, whose tasks have all completed, from pool 
2019-07-09 21:00:06,292   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 8 (count at test.scala:54) finished in 0.172 s
2019-07-09 21:00:06,307   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: count at test.scala:54, took 0.200298 s
2019-07-09 21:00:06,323   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 21:00:06,339   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 21:00:06,370   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 21:00:06,573   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 21:00:06,573   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 21:00:06,573   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 21:00:06,589   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 21:00:06,604   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 21:00:06,620   INFO --- [main]  WordCount$(line:60) : complete!
2019-07-09 21:00:06,620   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 21:00:06,636   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-e606d1ac-e50f-48ce-8fee-bf57ac38a259
2019-07-09 21:03:11,281   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 21:03:11,890   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 21:03:11,984   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 21:03:11,984   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 21:03:11,999   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 21:03:11,999   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 21:03:11,999   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 21:03:15,765   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 4167.
2019-07-09 21:03:15,812   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 21:03:15,843   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 21:03:15,843   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 21:03:15,843   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 21:03:15,859   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-49a714ce-58e0-41ec-961b-8e3a8196828b
2019-07-09 21:03:15,906   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 21:03:15,921   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 21:03:16,062   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16186ms
2019-07-09 21:03:16,140   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 21:03:16,171   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16295ms
2019-07-09 21:03:16,187   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 21:03:16,202   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 21:03:16,234   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,234   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,234   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,234   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,234   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,234   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,249   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,249   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,249   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,249   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,249   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,249   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,249   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,249   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,249   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,265   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,281   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,281   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,281   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,281   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,281   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 21:03:16,296   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 21:03:16,546   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 21:03:16,656   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4188.
2019-07-09 21:03:16,656   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:4188
2019-07-09 21:03:16,656   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 21:03:16,702   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 4188, None)
2019-07-09 21:03:16,702   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:4188 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 4188, None)
2019-07-09 21:03:16,718   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 4188, None)
2019-07-09 21:03:16,718   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 4188, None)
2019-07-09 21:03:16,968   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 21:03:17,890   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 21:03:18,015   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 21:03:18,030   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:4188 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 21:03:18,030   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:35
2019-07-09 21:03:18,374   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:51
2019-07-09 21:03:18,468   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 21:03:18,499   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:41)
2019-07-09 21:03:18,499   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:43)
2019-07-09 21:03:18,515   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:51) with 1 output partitions
2019-07-09 21:03:18,515   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:51)
2019-07-09 21:03:18,515   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 21:03:18,515   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 21:03:18,530   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41), which has no missing parents
2019-07-09 21:03:18,718   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 21:03:18,734   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 21:03:18,734   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:4188 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 21:03:18,734   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:03:18,780   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 21:03:18,780   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 21:03:18,921   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 21:03:18,952   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 21:03:18,984   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 21:03:18,984   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 21:03:19,812   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 21:03:19,812   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 21:03:20,796   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 21:03:20,796   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 21:03:20,827   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1953 ms on localhost (executor driver) (1/2)
2019-07-09 21:03:20,843   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1891 ms on localhost (executor driver) (2/2)
2019-07-09 21:03:20,843   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 21:03:20,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:41) finished in 2.219 s
2019-07-09 21:03:20,874   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 21:03:20,874   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 21:03:20,874   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 21:03:20,874   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 21:03:20,874   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43), which has no missing parents
2019-07-09 21:03:20,890   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 21:03:20,905   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 21:03:20,905   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:4188 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:03:20,905   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:03:20,905   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:03:20,905   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 21:03:20,921   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 21:03:20,921   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 21:03:20,968   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 21:03:20,984   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 32 ms
2019-07-09 21:03:21,374   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 21:03:21,374   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 453 ms on localhost (executor driver) (1/1)
2019-07-09 21:03:21,374   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 21:03:21,374   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:43) finished in 0.484 s
2019-07-09 21:03:21,374   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 21:03:21,374   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 21:03:21,374   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 21:03:21,374   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 21:03:21,374   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:43), which has no missing parents
2019-07-09 21:03:21,374   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 21:03:21,390   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 21:03:21,390   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:4188 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 21:03:21,390   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:03:21,390   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:03:21,390   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 21:03:21,405   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 21:03:21,405   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 21:03:21,405   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 21:03:21,405   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 21:03:21,671   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 602457 bytes result sent to driver
2019-07-09 21:03:21,702   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 312 ms on localhost (executor driver) (1/1)
2019-07-09 21:03:21,718   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 21:03:21,718   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:51) finished in 0.344 s
2019-07-09 21:03:21,733   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:51, took 3.370665 s
2019-07-09 21:03:21,749   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-07-09 21:03:21,749   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 73
2019-07-09 21:03:21,749   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 74
2019-07-09 21:03:21,749   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-09 21:03:21,749   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 69
2019-07-09 21:03:21,749   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 20
2019-07-09 21:03:21,796   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:4188 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 21:03:21,812   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-07-09 21:03:21,827   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 10
2019-07-09 21:03:21,858   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 14
2019-07-09 21:03:21,858   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 4
2019-07-09 21:03:21,858   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-07-09 21:03:21,858   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 23
2019-07-09 21:03:21,858   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-07-09 21:03:21,858   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 21
2019-07-09 21:03:21,874   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 22
2019-07-09 21:03:21,874   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 6
2019-07-09 21:03:21,874   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 62
2019-07-09 21:03:21,937   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-07-09 21:03:21,937   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-07-09 21:03:21,937   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 71
2019-07-09 21:03:21,937   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-07-09 21:03:21,937   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 12
2019-07-09 21:03:21,937   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 17
2019-07-09 21:03:21,937   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 67
2019-07-09 21:03:21,937   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-07-09 21:03:21,937   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-07-09 21:03:22,015   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 60
2019-07-09 21:03:22,015   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-07-09 21:03:22,015   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 13
2019-07-09 21:03:22,030   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 2
2019-07-09 21:03:22,030   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-07-09 21:03:22,030   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 50
2019-07-09 21:03:22,046   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 24
2019-07-09 21:03:22,062   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 1
2019-07-09 21:03:22,077   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-07-09 21:03:22,077   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 7
2019-07-09 21:03:22,077   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 72
2019-07-09 21:03:22,140   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 5
2019-07-09 21:03:22,155   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 8
2019-07-09 21:03:22,155   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 55
2019-07-09 21:03:22,155   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 0
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 64
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 65
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 9
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 19
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 3
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 16
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-07-09 21:03:22,171   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 58
2019-07-09 21:03:22,202   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-07-09 21:03:22,202   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-07-09 21:03:22,202   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-07-09 21:03:22,202   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 15
2019-07-09 21:03:22,202   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 18
2019-07-09 21:03:22,202   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 61
2019-07-09 21:03:22,202   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-07-09 21:03:22,202   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-07-09 21:03:22,202   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-07-09 21:03:22,218   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 54
2019-07-09 21:03:22,233   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 68
2019-07-09 21:03:22,233   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-07-09 21:03:22,233   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 66
2019-07-09 21:03:22,233   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 63
2019-07-09 21:03:22,233   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-09 21:03:22,233   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-09 21:03:22,233   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-07-09 21:03:22,233   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-07-09 21:03:22,233   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on fc-pc:4188 in memory (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 21:03:22,249   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:4188 in memory (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:03:22,249   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 70
2019-07-09 21:03:22,249   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 11
2019-07-09 21:03:22,265   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-07-09 21:03:22,452   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-07-09 21:03:22,468   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 21:03:22,468   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 21:03:22,499   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 21:03:22,655   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 21:03:22,655   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 21:03:22,655   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 21:03:22,671   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 21:03:22,687   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 21:03:22,687   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 21:03:22,687   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-391845a2-6e68-43aa-b91c-c69b00156e1f
2019-07-09 21:11:01,061   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 21:11:01,717   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 21:11:01,811   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 21:11:01,811   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 21:11:01,811   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 21:11:01,827   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 21:11:01,827   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 21:11:05,670   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 4375.
2019-07-09 21:11:05,717   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 21:11:05,748   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 21:11:05,748   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 21:11:05,764   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 21:11:05,780   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-0e3994ad-0cab-4e42-9893-d560a393a5fc
2019-07-09 21:11:05,811   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 21:11:05,842   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 21:11:06,092   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16396ms
2019-07-09 21:11:06,186   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 21:11:06,201   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16505ms
2019-07-09 21:11:06,248   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@665cf0a8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 21:11:06,248   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 21:11:06,280   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,280   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,280   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,280   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,295   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,311   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,311   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,311   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,311   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,311   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,311   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,311   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,326   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,342   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 21:11:06,342   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 21:11:06,592   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 21:11:06,717   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4396.
2019-07-09 21:11:06,717   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:4396
2019-07-09 21:11:06,717   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 21:11:06,764   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 4396, None)
2019-07-09 21:11:06,764   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:4396 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 4396, None)
2019-07-09 21:11:06,764   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 4396, None)
2019-07-09 21:11:06,764   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 4396, None)
2019-07-09 21:11:07,076   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 21:11:08,467   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 21:11:08,592   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 21:11:08,608   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:4396 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 21:11:08,608   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:35
2019-07-09 21:11:08,779   INFO --- [main]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 21:11:08,951   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:52
2019-07-09 21:11:08,998   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:52) with 2 output partitions
2019-07-09 21:11:08,998   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at test.scala:52)
2019-07-09 21:11:08,998   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 21:11:08,998   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 21:11:09,014   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:45), which has no missing parents
2019-07-09 21:11:09,108   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.0 KB, free 1426.3 MB)
2019-07-09 21:11:09,123   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.7 KB, free 1426.3 MB)
2019-07-09 21:11:09,123   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:4396 (size: 2.7 KB, free: 1426.5 MB)
2019-07-09 21:11:09,123   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:11:09,170   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at test.scala:45) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 21:11:09,170   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 21:11:09,279   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 21:11:09,295   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 21:11:09,311   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 21:11:09,311   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 21:11:10,170   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 21:11:10,170   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 21:11:10,967   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 608251 bytes result sent to driver
2019-07-09 21:11:10,967   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 607441 bytes result sent to driver
2019-07-09 21:11:11,139   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1844 ms on localhost (executor driver) (1/2)
2019-07-09 21:11:11,154   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1906 ms on localhost (executor driver) (2/2)
2019-07-09 21:11:11,154   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 21:11:11,170   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (collect at test.scala:52) finished in 2.078 s
2019-07-09 21:11:11,186   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:52, took 2.234567 s
2019-07-09 21:11:12,326   INFO --- [main]  org.apache.hadoop.conf.Configuration.deprecation(line:1129) : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-07-09 21:11:12,326   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 21:11:12,404   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-07-09 21:11:12,404   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions
2019-07-09 21:11:12,404   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:78)
2019-07-09 21:11:12,404   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-09 21:11:12,404   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 21:11:12,404   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at test.scala:53), which has no missing parents
2019-07-09 21:11:12,436   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 67.3 KB, free 1426.2 MB)
2019-07-09 21:11:12,451   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 24.5 KB, free 1426.2 MB)
2019-07-09 21:11:12,451   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:4396 (size: 24.5 KB, free: 1426.5 MB)
2019-07-09 21:11:12,451   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:11:12,451   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at test.scala:53) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 21:11:12,451   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 2 tasks
2019-07-09 21:11:12,451   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7362 bytes)
2019-07-09 21:11:12,451   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 7362 bytes)
2019-07-09 21:11:12,467   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 21:11:12,467   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 1.0 (TID 3)
2019-07-09 21:11:12,576   INFO --- [Executor task launch worker for task 3]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 21:11:12,576   INFO --- [Executor task launch worker for task 2]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 21:11:12,607   INFO --- [Executor task launch worker for task 2]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 21:11:12,623   INFO --- [Executor task launch worker for task 3]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 21:11:13,201   INFO --- [Executor task launch worker for task 2]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190709211112_0005_m_000000_0' to file:/D:/data/output/_temporary/0/task_20190709211112_0005_m_000000
2019-07-09 21:11:13,201   INFO --- [Executor task launch worker for task 3]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190709211112_0005_m_000001_0' to file:/D:/data/output/_temporary/0/task_20190709211112_0005_m_000001
2019-07-09 21:11:13,201   INFO --- [Executor task launch worker for task 2]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190709211112_0005_m_000000_0: Committed
2019-07-09 21:11:13,201   INFO --- [Executor task launch worker for task 3]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190709211112_0005_m_000001_0: Committed
2019-07-09 21:11:13,201   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1078 bytes result sent to driver
2019-07-09 21:11:13,201   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 1.0 (TID 3). 1078 bytes result sent to driver
2019-07-09 21:11:13,217   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 1.0 (TID 3) in 766 ms on localhost (executor driver) (1/2)
2019-07-09 21:11:13,217   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 766 ms on localhost (executor driver) (2/2)
2019-07-09 21:11:13,217   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 21:11:13,217   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (runJob at SparkHadoopWriter.scala:78) finished in 0.797 s
2019-07-09 21:11:13,232   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: runJob at SparkHadoopWriter.scala:78, took 0.828736 s
2019-07-09 21:11:13,295   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190709211112_0005 committed.
2019-07-09 21:11:13,311   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@665cf0a8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 21:11:13,311   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 21:11:13,326   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 21:11:13,404   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 21:11:13,404   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 21:11:13,404   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 21:11:13,404   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 21:11:13,467   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 21:11:13,467   INFO --- [main]  WordCount$(line:60) : complete!
2019-07-09 21:11:13,467   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 21:11:13,467   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-8022adda-ff2e-45f0-b30f-68b2f3c7dde0
2019-07-09 21:23:40,477   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 21:23:41,087   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 21:23:41,196   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 21:23:41,196   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 21:23:41,196   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 21:23:41,196   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 21:23:41,196   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 21:23:45,070   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 4627.
2019-07-09 21:23:45,104   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 21:23:45,136   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 21:23:45,151   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 21:23:45,151   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 21:23:45,167   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-35b4517d-a2d4-44d3-b24b-e9d5b7522101
2019-07-09 21:23:45,198   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 21:23:45,229   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 21:23:45,354   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16412ms
2019-07-09 21:23:45,433   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 21:23:45,448   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16513ms
2019-07-09 21:23:45,479   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 21:23:45,479   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 21:23:45,526   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,526   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,526   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,526   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,526   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,526   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,526   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,526   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,542   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,542   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,542   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,542   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,542   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,542   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,542   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,542   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,542   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,558   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,558   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,558   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,558   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,558   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,573   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,573   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,573   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 21:23:45,573   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 21:23:45,899   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 21:23:46,009   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4648.
2019-07-09 21:23:46,009   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:4648
2019-07-09 21:23:46,009   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 21:23:46,040   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 4648, None)
2019-07-09 21:23:46,040   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:4648 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 4648, None)
2019-07-09 21:23:46,056   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 4648, None)
2019-07-09 21:23:46,056   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 4648, None)
2019-07-09 21:23:46,321   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 21:23:47,337   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 21:23:47,462   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 21:23:47,477   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:4648 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 21:23:47,477   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:35
2019-07-09 21:23:47,868   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:50
2019-07-09 21:23:47,977   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 21:23:48,024   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:41)
2019-07-09 21:23:48,024   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:43)
2019-07-09 21:23:48,024   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:50) with 1 output partitions
2019-07-09 21:23:48,040   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:50)
2019-07-09 21:23:48,040   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 21:23:48,040   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 21:23:48,056   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41), which has no missing parents
2019-07-09 21:23:48,243   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 21:23:48,259   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 21:23:48,259   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:4648 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 21:23:48,259   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:23:48,290   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 21:23:48,290   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 21:23:48,399   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 21:23:48,415   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 21:23:48,431   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 21:23:48,431   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 21:23:49,352   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 21:23:49,352   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 21:23:50,530   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
2019-07-09 21:23:50,530   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1152 bytes result sent to driver
2019-07-09 21:23:50,548   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2130 ms on localhost (executor driver) (1/2)
2019-07-09 21:23:50,554   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2186 ms on localhost (executor driver) (2/2)
2019-07-09 21:23:50,556   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 21:23:50,574   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:41) finished in 2.407 s
2019-07-09 21:23:50,576   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 21:23:50,577   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 21:23:50,579   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 21:23:50,580   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 21:23:50,588   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43), which has no missing parents
2019-07-09 21:23:50,615   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 21:23:50,625   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 21:23:50,638   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:4648 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:23:50,641   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:23:50,644   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:23:50,644   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 21:23:50,658   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 21:23:50,661   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 21:23:50,726   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 21:23:50,730   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 13 ms
2019-07-09 21:23:51,069   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 21:23:51,069   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 412 ms on localhost (executor driver) (1/1)
2019-07-09 21:23:51,069   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 21:23:51,085   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:43) finished in 0.460 s
2019-07-09 21:23:51,085   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 21:23:51,085   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 21:23:51,085   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 21:23:51,085   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 21:23:51,085   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:43), which has no missing parents
2019-07-09 21:23:51,085   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-09 21:23:51,085   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-09 21:23:51,100   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:4648 (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 21:23:51,100   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:23:51,100   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:23:51,100   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 21:23:51,100   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 21:23:51,116   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 21:23:51,116   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 21:23:51,116   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 21:23:51,397   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 602457 bytes result sent to driver
2019-07-09 21:23:51,444   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 344 ms on localhost (executor driver) (1/1)
2019-07-09 21:23:51,444   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:50) finished in 0.359 s
2019-07-09 21:23:51,444   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 21:23:51,460   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:50, took 3.584992 s
2019-07-09 21:23:51,616   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 70
2019-07-09 21:23:51,663   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 16
2019-07-09 21:23:51,663   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 21
2019-07-09 21:23:51,678   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 11
2019-07-09 21:23:51,678   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 13
2019-07-09 21:23:51,678   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 62
2019-07-09 21:23:51,725   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:4648 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 21:23:51,741   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-07-09 21:23:51,757   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 18
2019-07-09 21:23:51,757   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 23
2019-07-09 21:23:51,757   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 58
2019-07-09 21:23:51,757   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-07-09 21:23:51,772   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 12
2019-07-09 21:23:51,772   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 69
2019-07-09 21:23:51,772   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-07-09 21:23:51,772   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-07-09 21:23:51,772   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-07-09 21:23:51,772   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 74
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 4
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 0
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 54
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 3
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 9
2019-07-09 21:23:51,788   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 50
2019-07-09 21:23:51,803   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 10
2019-07-09 21:23:51,803   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-07-09 21:23:51,803   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 1
2019-07-09 21:23:51,803   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-07-09 21:23:51,803   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 72
2019-07-09 21:23:51,819   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 7
2019-07-09 21:23:51,819   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-07-09 21:23:51,835   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 73
2019-07-09 21:23:51,835   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 67
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 8
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 64
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 15
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 68
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 71
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 2
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 24
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 5
2019-07-09 21:23:51,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-07-09 21:23:51,866   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 14
2019-07-09 21:23:51,866   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 63
2019-07-09 21:23:51,866   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-09 21:23:51,866   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:4648 in memory (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:23:51,866   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-07-09 21:23:51,866   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 19
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 55
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 20
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 61
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 65
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 6
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 60
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 66
2019-07-09 21:23:51,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 17
2019-07-09 21:23:51,928   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on fc-pc:4648 in memory (size: 2.5 KB, free: 1426.5 MB)
2019-07-09 21:23:51,928   INFO --- [main]  org.apache.hadoop.conf.Configuration.deprecation(line:1129) : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-07-09 21:23:51,928   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-09 21:23:51,928   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 22
2019-07-09 21:23:51,944   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 21:23:51,991   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-07-09 21:23:52,007   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-07-09 21:23:52,007   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (runJob at SparkHadoopWriter.scala:78)
2019-07-09 21:23:52,007   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 4)
2019-07-09 21:23:52,007   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 21:23:52,007   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (MapPartitionsRDD[10] at saveAsTextFile at test.scala:51), which has no missing parents
2019-07-09 21:23:52,038   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 66.8 KB, free 1426.2 MB)
2019-07-09 21:23:52,053   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.2 KB, free 1426.2 MB)
2019-07-09 21:23:52,053   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:4648 (size: 24.2 KB, free: 1426.5 MB)
2019-07-09 21:23:52,053   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:23:52,069   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[10] at saveAsTextFile at test.scala:51) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:23:52,069   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-07-09 21:23:52,069   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 21:23:52,069   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 4)
2019-07-09 21:23:52,163   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 21:23:52,163   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 21:23:52,397   INFO --- [Executor task launch worker for task 4]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 21:23:52,569   INFO --- [Executor task launch worker for task 4]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190709212351_0010_m_000000_0' to file:/D:/data/output/_temporary/0/task_20190709212351_0010_m_000000
2019-07-09 21:23:52,569   INFO --- [Executor task launch worker for task 4]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190709212351_0010_m_000000_0: Committed
2019-07-09 21:23:52,569   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 4). 1422 bytes result sent to driver
2019-07-09 21:23:52,569   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 4) in 500 ms on localhost (executor driver) (1/1)
2019-07-09 21:23:52,569   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-07-09 21:23:52,585   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (runJob at SparkHadoopWriter.scala:78) finished in 0.578 s
2019-07-09 21:23:52,585   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: runJob at SparkHadoopWriter.scala:78, took 0.584582 s
2019-07-09 21:23:52,678   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190709212351_0010 committed.
2019-07-09 21:23:52,694   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 21:23:52,694   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 21:23:52,725   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 21:23:52,866   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 21:23:52,866   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 21:23:52,882   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 21:23:52,882   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 21:23:52,897   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 21:23:52,897   INFO --- [main]  WordCount$(line:58) : complete!
2019-07-09 21:23:52,897   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 21:23:52,897   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-195e5b58-a7f8-4d88-8a16-af03ba8bb8d2
2019-07-09 21:29:46,549   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 21:29:47,237   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 21:29:47,346   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 21:29:47,362   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 21:29:47,362   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 21:29:47,362   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 21:29:47,362   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 21:29:51,487   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 4707.
2019-07-09 21:29:51,533   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 21:29:51,565   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 21:29:51,565   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 21:29:51,565   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 21:29:51,580   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-29c3c4fd-8f86-4724-99eb-7774f0d9a877
2019-07-09 21:29:51,643   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 21:29:51,658   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 21:29:51,815   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16888ms
2019-07-09 21:29:51,893   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 21:29:51,908   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16995ms
2019-07-09 21:29:51,940   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 21:29:51,940   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 21:29:51,971   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 21:29:51,987   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,002   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,002   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,002   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,002   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,002   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,002   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,018   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,018   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,018   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,018   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,018   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 21:29:52,018   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 21:29:52,283   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 21:29:52,393   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4728.
2019-07-09 21:29:52,393   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:4728
2019-07-09 21:29:52,393   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 21:29:52,424   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 4728, None)
2019-07-09 21:29:52,440   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:4728 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 4728, None)
2019-07-09 21:29:52,440   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 4728, None)
2019-07-09 21:29:52,440   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 4728, None)
2019-07-09 21:29:52,721   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 21:29:54,549   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 21:29:54,690   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 21:29:54,705   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:4728 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 21:29:54,705   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:35
2019-07-09 21:29:55,065   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:52
2019-07-09 21:29:55,158   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 21:29:55,190   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:41)
2019-07-09 21:29:55,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:43)
2019-07-09 21:29:55,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:52) with 1 output partitions
2019-07-09 21:29:55,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:52)
2019-07-09 21:29:55,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 21:29:55,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 21:29:55,221   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41), which has no missing parents
2019-07-09 21:29:55,346   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 21:29:55,361   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 21:29:55,377   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:4728 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 21:29:55,377   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:29:55,408   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 21:29:55,408   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 21:29:55,518   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 21:29:55,518   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 21:29:55,549   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 21:29:55,549   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 21:29:56,565   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 21:29:56,565   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 21:29:57,674   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 21:29:57,674   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 21:29:57,690   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2204 ms on localhost (executor driver) (1/2)
2019-07-09 21:29:57,690   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2172 ms on localhost (executor driver) (2/2)
2019-07-09 21:29:57,690   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 21:29:57,705   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:41) finished in 2.422 s
2019-07-09 21:29:57,705   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 21:29:57,705   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 21:29:57,705   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 21:29:57,705   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 21:29:57,721   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43), which has no missing parents
2019-07-09 21:29:57,736   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 21:29:57,752   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 21:29:57,752   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:4728 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:29:57,752   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:29:57,752   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:29:57,752   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 21:29:57,768   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 21:29:57,768   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 21:29:57,815   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 21:29:57,815   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 21:29:58,205   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 21:29:58,205   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 437 ms on localhost (executor driver) (1/1)
2019-07-09 21:29:58,205   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 21:29:58,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:43) finished in 0.469 s
2019-07-09 21:29:58,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 21:29:58,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 21:29:58,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 21:29:58,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 21:29:58,205   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[10] at map at test.scala:45), which has no missing parents
2019-07-09 21:29:58,221   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 5.0 KB, free 1426.2 MB)
2019-07-09 21:29:58,221   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.2 MB)
2019-07-09 21:29:58,221   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:4728 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:29:58,236   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:29:58,236   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at map at test.scala:45) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:29:58,236   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 21:29:58,236   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 21:29:58,236   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 21:29:58,252   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 21:29:58,252   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 21:29:58,377   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:4728 in memory (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:29:58,393   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:4728 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 21:29:58,549   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 423075 bytes result sent to driver
2019-07-09 21:29:58,580   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 344 ms on localhost (executor driver) (1/1)
2019-07-09 21:29:58,580   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:52) finished in 0.359 s
2019-07-09 21:29:58,580   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 21:29:58,596   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:52, took 3.525383 s
2019-07-09 21:29:59,330   INFO --- [main]  org.apache.hadoop.conf.Configuration.deprecation(line:1129) : mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
2019-07-09 21:29:59,346   INFO --- [main]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 21:29:59,393   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: runJob at SparkHadoopWriter.scala:78
2019-07-09 21:29:59,393   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
2019-07-09 21:29:59,393   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 5 (runJob at SparkHadoopWriter.scala:78)
2019-07-09 21:29:59,393   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 4)
2019-07-09 21:29:59,393   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-09 21:29:59,408   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 5 (MapPartitionsRDD[11] at saveAsTextFile at test.scala:53), which has no missing parents
2019-07-09 21:29:59,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 67.3 KB, free 1426.2 MB)
2019-07-09 21:29:59,424   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 24.5 KB, free 1426.2 MB)
2019-07-09 21:29:59,424   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:4728 (size: 24.5 KB, free: 1426.5 MB)
2019-07-09 21:29:59,439   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:29:59,439   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at saveAsTextFile at test.scala:53) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:29:59,439   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 5.0 with 1 tasks
2019-07-09 21:29:59,439   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 5.0 (TID 4, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 21:29:59,439   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 5.0 (TID 4)
2019-07-09 21:29:59,549   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 21:29:59,549   INFO --- [Executor task launch worker for task 4]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 21:29:59,736   INFO --- [Executor task launch worker for task 4]  org.apache.spark.internal.io.HadoopMapRedCommitProtocol(line:54) : Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
2019-07-09 21:29:59,955   INFO --- [Executor task launch worker for task 4]  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter(line:439) : Saved output of task 'attempt_20190709212959_0011_m_000000_0' to file:/D:/data/output/_temporary/0/task_20190709212959_0011_m_000000
2019-07-09 21:29:59,955   INFO --- [Executor task launch worker for task 4]  org.apache.spark.mapred.SparkHadoopMapRedUtil(line:54) : attempt_20190709212959_0011_m_000000_0: Committed
2019-07-09 21:29:59,955   INFO --- [Executor task launch worker for task 4]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 5.0 (TID 4). 1422 bytes result sent to driver
2019-07-09 21:29:59,955   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 5.0 (TID 4) in 516 ms on localhost (executor driver) (1/1)
2019-07-09 21:29:59,955   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 5.0, whose tasks have all completed, from pool 
2019-07-09 21:29:59,971   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 5 (runJob at SparkHadoopWriter.scala:78) finished in 0.563 s
2019-07-09 21:29:59,971   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: runJob at SparkHadoopWriter.scala:78, took 0.578084 s
2019-07-09 21:30:00,049   INFO --- [main]  org.apache.spark.internal.io.SparkHadoopWriter(line:54) : Job job_20190709212959_0011 committed.
2019-07-09 21:30:00,064   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 21:30:00,064   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 21:30:00,080   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 21:30:00,221   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 21:30:00,221   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 21:30:00,221   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 21:30:00,236   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 21:30:00,252   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 21:30:00,252   INFO --- [main]  WordCount$(line:60) : complete!
2019-07-09 21:30:00,252   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 21:30:00,252   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-af316d6d-044e-49a2-8145-c75b6f5630b6
2019-07-09 21:51:18,433   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-09 21:51:19,058   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-09 21:51:19,167   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-09 21:51:19,167   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-09 21:51:19,167   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-09 21:51:19,167   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-09 21:51:19,167   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-09 21:51:23,026   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 4866.
2019-07-09 21:51:23,073   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-09 21:51:23,104   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-09 21:51:23,104   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-09 21:51:23,104   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-09 21:51:23,120   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-0ba0e2be-e0f6-45ed-b38e-2d46661431da
2019-07-09 21:51:23,167   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-09 21:51:23,182   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-09 21:51:23,307   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16625ms
2019-07-09 21:51:23,401   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-09 21:51:23,417   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16731ms
2019-07-09 21:51:23,448   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 21:51:23,448   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-09 21:51:23,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,479   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,495   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,511   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,511   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,511   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,511   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,511   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-09 21:51:23,511   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-09 21:51:23,761   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-09 21:51:23,870   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4887.
2019-07-09 21:51:23,870   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:4887
2019-07-09 21:51:23,870   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-09 21:51:23,932   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 4887, None)
2019-07-09 21:51:23,932   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:4887 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 4887, None)
2019-07-09 21:51:23,932   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 4887, None)
2019-07-09 21:51:23,932   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 4887, None)
2019-07-09 21:51:24,182   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-09 21:51:25,057   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-09 21:51:25,182   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-09 21:51:25,198   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:4887 (size: 20.4 KB, free: 1426.5 MB)
2019-07-09 21:51:25,198   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at test.scala:35
2019-07-09 21:51:25,542   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at test.scala:52
2019-07-09 21:51:25,651   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-09 21:51:25,682   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (map at test.scala:41)
2019-07-09 21:51:25,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 7 (sortBy at test.scala:43)
2019-07-09 21:51:25,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at test.scala:52) with 1 output partitions
2019-07-09 21:51:25,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at test.scala:52)
2019-07-09 21:51:25,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-09 21:51:25,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-09 21:51:25,714   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41), which has no missing parents
2019-07-09 21:51:25,839   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 1426.3 MB)
2019-07-09 21:51:25,854   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-09 21:51:25,854   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:4887 (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 21:51:25,854   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:51:25,885   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at map at test.scala:41) (first 15 tasks are for partitions Vector(0, 1))
2019-07-09 21:51:25,885   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-09 21:51:26,010   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7351 bytes)
2019-07-09 21:51:26,026   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 7351 bytes)
2019-07-09 21:51:26,057   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-09 21:51:26,057   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-09 21:51:26,901   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:0+2503492
2019-07-09 21:51:26,901   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: file:/D:/data/train.txt:2503492+2503493
2019-07-09 21:51:27,885   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-09 21:51:27,885   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-09 21:51:27,901   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1875 ms on localhost (executor driver) (1/2)
2019-07-09 21:51:27,901   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1922 ms on localhost (executor driver) (2/2)
2019-07-09 21:51:27,901   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-09 21:51:27,917   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at test.scala:41) finished in 2.125 s
2019-07-09 21:51:27,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 21:51:27,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 21:51:27,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-09 21:51:27,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 21:51:27,932   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43), which has no missing parents
2019-07-09 21:51:27,948   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-09 21:51:27,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.3 MB)
2019-07-09 21:51:27,964   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:4887 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:51:27,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:51:27,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at sortBy at test.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:51:27,964   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-09 21:51:27,979   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-09 21:51:27,979   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-09 21:51:28,026   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-09 21:51:28,026   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 16 ms
2019-07-09 21:51:28,401   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-09 21:51:28,401   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 422 ms on localhost (executor driver) (1/1)
2019-07-09 21:51:28,417   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-09 21:51:28,417   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at test.scala:43) finished in 0.469 s
2019-07-09 21:51:28,417   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-09 21:51:28,417   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-09 21:51:28,417   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-09 21:51:28,417   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-09 21:51:28,417   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[10] at map at test.scala:45), which has no missing parents
2019-07-09 21:51:28,417   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 5.0 KB, free 1426.2 MB)
2019-07-09 21:51:28,448   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.8 KB, free 1426.2 MB)
2019-07-09 21:51:28,448   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:4887 (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:51:28,463   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-09 21:51:28,463   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at map at test.scala:45) (first 15 tasks are for partitions Vector(0))
2019-07-09 21:51:28,463   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-09 21:51:28,463   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-09 21:51:28,463   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-09 21:51:28,479   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-09 21:51:28,479   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-09 21:51:28,698   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 423032 bytes result sent to driver
2019-07-09 21:51:28,713   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 250 ms on localhost (executor driver) (1/1)
2019-07-09 21:51:28,713   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at test.scala:52) finished in 0.296 s
2019-07-09 21:51:28,713   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-09 21:51:28,729   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at test.scala:52, took 3.187499 s
2019-07-09 21:51:28,932   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 68
2019-07-09 21:51:29,010   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 63
2019-07-09 21:51:29,010   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-09 21:51:29,010   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-07-09 21:51:29,010   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 66
2019-07-09 21:51:29,010   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 71
2019-07-09 21:51:29,010   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 11
2019-07-09 21:51:29,026   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 22
2019-07-09 21:51:29,026   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 1
2019-07-09 21:51:29,026   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-07-09 21:51:29,026   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 73
2019-07-09 21:51:29,026   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 4
2019-07-09 21:51:29,026   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 74
2019-07-09 21:51:29,026   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 64
2019-07-09 21:51:29,026   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-07-09 21:51:29,088   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on fc-pc:4887 in memory (size: 2.8 KB, free: 1426.5 MB)
2019-07-09 21:51:29,104   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Invoking stop() from shutdown hook
2019-07-09 21:51:29,120   INFO --- [Thread-1]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 15
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 67
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 70
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 9
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 12
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 60
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 20
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 10
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 14
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 69
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-07-09 21:51:29,135   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-07-09 21:51:29,151   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-07-09 21:51:29,151   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 5
2019-07-09 21:51:29,151   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-07-09 21:51:29,151   INFO --- [Thread-1]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-09 21:51:29,167   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:4887 in memory (size: 3.3 KB, free: 1426.5 MB)
2019-07-09 21:51:29,182   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-09 21:51:29,307   INFO --- [Thread-1]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-09 21:51:29,307   INFO --- [Thread-1]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-09 21:51:29,307   INFO --- [Thread-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-09 21:51:29,307   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-09 21:51:29,323   INFO --- [Thread-1]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-09 21:51:29,323   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-09 21:51:29,323   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-8731aa7e-de26-4e2a-b8a7-33a67faaa00e
2019-07-11 11:24:26,157   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-11 11:24:26,905   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: sqltest
2019-07-11 11:24:27,026   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-11 11:24:27,030   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-11 11:24:27,032   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-11 11:24:27,033   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-11 11:24:27,034   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-11 11:24:31,107   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 3376.
2019-07-11 11:24:31,150   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-11 11:24:31,189   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-11 11:24:31,194   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-11 11:24:31,196   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-11 11:24:31,218   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-0860d5e0-9560-4a1f-a5fd-7fd534cd2a7e
2019-07-11 11:24:31,268   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-11 11:24:31,296   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-11 11:24:31,460   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17013ms
2019-07-11 11:24:31,554   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-11 11:24:31,574   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17129ms
2019-07-11 11:24:31,613   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@5fe2f41b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-11 11:24:31,613   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-11 11:24:31,649   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,651   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,653   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,654   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,656   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,656   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,658   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,659   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,660   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,662   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,663   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,665   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,667   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,669   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,674   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,677   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,682   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,685   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,687   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,689   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,704   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,706   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,711   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,721   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,723   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-11 11:24:31,728   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-11 11:24:32,043   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-11 11:24:32,225   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 3397.
2019-07-11 11:24:32,229   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:3397
2019-07-11 11:24:32,238   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-11 11:24:32,346   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 3397, None)
2019-07-11 11:24:32,357   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:3397 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 3397, None)
2019-07-11 11:24:32,365   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 3397, None)
2019-07-11 11:24:32,366   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 3397, None)
2019-07-11 11:24:32,696   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:32,790   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-11 11:24:33,039   INFO --- [main]  org.apache.spark.sql.internal.SharedState(line:54) : Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/home/code/bigdata/bigdata-learning/06spark/spark-warehouse/').
2019-07-11 11:24:33,040   INFO --- [main]  org.apache.spark.sql.internal.SharedState(line:54) : Warehouse path is 'file:/D:/home/code/bigdata/bigdata-learning/06spark/spark-warehouse/'.
2019-07-11 11:24:33,060   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d8e2eea{/SQL,null,AVAILABLE,@Spark}
2019-07-11 11:24:33,061   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@240139e1{/SQL/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:33,063   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@8dfe921{/SQL/execution,null,AVAILABLE,@Spark}
2019-07-11 11:24:33,064   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@503fbbc6{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-07-11 11:24:33,067   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3dedb4a6{/static/sql,null,AVAILABLE,@Spark}
2019-07-11 11:24:34,251   INFO --- [main]  org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef(line:54) : Registered StateStoreCoordinator endpoint
2019-07-11 11:24:38,486   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Pruning directories with: 
2019-07-11 11:24:38,489   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Post-Scan Filters: 
2019-07-11 11:24:38,495   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Output Data Schema: struct<value: string>
2019-07-11 11:24:38,504   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Pushed Filters: 
2019-07-11 11:24:39,456   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 410.8284 ms
2019-07-11 11:24:39,632   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 219.8 KB, free 1426.3 MB)
2019-07-11 11:24:39,785   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1426.3 MB)
2019-07-11 11:24:39,790   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:3397 (size: 20.6 KB, free: 1426.5 MB)
2019-07-11 11:24:39,798   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from json at sqlTest.scala:19
2019-07-11 11:24:39,806   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-07-11 11:24:40,070   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: json at sqlTest.scala:19
2019-07-11 11:24:40,118   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (json at sqlTest.scala:19) with 1 output partitions
2019-07-11 11:24:40,119   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (json at sqlTest.scala:19)
2019-07-11 11:24:40,120   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-11 11:24:40,122   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-11 11:24:40,132   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at sqlTest.scala:19), which has no missing parents
2019-07-11 11:24:40,265   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 11.9 KB, free 1426.3 MB)
2019-07-11 11:24:40,271   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1426.2 MB)
2019-07-11 11:24:40,273   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:3397 (size: 6.1 KB, free: 1426.5 MB)
2019-07-11 11:24:40,274   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-11 11:24:40,301   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at sqlTest.scala:19) (first 15 tasks are for partitions Vector(0))
2019-07-11 11:24:40,303   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-07-11 11:24:40,410   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7853 bytes)
2019-07-11 11:24:40,432   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-11 11:24:41,049   INFO --- [Executor task launch worker for task 0]  org.apache.spark.sql.execution.datasources.FileScanRDD(line:54) : Reading File path: file:///D:/home/code/bigdata/bigdata-learning/06spark/sparksql/src/main/resources/employees.json, range: 0-130, partition values: [empty row]
2019-07-11 11:24:41,119   INFO --- [Executor task launch worker for task 0]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 32.9463 ms
2019-07-11 11:24:41,233   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1923 bytes result sent to driver
2019-07-11 11:24:41,251   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 876 ms on localhost (executor driver) (1/1)
2019-07-11 11:24:41,261   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-11 11:24:41,284   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (json at sqlTest.scala:19) finished in 1.114 s
2019-07-11 11:24:41,297   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: json at sqlTest.scala:19, took 1.227070 s
2019-07-11 11:24:41,453   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Pruning directories with: 
2019-07-11 11:24:41,454   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Post-Scan Filters: 
2019-07-11 11:24:41,455   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Output Data Schema: struct<name: string>
2019-07-11 11:24:41,456   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Pushed Filters: 
2019-07-11 11:24:41,576   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 36.0078 ms
2019-07-11 11:24:41,637   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 219.8 KB, free 1426.0 MB)
2019-07-11 11:24:41,661   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1426.0 MB)
2019-07-11 11:24:41,665   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:3397 (size: 20.6 KB, free: 1426.5 MB)
2019-07-11 11:24:41,668   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from show at sqlTest.scala:22
2019-07-11 11:24:41,672   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-07-11 11:24:41,739   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: show at sqlTest.scala:22
2019-07-11 11:24:41,742   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (show at sqlTest.scala:22) with 1 output partitions
2019-07-11 11:24:41,742   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (show at sqlTest.scala:22)
2019-07-11 11:24:41,743   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-11 11:24:41,744   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-11 11:24:41,745   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[6] at show at sqlTest.scala:22), which has no missing parents
2019-07-11 11:24:41,767   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 10.8 KB, free 1426.0 MB)
2019-07-11 11:24:41,778   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KB, free 1426.0 MB)
2019-07-11 11:24:41,780   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:3397 (size: 5.8 KB, free: 1426.4 MB)
2019-07-11 11:24:41,781   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-11 11:24:41,784   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at sqlTest.scala:22) (first 15 tasks are for partitions Vector(0))
2019-07-11 11:24:41,784   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-11 11:24:41,788   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7853 bytes)
2019-07-11 11:24:41,790   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-07-11 11:24:41,860   INFO --- [Executor task launch worker for task 1]  org.apache.spark.sql.execution.datasources.FileScanRDD(line:54) : Reading File path: file:///D:/home/code/bigdata/bigdata-learning/06spark/sparksql/src/main/resources/employees.json, range: 0-130, partition values: [empty row]
2019-07-11 11:24:41,901   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1216 bytes result sent to driver
2019-07-11 11:24:41,908   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 120 ms on localhost (executor driver) (1/1)
2019-07-11 11:24:41,910   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (show at sqlTest.scala:22) finished in 0.156 s
2019-07-11 11:24:41,918   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: show at sqlTest.scala:22, took 0.177101 s
2019-07-11 11:24:41,924   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-11 11:24:42,704   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Pruning directories with: 
2019-07-11 11:24:42,705   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Post-Scan Filters: 
2019-07-11 11:24:42,706   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Output Data Schema: struct<name: string, salary: bigint>
2019-07-11 11:24:42,707   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Pushed Filters: 
2019-07-11 11:24:42,776   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_0_piece0 on fc-pc:3397 in memory (size: 20.6 KB, free: 1426.5 MB)
2019-07-11 11:24:42,787   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 28.343 ms
2019-07-11 11:24:42,798   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 10
2019-07-11 11:24:42,798   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 20
2019-07-11 11:24:42,799   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-07-11 11:24:42,799   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 18
2019-07-11 11:24:42,799   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-11 11:24:42,800   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-07-11 11:24:42,800   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-11 11:24:42,801   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-07-11 11:24:42,801   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 21
2019-07-11 11:24:42,802   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 19
2019-07-11 11:24:42,804   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 54
2019-07-11 11:24:42,805   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-07-11 11:24:42,805   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 11
2019-07-11 11:24:42,805   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-07-11 11:24:42,805   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-07-11 11:24:42,807   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-11 11:24:42,807   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-07-11 11:24:42,808   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 9
2019-07-11 11:24:42,814   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:3397 in memory (size: 6.1 KB, free: 1426.5 MB)
2019-07-11 11:24:42,820   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-07-11 11:24:42,821   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-07-11 11:24:42,822   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-07-11 11:24:42,827   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 22
2019-07-11 11:24:42,849   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 7
2019-07-11 11:24:42,850   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 6
2019-07-11 11:24:42,851   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 15
2019-07-11 11:24:42,851   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-07-11 11:24:42,851   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 58
2019-07-11 11:24:42,852   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-07-11 11:24:42,852   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-07-11 11:24:42,852   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 16
2019-07-11 11:24:42,865   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 39.6195 ms
2019-07-11 11:24:42,874   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:3397 in memory (size: 20.6 KB, free: 1426.5 MB)
2019-07-11 11:24:42,877   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 219.7 KB, free 1426.3 MB)
2019-07-11 11:24:42,894   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 4
2019-07-11 11:24:42,895   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-11 11:24:42,895   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-07-11 11:24:42,895   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-07-11 11:24:42,897   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 13
2019-07-11 11:24:42,898   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 55
2019-07-11 11:24:42,899   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-07-11 11:24:42,900   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-07-11 11:24:42,900   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-07-11 11:24:42,900   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-07-11 11:24:42,901   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 14
2019-07-11 11:24:42,901   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 12
2019-07-11 11:24:42,901   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-07-11 11:24:42,901   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 5
2019-07-11 11:24:42,902   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 50
2019-07-11 11:24:42,902   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 23
2019-07-11 11:24:42,902   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 2
2019-07-11 11:24:42,902   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 0
2019-07-11 11:24:42,903   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-07-11 11:24:42,903   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-07-11 11:24:42,903   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 8
2019-07-11 11:24:42,903   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-07-11 11:24:42,904   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-07-11 11:24:42,904   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 24
2019-07-11 11:24:42,923   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1426.2 MB)
2019-07-11 11:24:42,929   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:3397 (size: 20.6 KB, free: 1426.5 MB)
2019-07-11 11:24:42,930   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from show at sqlTest.scala:26
2019-07-11 11:24:42,931   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-07-11 11:24:42,974   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on fc-pc:3397 in memory (size: 5.8 KB, free: 1426.5 MB)
2019-07-11 11:24:42,979   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: show at sqlTest.scala:26
2019-07-11 11:24:42,984   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (show at sqlTest.scala:26) with 1 output partitions
2019-07-11 11:24:42,985   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (show at sqlTest.scala:26)
2019-07-11 11:24:42,985   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-11 11:24:42,986   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-11 11:24:42,988   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[10] at show at sqlTest.scala:26), which has no missing parents
2019-07-11 11:24:42,994   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 12.0 KB, free 1426.3 MB)
2019-07-11 11:24:43,001   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1426.2 MB)
2019-07-11 11:24:43,005   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on fc-pc:3397 (size: 6.3 KB, free: 1426.5 MB)
2019-07-11 11:24:43,006   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-07-11 11:24:43,008   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at show at sqlTest.scala:26) (first 15 tasks are for partitions Vector(0))
2019-07-11 11:24:43,009   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-11 11:24:43,015   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7853 bytes)
2019-07-11 11:24:43,016   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-07-11 11:24:43,041   INFO --- [Executor task launch worker for task 2]  org.apache.spark.sql.execution.datasources.FileScanRDD(line:54) : Reading File path: file:///D:/home/code/bigdata/bigdata-learning/06spark/sparksql/src/main/resources/employees.json, range: 0-130, partition values: [empty row]
2019-07-11 11:24:43,049   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-07-11 11:24:43,050   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-07-11 11:24:43,050   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 17
2019-07-11 11:24:43,050   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-07-11 11:24:43,051   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 3
2019-07-11 11:24:43,051   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 1
2019-07-11 11:24:43,082   INFO --- [Executor task launch worker for task 2]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 29.2669 ms
2019-07-11 11:24:43,093   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1246 bytes result sent to driver
2019-07-11 11:24:43,096   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 81 ms on localhost (executor driver) (1/1)
2019-07-11 11:24:43,097   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-11 11:24:43,098   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (show at sqlTest.scala:26) finished in 0.107 s
2019-07-11 11:24:43,102   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: show at sqlTest.scala:26, took 0.121116 s
2019-07-11 11:24:43,116   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@5fe2f41b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-11 11:24:43,119   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-11 11:24:43,144   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-11 11:24:43,270   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-11 11:24:43,271   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-11 11:24:43,275   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-11 11:24:43,282   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-11 11:24:43,306   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-11 11:24:43,307   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-07-11 11:24:43,312   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-11 11:24:43,314   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-564a7c43-7c92-475d-973d-04a652352bcc
2019-07-11 11:25:53,307   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-13 22:27:41,099   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-13 22:27:41,730   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: HelloWorld
2019-07-13 22:27:41,844   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-13 22:27:41,845   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-13 22:27:41,846   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-13 22:27:41,848   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-13 22:27:41,850   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-13 22:27:45,213   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 5775.
2019-07-13 22:27:45,257   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-13 22:27:45,415   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-13 22:27:45,419   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-13 22:27:45,420   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-13 22:27:45,443   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-af0c1602-61bb-4f0f-b89d-6a89117ef428
2019-07-13 22:27:45,491   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-13 22:27:45,520   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-13 22:27:45,663   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17142ms
2019-07-13 22:27:45,753   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-13 22:27:45,772   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17252ms
2019-07-13 22:27:45,803   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@3c8fd55a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-13 22:27:45,804   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-13 22:27:45,851   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4264b240{/jobs,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,853   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/jobs/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,859   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/jobs/job,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,863   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,865   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/stages,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,866   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/stages/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,868   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/stages/stage,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,879   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,889   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38be305c{/stages/pool,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,893   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@269f4bad{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,897   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ed731d0{/storage,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,898   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3234f74e{/storage/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,901   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7bc10d84{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,903   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@275fe372{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,905   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@40e10ff8{/environment,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,907   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@557a1e2d{/environment/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,910   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@26a4842b{/executors,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,912   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7e38a7fe{/executors/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,915   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@366ef90e{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,917   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@33e01298{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,929   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@31e75d13{/static,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,931   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c80e49b{/,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,937   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@458342d3{/api,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,938   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56276db8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,940   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@51e8e6e6{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-13 22:27:45,946   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-13 22:27:46,215   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-13 22:27:46,323   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 5796.
2019-07-13 22:27:46,325   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:5796
2019-07-13 22:27:46,329   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-13 22:27:46,372   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 5796, None)
2019-07-13 22:27:46,382   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:5796 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 5796, None)
2019-07-13 22:27:46,387   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 5796, None)
2019-07-13 22:27:46,388   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 5796, None)
2019-07-13 22:27:46,658   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@492fc69e{/metrics/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:46,990   INFO --- [main]  org.apache.spark.sql.internal.SharedState(line:54) : Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/home/code/bigdata/bigdata-learning/06spark/spark-warehouse/').
2019-07-13 22:27:46,992   INFO --- [main]  org.apache.spark.sql.internal.SharedState(line:54) : Warehouse path is 'file:/D:/home/code/bigdata/bigdata-learning/06spark/spark-warehouse/'.
2019-07-13 22:27:47,018   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7f34a967{/SQL,null,AVAILABLE,@Spark}
2019-07-13 22:27:47,020   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@77e80a5e{/SQL/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:47,022   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@49298ce7{/SQL/execution,null,AVAILABLE,@Spark}
2019-07-13 22:27:47,024   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@253c1256{/SQL/execution/json,null,AVAILABLE,@Spark}
2019-07-13 22:27:47,027   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@67fe380b{/static/sql,null,AVAILABLE,@Spark}
2019-07-13 22:27:48,112   INFO --- [main]  org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef(line:54) : Registered StateStoreCoordinator endpoint
2019-07-13 22:27:54,775   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Pruning directories with: 
2019-07-13 22:27:54,778   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Post-Scan Filters: 
2019-07-13 22:27:54,784   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Output Data Schema: struct<value: string>
2019-07-13 22:27:54,795   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Pushed Filters: 
2019-07-13 22:27:55,922   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 403.9665 ms
2019-07-13 22:27:56,085   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 219.9 KB, free 1426.3 MB)
2019-07-13 22:27:56,237   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1426.3 MB)
2019-07-13 22:27:56,243   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:5796 (size: 20.6 KB, free: 1426.5 MB)
2019-07-13 22:27:56,249   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from json at HelloWorld.scala:37
2019-07-13 22:27:56,257   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-07-13 22:27:56,539   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: json at HelloWorld.scala:37
2019-07-13 22:27:56,585   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (json at HelloWorld.scala:37) with 1 output partitions
2019-07-13 22:27:56,586   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (json at HelloWorld.scala:37)
2019-07-13 22:27:56,587   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-13 22:27:56,589   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-13 22:27:56,598   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (MapPartitionsRDD[2] at json at HelloWorld.scala:37), which has no missing parents
2019-07-13 22:27:56,711   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 11.9 KB, free 1426.3 MB)
2019-07-13 22:27:56,716   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.1 KB, free 1426.2 MB)
2019-07-13 22:27:56,718   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:5796 (size: 6.1 KB, free: 1426.5 MB)
2019-07-13 22:27:56,719   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-13 22:27:56,738   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at HelloWorld.scala:37) (first 15 tasks are for partitions Vector(0))
2019-07-13 22:27:56,739   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 1 tasks
2019-07-13 22:27:56,837   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7853 bytes)
2019-07-13 22:27:56,863   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-13 22:27:57,411   INFO --- [Executor task launch worker for task 0]  org.apache.spark.sql.execution.datasources.FileScanRDD(line:54) : Reading File path: file:///D:/home/code/bigdata/bigdata-learning/06spark/sparksql/src/main/resources/employees.json, range: 0-130, partition values: [empty row]
2019-07-13 22:27:57,474   INFO --- [Executor task launch worker for task 0]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 30.5437 ms
2019-07-13 22:27:57,564   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1923 bytes result sent to driver
2019-07-13 22:27:57,579   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 772 ms on localhost (executor driver) (1/1)
2019-07-13 22:27:57,586   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-13 22:27:57,599   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 0 (json at HelloWorld.scala:37) finished in 0.960 s
2019-07-13 22:27:57,608   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: json at HelloWorld.scala:37, took 1.068821 s
2019-07-13 22:27:57,769   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Pruning directories with: 
2019-07-13 22:27:57,770   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Post-Scan Filters: 
2019-07-13 22:27:57,770   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Output Data Schema: struct<name: string>
2019-07-13 22:27:57,771   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Pushed Filters: 
2019-07-13 22:27:57,872   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 32.4424 ms
2019-07-13 22:27:57,928   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 219.9 KB, free 1426.0 MB)
2019-07-13 22:27:57,952   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1426.0 MB)
2019-07-13 22:27:57,956   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:5796 (size: 20.6 KB, free: 1426.5 MB)
2019-07-13 22:27:57,958   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from show at HelloWorld.scala:42
2019-07-13 22:27:57,961   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-07-13 22:27:58,054   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: show at HelloWorld.scala:42
2019-07-13 22:27:58,061   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 1 (show at HelloWorld.scala:42) with 1 output partitions
2019-07-13 22:27:58,062   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 1 (show at HelloWorld.scala:42)
2019-07-13 22:27:58,062   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-13 22:27:58,063   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-13 22:27:58,064   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 1 (MapPartitionsRDD[6] at show at HelloWorld.scala:42), which has no missing parents
2019-07-13 22:27:58,090   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 10.8 KB, free 1426.0 MB)
2019-07-13 22:27:58,097   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KB, free 1426.0 MB)
2019-07-13 22:27:58,098   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:5796 (size: 5.8 KB, free: 1426.4 MB)
2019-07-13 22:27:58,102   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-13 22:27:58,107   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at HelloWorld.scala:42) (first 15 tasks are for partitions Vector(0))
2019-07-13 22:27:58,108   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-13 22:27:58,111   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 7853 bytes)
2019-07-13 22:27:58,113   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 1)
2019-07-13 22:27:58,159   INFO --- [Executor task launch worker for task 1]  org.apache.spark.sql.execution.datasources.FileScanRDD(line:54) : Reading File path: file:///D:/home/code/bigdata/bigdata-learning/06spark/sparksql/src/main/resources/employees.json, range: 0-130, partition values: [empty row]
2019-07-13 22:27:58,192   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 1). 1216 bytes result sent to driver
2019-07-13 22:27:58,196   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 1) in 85 ms on localhost (executor driver) (1/1)
2019-07-13 22:27:58,197   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 1 (show at HelloWorld.scala:42) finished in 0.121 s
2019-07-13 22:27:58,204   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 1 finished: show at HelloWorld.scala:42, took 0.149303 s
2019-07-13 22:27:58,211   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-13 22:27:58,922   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 49
2019-07-13 22:27:58,958   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Pruning directories with: 
2019-07-13 22:27:58,958   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Post-Scan Filters: 
2019-07-13 22:27:58,959   INFO --- [main]  org.apache.spark.sql.execution.datasources.FileSourceStrategy(line:54) : Output Data Schema: struct<name: string, salary: bigint>
2019-07-13 22:27:58,960   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Pushed Filters: 
2019-07-13 22:27:58,985   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:5796 in memory (size: 6.1 KB, free: 1426.5 MB)
2019-07-13 22:27:59,001   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 39
2019-07-13 22:27:59,001   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 26
2019-07-13 22:27:59,001   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 11
2019-07-13 22:27:59,002   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 10
2019-07-13 22:27:59,002   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 23
2019-07-13 22:27:59,002   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 54
2019-07-13 22:27:59,002   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 19
2019-07-13 22:27:59,003   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 50
2019-07-13 22:27:59,003   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 6
2019-07-13 22:27:59,004   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 29
2019-07-13 22:27:59,004   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 56
2019-07-13 22:27:59,004   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 52
2019-07-13 22:27:59,005   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 21
2019-07-13 22:27:59,005   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 20
2019-07-13 22:27:59,005   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 59
2019-07-13 22:27:59,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 43
2019-07-13 22:27:59,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 51
2019-07-13 22:27:59,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 15
2019-07-13 22:27:59,006   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 12
2019-07-13 22:27:59,007   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 22
2019-07-13 22:27:59,007   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 32
2019-07-13 22:27:59,007   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 31
2019-07-13 22:27:59,007   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 55
2019-07-13 22:27:59,008   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 24
2019-07-13 22:27:59,018   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_0_piece0 on fc-pc:5796 in memory (size: 20.6 KB, free: 1426.5 MB)
2019-07-13 22:27:59,030   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 13
2019-07-13 22:27:59,031   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 25
2019-07-13 22:27:59,032   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 0
2019-07-13 22:27:59,032   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 7
2019-07-13 22:27:59,032   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 57
2019-07-13 22:27:59,033   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 42
2019-07-13 22:27:59,033   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 40
2019-07-13 22:27:59,033   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 47
2019-07-13 22:27:59,033   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 18
2019-07-13 22:27:59,034   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 8
2019-07-13 22:27:59,034   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 46
2019-07-13 22:27:59,034   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 17
2019-07-13 22:27:59,034   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 36
2019-07-13 22:27:59,035   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 4
2019-07-13 22:27:59,035   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 27
2019-07-13 22:27:59,036   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 45
2019-07-13 22:27:59,037   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 2
2019-07-13 22:27:59,037   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 28
2019-07-13 22:27:59,037   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 41
2019-07-13 22:27:59,041   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 53
2019-07-13 22:27:59,041   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 35
2019-07-13 22:27:59,042   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 37
2019-07-13 22:27:59,042   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 58
2019-07-13 22:27:59,042   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 48
2019-07-13 22:27:59,056   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 32.4046 ms
2019-07-13 22:27:59,070   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_3_piece0 on fc-pc:5796 in memory (size: 5.8 KB, free: 1426.5 MB)
2019-07-13 22:27:59,077   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 33
2019-07-13 22:27:59,077   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 1
2019-07-13 22:27:59,077   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 14
2019-07-13 22:27:59,078   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 5
2019-07-13 22:27:59,078   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 9
2019-07-13 22:27:59,078   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 30
2019-07-13 22:27:59,078   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 16
2019-07-13 22:27:59,078   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 38
2019-07-13 22:27:59,079   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 44
2019-07-13 22:27:59,079   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 34
2019-07-13 22:27:59,084   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on fc-pc:5796 in memory (size: 20.6 KB, free: 1426.5 MB)
2019-07-13 22:27:59,095   INFO --- [Spark Context Cleaner]  org.apache.spark.ContextCleaner(line:54) : Cleaned accumulator 3
2019-07-13 22:27:59,151   INFO --- [main]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 52.5016 ms
2019-07-13 22:27:59,164   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4 stored as values in memory (estimated size 219.8 KB, free 1426.3 MB)
2019-07-13 22:27:59,193   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.6 KB, free 1426.3 MB)
2019-07-13 22:27:59,200   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_4_piece0 in memory on fc-pc:5796 (size: 20.6 KB, free: 1426.5 MB)
2019-07-13 22:27:59,202   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 4 from show at HelloWorld.scala:46
2019-07-13 22:27:59,203   INFO --- [main]  org.apache.spark.sql.execution.FileSourceScanExec(line:54) : Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2019-07-13 22:27:59,229   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: show at HelloWorld.scala:46
2019-07-13 22:27:59,233   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 2 (show at HelloWorld.scala:46) with 1 output partitions
2019-07-13 22:27:59,233   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (show at HelloWorld.scala:46)
2019-07-13 22:27:59,234   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-13 22:27:59,236   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-13 22:27:59,238   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[10] at show at HelloWorld.scala:46), which has no missing parents
2019-07-13 22:27:59,246   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5 stored as values in memory (estimated size 12.0 KB, free 1426.3 MB)
2019-07-13 22:27:59,253   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.3 KB, free 1426.2 MB)
2019-07-13 22:27:59,258   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_5_piece0 in memory on fc-pc:5796 (size: 6.3 KB, free: 1426.5 MB)
2019-07-13 22:27:59,259   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 5 from broadcast at DAGScheduler.scala:1161
2019-07-13 22:27:59,260   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at show at HelloWorld.scala:46) (first 15 tasks are for partitions Vector(0))
2019-07-13 22:27:59,261   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-13 22:27:59,264   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7853 bytes)
2019-07-13 22:27:59,265   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 2)
2019-07-13 22:27:59,305   INFO --- [Executor task launch worker for task 2]  org.apache.spark.sql.execution.datasources.FileScanRDD(line:54) : Reading File path: file:///D:/home/code/bigdata/bigdata-learning/06spark/sparksql/src/main/resources/employees.json, range: 0-130, partition values: [empty row]
2019-07-13 22:27:59,329   INFO --- [Executor task launch worker for task 2]  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator(line:54) : Code generated in 19.429 ms
2019-07-13 22:27:59,341   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 2). 1203 bytes result sent to driver
2019-07-13 22:27:59,344   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 2) in 81 ms on localhost (executor driver) (1/1)
2019-07-13 22:27:59,346   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (show at HelloWorld.scala:46) finished in 0.106 s
2019-07-13 22:27:59,348   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 2 finished: show at HelloWorld.scala:46, took 0.118241 s
2019-07-13 22:27:59,378   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-13 22:27:59,379   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@3c8fd55a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-13 22:27:59,382   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-13 22:27:59,418   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-13 22:27:59,529   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-13 22:27:59,529   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-13 22:27:59,532   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-13 22:27:59,537   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-13 22:27:59,553   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-13 22:27:59,559   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-13 22:27:59,560   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-28b3c7fb-46a4-4882-ba8b-6b59cd2235d9
2019-07-13 22:33:13,371   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-13 22:33:14,022   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: HelloWorld
2019-07-13 22:33:14,140   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-13 22:33:14,142   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-13 22:33:14,143   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-13 22:33:14,144   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-13 22:33:14,145   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 10:05:27,722   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-14 10:23:22,758  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-07-14 10:23:22,758   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-07-14 10:23:22,883   WARN --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-07-14 10:23:22,945   WARN --- [stop-spark-context]  org.apache.spark.metrics.MetricsSystem(line:66) : Stopping a MetricsSystem that is not running
2019-07-14 10:23:23,336  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.NullPointerException
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:560)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$5(SparkSession.scala:935)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)
	at DataFrames1$.main(DataFrames1.scala:18)
	at DataFrames1.main(DataFrames1.scala)
2019-07-14 10:23:23,351  ERROR --- [spark-listener-group-appStatus]  org.apache.spark.scheduler.AsyncEventQueue(line:91) : Listener AppStatusListener threw an exception
java.lang.NullPointerException
	at org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:167)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:91)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:76)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:92)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:83)
2019-07-14 10:26:24,332   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 10:26:25,129   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 10:26:25,254   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 10:26:25,269   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 10:26:25,269   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 10:26:25,269   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 10:26:25,269   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 10:26:29,160   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 7563.
2019-07-14 10:26:29,191   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 10:26:29,238   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 10:26:29,238   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 10:26:29,238   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 10:26:29,253   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-1f800211-088a-4910-bc03-15757aea05d2
2019-07-14 10:26:29,300   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 10:26:29,316   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 10:26:29,441   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16602ms
2019-07-14 10:26:29,535   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 10:26:29,550   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16710ms
2019-07-14 10:26:29,597   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@3a696a21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 10:26:29,597   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 10:26:29,628   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,628   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,628   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,644   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,644   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,644   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,644   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,644   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,644   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,644   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,644   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,644   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,660   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,660   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,660   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,675   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,691   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,691   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,691   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,691   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,707   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,707   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,707   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,707   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,722   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 10:26:29,722   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-14 10:26:30,003   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:26:30,082   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 44 ms (0 ms spent in bootstraps)
2019-07-14 10:26:50,013   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:27:10,016   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:27:30,028  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-07-14 10:27:30,029   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-07-14 10:27:30,055   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@3a696a21{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 10:27:30,060   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-14 10:27:30,069   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7598.
2019-07-14 10:27:30,070   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:7598
2019-07-14 10:27:30,074   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 10:27:30,077   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-14 10:27:30,085   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-14 10:27:30,097   WARN --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-07-14 10:27:30,118   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 10:27:30,143   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 10:27:30,144   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 10:27:30,147   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 7598, None)
2019-07-14 10:27:30,158   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 10:27:30,159   WARN --- [stop-spark-context]  org.apache.spark.metrics.MetricsSystem(line:66) : Stopping a MetricsSystem that is not running
2019-07-14 10:27:30,162   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:7598 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 7598, None)
2019-07-14 10:27:30,165   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 10:27:30,171   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 7598, None)
2019-07-14 10:27:30,173   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 7598, None)
2019-07-14 10:27:30,185   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 10:27:30,564   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@42deb43a{/metrics/json,null,AVAILABLE,@Spark}
2019-07-14 10:27:30,577  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.NullPointerException
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:560)
	at WordCount$.main(WordCount.scala:26)
	at WordCount.main(WordCount.scala)
2019-07-14 10:27:30,581   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-07-14 10:27:30,589  ERROR --- [spark-listener-group-appStatus]  org.apache.spark.scheduler.AsyncEventQueue(line:91) : Listener AppStatusListener threw an exception
java.lang.NullPointerException
	at org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:167)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:91)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:76)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:92)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:83)
2019-07-14 10:27:30,590   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 10:27:30,592   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-b05abf48-5ff0-402a-bdb3-644f905324d7
2019-07-14 10:32:11,236   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 10:32:11,846   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 10:32:11,955   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 10:32:11,955   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 10:32:11,955   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 10:32:11,955   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 10:32:11,955   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 10:32:15,736   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 7645.
2019-07-14 10:32:15,783   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 10:32:15,814   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 10:32:15,814   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 10:32:15,830   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 10:32:15,846   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-06e298f1-b580-4912-9681-6ee476947702
2019-07-14 10:32:15,877   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 10:32:15,908   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 10:32:16,049   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16296ms
2019-07-14 10:32:16,127   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 10:32:16,158   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16407ms
2019-07-14 10:32:16,189   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@6111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 10:32:16,189   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 10:32:16,221   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ce33a58{/jobs,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,221   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,221   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,221   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@44828f6b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/storage,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,236   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/environment,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/executors,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38be305c{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@269f4bad{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ed731d0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3234f74e{/static,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,268   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/api,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,268   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b64c4b7{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,268   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4763c727{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 10:32:16,268   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-14 10:32:16,346   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR D:\home\code\bigdata\bigdata-learning\06spark\sparkcore\target\sparkcore-1.0-SNAPSHOT.jar at spark://fc-pc:7645/jars/sparkcore-1.0-SNAPSHOT.jar with timestamp 1563071536346
2019-07-14 10:32:16,627   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:32:16,705   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 47 ms (0 ms spent in bootstraps)
2019-07-14 10:32:36,642   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:32:56,657   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:33:16,672  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-07-14 10:33:16,672   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-07-14 10:33:16,703   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@6111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 10:33:16,719   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-14 10:33:16,719   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7669.
2019-07-14 10:33:16,719   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:7669
2019-07-14 10:33:16,719   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 10:33:16,734   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-14 10:33:16,734   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-14 10:33:16,750   WARN --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-07-14 10:33:16,765   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 10:33:16,797   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 7669, None)
2019-07-14 10:33:16,797   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 10:33:16,797   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 10:33:16,812   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:7669 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 7669, None)
2019-07-14 10:33:16,812   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 7669, None)
2019-07-14 10:33:16,812   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 7669, None)
2019-07-14 10:33:16,828   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 10:33:16,828   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 10:33:16,859   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 10:33:17,234  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem
	at scala.Predef$.require(Predef.scala:277)
	at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:91)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:516)
	at WordCount$.main(WordCount.scala:29)
	at WordCount.main(WordCount.scala)
2019-07-14 10:33:17,234   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-07-14 10:33:17,234   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 10:33:17,234   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-79b412b7-bc57-47eb-a335-d7d65be537d5
2019-07-14 10:44:02,556   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 10:44:03,150   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 10:44:03,275   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 10:44:03,275   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 10:44:03,275   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 10:44:03,290   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 10:44:03,290   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 10:44:06,993   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,009   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,009   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,025   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,025   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,040   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,040   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,056   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,056   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,072   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,072   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,087   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,087   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,103   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,103   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,118   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2019-07-14 10:44:07,134  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.net.BindException: Cannot assign requested address: bind: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:433)
	at sun.nio.ch.Net.bind(Net.java:425)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
	at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)
	at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)
	at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)
	at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)
	at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)
	at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)
	at java.lang.Thread.run(Thread.java:748)
2019-07-14 10:44:07,150   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 10:48:13,794   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 10:48:14,583   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 10:48:14,711   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 10:48:14,714   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 10:48:14,716   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 10:48:14,717   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 10:48:14,718   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 10:48:18,818   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 8101.
2019-07-14 10:48:18,864   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 10:48:18,901   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 10:48:18,906   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 10:48:18,907   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 10:48:18,932   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-0b03545b-e066-412f-acea-626018f1c7a0
2019-07-14 10:48:18,985   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 10:48:19,012   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 10:48:19,186   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17440ms
2019-07-14 10:48:19,294   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 10:48:19,315   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17572ms
2019-07-14 10:48:19,357   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@cf82e89{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 10:48:19,358   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 10:48:19,405   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,407   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,408   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,410   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,411   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,413   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,415   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,418   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,419   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,420   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,422   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,423   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,425   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,426   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,427   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,429   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,431   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,434   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,435   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,437   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,451   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,452   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,454   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,456   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,461   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 10:48:19,466   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-14 10:48:19,786   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:48:19,864   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 44 ms (0 ms spent in bootstraps)
2019-07-14 10:48:39,805   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:48:59,849   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:49:19,863  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-07-14 10:49:19,863   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-07-14 10:49:19,894   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@cf82e89{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 10:49:19,894   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-14 10:49:19,910   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8130.
2019-07-14 10:49:19,910   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:8130
2019-07-14 10:49:19,910   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 10:49:19,910   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-14 10:49:19,926   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-14 10:49:19,926   WARN --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-07-14 10:49:19,941   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 10:49:19,973   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 10:49:19,973   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 10:49:19,973   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 8130, None)
2019-07-14 10:49:19,988   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 10:49:19,988   WARN --- [stop-spark-context]  org.apache.spark.metrics.MetricsSystem(line:66) : Stopping a MetricsSystem that is not running
2019-07-14 10:49:19,988   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:8130 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 8130, None)
2019-07-14 10:49:19,988   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 10:49:19,988   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 8130, None)
2019-07-14 10:49:19,988   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 8130, None)
2019-07-14 10:49:20,004   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 10:49:20,488   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@42deb43a{/metrics/json,null,AVAILABLE,@Spark}
2019-07-14 10:49:20,488  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.NullPointerException
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:560)
	at WordCount$.main(WordCount.scala:28)
	at WordCount.main(WordCount.scala)
2019-07-14 10:49:20,504   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-07-14 10:49:20,504   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 10:49:20,504   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-1d7da5b1-c7d6-476a-b73e-c6654ccedbdb
2019-07-14 10:49:20,519  ERROR --- [spark-listener-group-appStatus]  org.apache.spark.scheduler.AsyncEventQueue(line:91) : Listener AppStatusListener threw an exception
java.lang.NullPointerException
	at org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:167)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:91)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:76)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:92)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:83)
2019-07-14 10:55:37,326   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 10:55:37,936   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 10:55:38,029   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 10:55:38,045   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 10:55:38,045   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 10:55:38,045   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 10:55:38,045   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 10:55:42,329   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 8309.
2019-07-14 10:55:42,377   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 10:55:42,415   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 10:55:42,420   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 10:55:42,421   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 10:55:42,440   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-5792f72e-ff71-4810-b931-254985a4249c
2019-07-14 10:55:42,482   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 10:55:42,504   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 10:55:42,629   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17240ms
2019-07-14 10:55:42,711   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 10:55:42,727   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17342ms
2019-07-14 10:55:42,767   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@6111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 10:55:42,767   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 10:55:42,807   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ce33a58{/jobs,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,809   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,810   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,813   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@44828f6b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,814   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,816   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,817   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,819   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,821   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,824   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/storage,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,828   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,831   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,833   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,835   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/environment,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,837   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,838   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/executors,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,840   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38be305c{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,842   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@269f4bad{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,844   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ed731d0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,865   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3234f74e{/static,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,868   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,871   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/api,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,873   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b64c4b7{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,876   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4763c727{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 10:55:42,884   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.1.103:4040
2019-07-14 10:55:42,982   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR D:\home\code\bigdata\bigdata-learning\06spark\sparkcore\target\sparkcore-1.0-SNAPSHOT.jar at spark://192.168.1.103:8309/jars/sparkcore-1.0-SNAPSHOT.jar with timestamp 1563072942981
2019-07-14 10:55:43,237   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:55:43,318   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 45 ms (0 ms spent in bootstraps)
2019-07-14 10:56:03,249   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:56:23,267   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 10:56:43,283  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-07-14 10:56:43,283   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-07-14 10:56:43,314   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@6111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 10:56:43,314   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.1.103:4040
2019-07-14 10:56:43,314   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8339.
2019-07-14 10:56:43,314   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.1.103:8339
2019-07-14 10:56:43,330   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 10:56:43,330   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-14 10:56:43,346   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-14 10:56:43,361   WARN --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-07-14 10:56:43,377   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 10:56:43,393   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.1.103, 8339, None)
2019-07-14 10:56:43,393   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 10:56:43,393   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 10:56:43,393   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.103:8339 with 1426.5 MB RAM, BlockManagerId(driver, 192.168.1.103, 8339, None)
2019-07-14 10:56:43,393   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.1.103, 8339, None)
2019-07-14 10:56:43,393   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.1.103, 8339, None)
2019-07-14 10:56:43,408   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 10:56:43,424   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 10:56:43,439   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 10:56:43,783  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem
	at scala.Predef$.require(Predef.scala:277)
	at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:91)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:516)
	at WordCount$.main(WordCount.scala:30)
	at WordCount.main(WordCount.scala)
2019-07-14 10:56:43,783   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-07-14 10:56:43,799   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 10:56:43,799   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-0740ff5c-a94f-4c7b-84f1-8360a8c29eac
2019-07-14 11:01:36,777   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 11:01:39,012   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 11:01:39,340   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 11:01:39,356   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 11:01:39,356   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 11:01:39,356   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 11:01:39,356   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 11:01:45,941   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 8432.
2019-07-14 11:01:46,080   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 11:01:46,180   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 11:01:46,196   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 11:01:46,196   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 11:01:46,243   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-d87f8e9d-87fb-4ef2-b807-49110694b11f
2019-07-14 11:01:46,334   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 11:01:46,410   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 11:01:46,905   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @46904ms
2019-07-14 11:01:47,217   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 11:01:47,277   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @47281ms
2019-07-14 11:01:47,400   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@44d2f794{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 11:01:47,401   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 11:01:47,484   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4393593c{/jobs,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,484   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d000e80{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,484   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7cf283e1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,500   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4d2a1da3{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,500   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@252f626c{/stages,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,500   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@33f98231{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,515   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@48284d0e{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,515   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f4b840d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,531   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@31464a43{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,531   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7f8633ae{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,531   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@68c87fc3{/storage,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,531   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@bc0f53b{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,546   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@8d7b252{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,546   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4682eba5{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,546   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6d9fb2d1{/environment,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,546   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@61fafb74{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,562   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@540a903b{/executors,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,562   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@58496dc{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,562   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@151db587{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,578   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@238acd0b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,605   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@23811a09{/static,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,610   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@36f1046f{/,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,617   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56d93692{/api,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,622   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@26722665{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,631   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7d0614f{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 11:01:47,638   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.1.103:4040
2019-07-14 11:01:48,290   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR D:\home\code\bigdata\bigdata-learning\06spark\sparkcore\target\sparkcore-1.0-SNAPSHOT.jar at spark://192.168.1.103:8432/jars/sparkcore-1.0-SNAPSHOT.jar with timestamp 1563073308280
2019-07-14 11:01:48,788   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:01:48,905   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 58 ms (0 ms spent in bootstraps)
2019-07-14 11:02:08,788   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:02:28,806   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:02:46,620   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 11:02:47,620   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 11:02:47,780   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 11:02:47,784   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 11:02:47,785   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 11:02:47,786   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 11:02:47,787   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 11:02:52,241   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 8465.
2019-07-14 11:02:52,282   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 11:02:52,317   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 11:02:52,321   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 11:02:52,322   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 11:02:52,343   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-4dd52b75-1459-40e5-91a7-6e277d8a8fd9
2019-07-14 11:02:52,385   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 11:02:52,411   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 11:02:52,561   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17455ms
2019-07-14 11:02:52,662   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 11:02:52,683   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17577ms
2019-07-14 11:02:52,716   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@6111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 11:02:52,717   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 11:02:52,756   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ce33a58{/jobs,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,757   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,760   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,763   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@44828f6b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,764   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,765   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,767   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,769   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,770   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,772   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,798   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/storage,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,802   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,806   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,810   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,813   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/environment,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,815   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,817   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/executors,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,819   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38be305c{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,821   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@269f4bad{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,823   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ed731d0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,840   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3234f74e{/static,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,843   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,848   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/api,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,849   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b64c4b7{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,851   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4763c727{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 11:02:52,857   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.1.103:4040
2019-07-14 11:02:52,936   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR D:\home\code\bigdata\bigdata-learning\06spark\sparkcore\target\sparkcore-1.0-SNAPSHOT.jar at spark://192.168.1.103:8465/jars/sparkcore-1.0-SNAPSHOT.jar with timestamp 1563073372935
2019-07-14 11:02:53,214   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:02:53,278   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 41 ms (0 ms spent in bootstraps)
2019-07-14 11:03:13,214   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:03:33,216   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:03:53,218  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-07-14 11:03:53,218   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-07-14 11:03:53,233   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@6111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 11:03:53,237   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.1.103:4040
2019-07-14 11:03:53,248   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8549.
2019-07-14 11:03:53,249   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.1.103:8549
2019-07-14 11:03:53,252   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-14 11:03:53,255   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 11:03:53,261   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-14 11:03:53,271   WARN --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-07-14 11:03:53,290   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 11:03:53,315   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 11:03:53,317   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 11:03:53,326   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.1.103, 8549, None)
2019-07-14 11:03:53,331   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 11:03:53,333   WARN --- [stop-spark-context]  org.apache.spark.metrics.MetricsSystem(line:66) : Stopping a MetricsSystem that is not running
2019-07-14 11:03:53,335   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.103:8549 with 1426.5 MB RAM, BlockManagerId(driver, 192.168.1.103, 8549, None)
2019-07-14 11:03:53,338   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 11:03:53,342   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.1.103, 8549, None)
2019-07-14 11:03:53,344   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.1.103, 8549, None)
2019-07-14 11:03:53,357   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 11:03:53,814   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6ca320ab{/metrics/json,null,AVAILABLE,@Spark}
2019-07-14 11:03:53,827  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.NullPointerException
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:560)
	at WordCount$.main(WordCount.scala:30)
	at WordCount.main(WordCount.scala)
2019-07-14 11:03:53,832   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-07-14 11:03:53,838  ERROR --- [spark-listener-group-appStatus]  org.apache.spark.scheduler.AsyncEventQueue(line:91) : Listener AppStatusListener threw an exception
java.lang.NullPointerException
	at org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:167)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:91)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:76)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:92)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:83)
2019-07-14 11:03:53,839   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 11:03:53,841   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-eaae2a69-8832-42c0-81a9-d69be8c54ec1
2019-07-14 11:07:07,976   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 11:07:08,601   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 11:07:08,694   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 11:07:08,710   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 11:07:08,710   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 11:07:08,710   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 11:07:08,710   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 11:07:12,460   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 8625.
2019-07-14 11:07:12,491   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 11:07:12,522   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 11:07:12,522   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 11:07:12,538   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 11:07:12,554   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-8f336e08-c836-44c2-a9c4-5694e499e435
2019-07-14 11:07:12,601   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 11:07:12,616   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 11:07:12,772   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @18648ms
2019-07-14 11:07:12,866   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 11:07:12,882   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @18755ms
2019-07-14 11:07:12,913   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@6111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 11:07:12,913   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 11:07:12,944   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ce33a58{/jobs,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,944   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,944   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,944   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@44828f6b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,944   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,944   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,944   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,944   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,944   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,944   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,944   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/storage,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,960   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,960   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,960   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,960   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/environment,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,960   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,960   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/executors,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,960   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38be305c{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,960   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@269f4bad{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,960   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ed731d0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,976   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3234f74e{/static,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,976   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,976   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/api,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,976   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b64c4b7{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,976   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4763c727{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 11:07:12,991   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.1.103:4040
2019-07-14 11:07:13,054   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR D:\home\code\bigdata\bigdata-learning\06spark\sparkcore\target\sparkcore-1.0-SNAPSHOT.jar at spark://192.168.1.103:8625/jars/sparkcore-1.0-SNAPSHOT.jar with timestamp 1563073633054
2019-07-14 11:07:13,335   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:07:13,413   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 43 ms (0 ms spent in bootstraps)
2019-07-14 11:07:33,350   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:07:53,365   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:08:13,380  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-07-14 11:08:13,380   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-07-14 11:08:13,411   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@6111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 11:08:13,411   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.1.103:4040
2019-07-14 11:08:13,411   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8658.
2019-07-14 11:08:13,411   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.1.103:8658
2019-07-14 11:08:13,411   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 11:08:13,427   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-14 11:08:13,442   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-14 11:08:13,458   WARN --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-07-14 11:08:13,473   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 11:08:13,489   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.1.103, 8658, None)
2019-07-14 11:08:13,505   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.103:8658 with 1426.5 MB RAM, BlockManagerId(driver, 192.168.1.103, 8658, None)
2019-07-14 11:08:13,505   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 11:08:13,505   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 11:08:13,505   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.1.103, 8658, None)
2019-07-14 11:08:13,505   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.1.103, 8658, None)
2019-07-14 11:08:13,520   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 11:08:13,520   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 11:08:13,536   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 11:08:13,895  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem
	at scala.Predef$.require(Predef.scala:277)
	at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:91)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:516)
	at WordCount$.main(WordCount.scala:30)
	at WordCount.main(WordCount.scala)
2019-07-14 11:08:13,911   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-07-14 11:08:13,911   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 11:08:13,911   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-778f74c6-7627-4dee-a009-aab941aa37fa
2019-07-14 11:12:33,513   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 11:12:34,201   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 11:12:34,310   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 11:12:34,326   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 11:12:34,326   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 11:12:34,326   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 11:12:34,326   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 11:12:39,029   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 8785.
2019-07-14 11:12:39,076   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 11:12:39,108   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 11:12:39,108   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 11:12:39,108   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 11:12:39,139   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-2de7041e-97fa-466d-b59a-a338b9a30be5
2019-07-14 11:12:39,186   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 11:12:39,201   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 11:12:39,358   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @18737ms
2019-07-14 11:12:39,451   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 11:12:39,467   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @18849ms
2019-07-14 11:12:39,498   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@6111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 11:12:39,498   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 11:12:39,545   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ce33a58{/jobs,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,545   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,545   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,545   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@44828f6b{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,545   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/storage,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,561   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/environment,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,576   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,576   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/executors,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,576   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@38be305c{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,576   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@269f4bad{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,576   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ed731d0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3234f74e{/static,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/api,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5b64c4b7{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,592   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4763c727{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 11:12:39,608   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.1.103:4040
2019-07-14 11:12:39,686   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR D:\home\code\bigdata\bigdata-learning\06spark\sparkcore\target\sparkcore-1.0-SNAPSHOT.jar at spark://192.168.1.103:8785/jars/sparkcore-1.0-SNAPSHOT.jar with timestamp 1563073959686
2019-07-14 11:12:39,967   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:12:40,061   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 44 ms (0 ms spent in bootstraps)
2019-07-14 11:12:59,993   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:13:20,008   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:13:40,023  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-07-14 11:13:40,023   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-07-14 11:13:40,054   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@6111ba37{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 11:13:40,054   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.1.103:4040
2019-07-14 11:13:40,070   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8810.
2019-07-14 11:13:40,070   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.1.103:8810
2019-07-14 11:13:40,070   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 11:13:40,070   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-14 11:13:40,085   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-14 11:13:40,085   WARN --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-07-14 11:13:40,101   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 11:13:40,132   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 11:13:40,132   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.1.103, 8810, None)
2019-07-14 11:13:40,132   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 11:13:40,148   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 11:13:40,148   WARN --- [stop-spark-context]  org.apache.spark.metrics.MetricsSystem(line:66) : Stopping a MetricsSystem that is not running
2019-07-14 11:13:40,148   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.103:8810 with 1426.5 MB RAM, BlockManagerId(driver, 192.168.1.103, 8810, None)
2019-07-14 11:13:40,148   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 11:13:40,163   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.1.103, 8810, None)
2019-07-14 11:13:40,163   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.1.103, 8810, None)
2019-07-14 11:13:40,163   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 11:13:40,538   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e53135d{/metrics/json,null,AVAILABLE,@Spark}
2019-07-14 11:13:40,554  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.NullPointerException
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:560)
	at WordCount$.main(WordCount.scala:30)
	at WordCount.main(WordCount.scala)
2019-07-14 11:13:40,554   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-07-14 11:13:40,570   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 11:13:40,570  ERROR --- [spark-listener-group-appStatus]  org.apache.spark.scheduler.AsyncEventQueue(line:91) : Listener AppStatusListener threw an exception
java.lang.NullPointerException
	at org.apache.spark.status.AppStatusListener.onApplicationEnd(AppStatusListener.scala:167)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:57)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:91)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:76)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:92)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:12)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1302)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:83)
2019-07-14 11:13:40,570   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-82e748ab-f263-4e13-bb32-08b1d3be552a
2019-07-14 11:15:13,873   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 11:15:14,513   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 11:15:14,623   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 11:15:14,623   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 11:15:14,623   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 11:15:14,623   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 11:15:14,623   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 11:15:18,357   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 8840.
2019-07-14 11:15:18,388   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 11:15:18,419   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 11:15:18,435   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 11:15:18,435   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 11:15:18,451   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-40a41208-f8f2-497e-9085-83236e120d17
2019-07-14 11:15:18,482   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 11:15:18,513   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 11:15:18,638   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16459ms
2019-07-14 11:15:18,732   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 11:15:18,747   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16560ms
2019-07-14 11:15:18,779   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 11:15:18,779   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 11:15:18,810   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,810   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,810   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,826   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,841   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,841   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,841   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,841   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,841   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,841   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,841   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,841   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,857   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,857   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,857   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,857   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,857   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 11:15:18,872   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.1.103:4040
2019-07-14 11:15:19,201   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:15:19,263   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 43 ms (0 ms spent in bootstraps)
2019-07-14 11:15:39,216   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:15:59,220   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 11:16:19,228  ERROR --- [appclient-registration-retry-thread]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:70) : Application has been killed. Reason: All masters are unresponsive! Giving up.
2019-07-14 11:16:19,228   WARN --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:66) : Application ID is not initialized yet.
2019-07-14 11:16:19,260   INFO --- [stop-spark-context]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 11:16:19,260   INFO --- [stop-spark-context]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.1.103:4040
2019-07-14 11:16:19,275   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 8867.
2019-07-14 11:16:19,275   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.1.103:8867
2019-07-14 11:16:19,275   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 11:16:19,275   INFO --- [stop-spark-context]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-14 11:16:19,291   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-14 11:16:19,291   WARN --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:66) : Drop UnregisterApplication(null) because has not yet connected to master
2019-07-14 11:16:19,306   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 11:16:19,338   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.1.103, 8867, None)
2019-07-14 11:16:19,338   INFO --- [stop-spark-context]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 11:16:19,338   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 11:16:19,338   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.103:8867 with 1426.5 MB RAM, BlockManagerId(driver, 192.168.1.103, 8867, None)
2019-07-14 11:16:19,338   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.1.103, 8867, None)
2019-07-14 11:16:19,338   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.1.103, 8867, None)
2019-07-14 11:16:19,353   INFO --- [stop-spark-context]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 11:16:19,369   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 11:16:19,385   INFO --- [stop-spark-context]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 11:16:19,728  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem
	at scala.Predef$.require(Predef.scala:277)
	at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:91)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:516)
	at WordCount$.main(WordCount.scala:30)
	at WordCount.main(WordCount.scala)
2019-07-14 11:16:19,728   INFO --- [main]  org.apache.spark.SparkContext(line:54) : SparkContext already stopped.
2019-07-14 11:16:19,744   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 11:16:19,744   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-f651e076-327e-4067-97a2-8a49edd9537f
2019-07-14 12:57:20,270   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 12:57:21,051   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 12:57:21,176   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 12:57:21,191   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 12:57:21,191   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 12:57:21,191   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 12:57:21,191   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 12:57:25,332   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 9428.
2019-07-14 12:57:25,363   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 12:57:25,410   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 12:57:25,410   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 12:57:25,410   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 12:57:25,426   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-61bcd938-cc9c-4852-9b87-b76de4984e5a
2019-07-14 12:57:25,472   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 12:57:25,504   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 12:57:25,644   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17727ms
2019-07-14 12:57:25,738   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 12:57:25,754   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17837ms
2019-07-14 12:57:25,785   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@3f8cc50e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 12:57:25,785   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 12:57:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,832   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,847   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,863   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,863   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,863   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,879   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,879   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 12:57:25,879   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-14 12:57:26,238   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-14 12:57:26,332   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9449.
2019-07-14 12:57:26,332   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:9449
2019-07-14 12:57:26,332   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 12:57:26,394   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 9449, None)
2019-07-14 12:57:26,410   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:9449 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 9449, None)
2019-07-14 12:57:26,410   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 9449, None)
2019-07-14 12:57:26,410   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 9449, None)
2019-07-14 12:57:26,722   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-14 12:57:27,926   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-14 12:57:28,097   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-14 12:57:28,113   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:9449 (size: 20.4 KB, free: 1426.5 MB)
2019-07-14 12:57:28,113   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at WordCount.scala:37
2019-07-14 12:57:28,660   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at WordCount.scala:47
2019-07-14 12:57:32,582   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-14 12:57:33,472   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at WordCount.scala:39)
2019-07-14 12:57:33,488   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at WordCount.scala:41)
2019-07-14 12:57:33,488   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at WordCount.scala:47) with 1 output partitions
2019-07-14 12:57:33,488   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at WordCount.scala:47)
2019-07-14 12:57:33,488   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-14 12:57:33,488   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-14 12:57:33,503   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:39), which has no missing parents
2019-07-14 12:57:33,628   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.6 KB, free 1426.3 MB)
2019-07-14 12:57:33,628   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-14 12:57:33,644   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:9449 (size: 3.3 KB, free: 1426.5 MB)
2019-07-14 12:57:33,644   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-14 12:57:33,691   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-14 12:57:33,722   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-14 12:57:33,832   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7360 bytes)
2019-07-14 12:57:33,832   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7360 bytes)
2019-07-14 12:57:33,847   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-14 12:57:33,847   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-14 12:57:35,316   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:21+21
2019-07-14 12:57:35,316   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:0+21
2019-07-14 12:57:37,472   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
2019-07-14 12:57:37,472   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1152 bytes result sent to driver
2019-07-14 12:57:37,488   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 3656 ms on localhost (executor driver) (1/2)
2019-07-14 12:57:37,488   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 3688 ms on localhost (executor driver) (2/2)
2019-07-14 12:57:37,503   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-14 12:57:37,503   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at WordCount.scala:39) finished in 3.921 s
2019-07-14 12:57:37,503   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-14 12:57:37,503   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-14 12:57:37,503   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-14 12:57:37,503   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-14 12:57:37,519   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:41), which has no missing parents
2019-07-14 12:57:37,535   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.1 KB, free 1426.3 MB)
2019-07-14 12:57:37,550   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 1426.3 MB)
2019-07-14 12:57:37,550   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:9449 (size: 3.0 KB, free: 1426.5 MB)
2019-07-14 12:57:37,550   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-14 12:57:37,550   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:41) (first 15 tasks are for partitions Vector(0))
2019-07-14 12:57:37,550   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-14 12:57:37,566   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-14 12:57:37,566   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-14 12:57:37,628   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-14 12:57:37,628   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-14 12:57:37,831   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-14 12:57:37,831   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 281 ms on localhost (executor driver) (1/1)
2019-07-14 12:57:37,831   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-14 12:57:37,831   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at WordCount.scala:41) finished in 0.296 s
2019-07-14 12:57:37,831   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-14 12:57:37,831   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-14 12:57:37,831   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-14 12:57:37,831   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-14 12:57:37,831   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:41), which has no missing parents
2019-07-14 12:57:37,847   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-14 12:57:37,847   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-14 12:57:37,847   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:9449 (size: 2.5 KB, free: 1426.5 MB)
2019-07-14 12:57:37,847   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-14 12:57:37,847   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:41) (first 15 tasks are for partitions Vector(0))
2019-07-14 12:57:37,847   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-14 12:57:37,863   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-14 12:57:37,863   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-14 12:57:37,863   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-14 12:57:37,863   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-14 12:57:37,910   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1301 bytes result sent to driver
2019-07-14 12:57:37,910   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 47 ms on localhost (executor driver) (1/1)
2019-07-14 12:57:37,910   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-14 12:57:37,910   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at WordCount.scala:47) finished in 0.079 s
2019-07-14 12:57:37,925   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at WordCount.scala:47, took 9.262325 s
2019-07-14 12:57:37,941   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@3f8cc50e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 12:57:37,941   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-14 12:57:37,956   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 12:57:38,066   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 12:57:38,066   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 12:57:38,066   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 12:57:38,066   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 12:57:38,081   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 12:57:38,081   INFO --- [main]  WordCount$(line:56) : complete!
2019-07-14 12:57:38,081   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 12:57:38,081   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-0c0239b5-6fbf-485e-bc3d-bc8c531d94b7
2019-07-14 13:17:23,703   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 13:17:24,375   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 13:17:24,485   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 13:17:24,485   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 13:17:24,485   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 13:17:24,485   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 13:17:24,500   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 13:17:28,500   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 9717.
2019-07-14 13:17:28,531   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 13:17:28,563   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 13:17:28,578   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 13:17:28,578   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 13:17:28,594   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-a26ce954-5db5-4ac8-9ed9-f94cb2595168
2019-07-14 13:17:28,641   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 13:17:28,656   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 13:17:28,797   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16721ms
2019-07-14 13:17:28,875   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 13:17:28,906   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16829ms
2019-07-14 13:17:28,922   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 13:17:28,922   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3c435123{/jobs,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4baf352a{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2453f95d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,969   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,984   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,984   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,984   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,984   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,984   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,984   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 13:17:28,984   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 13:17:29,000   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@339bf286{/static,null,AVAILABLE,@Spark}
2019-07-14 13:17:29,000   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/,null,AVAILABLE,@Spark}
2019-07-14 13:17:29,000   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f9a01c1{/api,null,AVAILABLE,@Spark}
2019-07-14 13:17:29,000   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 13:17:29,000   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f446bef{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 13:17:29,000   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-14 13:17:29,281   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-14 13:17:29,391   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9738.
2019-07-14 13:17:29,391   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:9738
2019-07-14 13:17:29,391   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 13:17:29,422   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 9738, None)
2019-07-14 13:17:29,438   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:9738 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 9738, None)
2019-07-14 13:17:29,438   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 9738, None)
2019-07-14 13:17:29,438   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 9738, None)
2019-07-14 13:17:29,781   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1556f2dd{/metrics/json,null,AVAILABLE,@Spark}
2019-07-14 13:17:31,000   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-14 13:17:31,203   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-14 13:17:31,203   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:9738 (size: 20.4 KB, free: 1426.5 MB)
2019-07-14 13:17:31,219   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at WordCount.scala:37
2019-07-14 13:17:31,578   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at WordCount.scala:47
2019-07-14 13:17:32,469   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-14 13:17:32,844   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at WordCount.scala:39)
2019-07-14 13:17:32,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at WordCount.scala:41)
2019-07-14 13:17:32,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at WordCount.scala:47) with 1 output partitions
2019-07-14 13:17:32,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at WordCount.scala:47)
2019-07-14 13:17:32,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-14 13:17:32,875   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-14 13:17:32,891   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:39), which has no missing parents
2019-07-14 13:17:33,000   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.6 KB, free 1426.3 MB)
2019-07-14 13:17:33,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-14 13:17:33,016   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:9738 (size: 3.3 KB, free: 1426.5 MB)
2019-07-14 13:17:33,016   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-14 13:17:33,047   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-14 13:17:33,047   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-14 13:17:33,125   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7360 bytes)
2019-07-14 13:17:33,125   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7360 bytes)
2019-07-14 13:17:33,141   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-14 13:17:33,141   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-14 13:17:34,125   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:21+21
2019-07-14 13:17:34,125   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:0+21
2019-07-14 13:17:35,547   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1152 bytes result sent to driver
2019-07-14 13:17:35,547   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1152 bytes result sent to driver
2019-07-14 13:17:35,562   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2453 ms on localhost (executor driver) (1/2)
2019-07-14 13:17:35,578   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2453 ms on localhost (executor driver) (2/2)
2019-07-14 13:17:35,578   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-14 13:17:35,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at WordCount.scala:39) finished in 2.625 s
2019-07-14 13:17:35,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-14 13:17:35,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-14 13:17:35,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-14 13:17:35,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-14 13:17:35,609   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:41), which has no missing parents
2019-07-14 13:17:35,625   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.1 KB, free 1426.3 MB)
2019-07-14 13:17:35,641   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 1426.3 MB)
2019-07-14 13:17:35,641   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:9738 (size: 3.0 KB, free: 1426.5 MB)
2019-07-14 13:17:35,641   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-14 13:17:35,656   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:41) (first 15 tasks are for partitions Vector(0))
2019-07-14 13:17:35,656   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-14 13:17:35,656   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-14 13:17:35,656   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-14 13:17:35,703   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-14 13:17:35,703   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-14 13:17:35,844   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-14 13:17:35,844   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 188 ms on localhost (executor driver) (1/1)
2019-07-14 13:17:35,844   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-14 13:17:35,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at WordCount.scala:41) finished in 0.234 s
2019-07-14 13:17:35,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-14 13:17:35,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-14 13:17:35,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-14 13:17:35,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-14 13:17:35,859   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:41), which has no missing parents
2019-07-14 13:17:35,890   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-14 13:17:35,890   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-14 13:17:35,890   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:9738 (size: 2.5 KB, free: 1426.5 MB)
2019-07-14 13:17:35,906   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-14 13:17:35,906   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:41) (first 15 tasks are for partitions Vector(0))
2019-07-14 13:17:35,906   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-14 13:17:35,906   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-14 13:17:35,906   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-14 13:17:35,922   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-14 13:17:35,937   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 15 ms
2019-07-14 13:17:35,969   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1387 bytes result sent to driver
2019-07-14 13:17:35,969   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 63 ms on localhost (executor driver) (1/1)
2019-07-14 13:17:35,969   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-14 13:17:35,969   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at WordCount.scala:47) finished in 0.094 s
2019-07-14 13:17:36,000   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at WordCount.scala:47, took 4.411284 s
2019-07-14 13:17:36,015   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2ece4966{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 13:17:36,015   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-14 13:17:36,047   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 13:17:36,203   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 13:17:36,203   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 13:17:36,219   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 13:17:36,219   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 13:17:36,234   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 13:17:36,234   INFO --- [main]  WordCount$(line:56) : complete!
2019-07-14 13:17:36,250   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 13:17:36,250   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-20d6135b-95fc-496f-b7a4-c048cc295aee
2019-07-14 13:19:29,190   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 13:19:29,830   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 13:19:29,939   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 13:19:29,939   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 13:19:29,955   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 13:19:29,955   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 13:19:29,955   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 13:19:33,752   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 9782.
2019-07-14 13:19:33,799   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 13:19:33,830   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 13:19:33,830   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 13:19:33,830   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 13:19:33,846   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-03e1aeeb-50da-47c5-9f9e-e582a628d6ad
2019-07-14 13:19:33,892   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 13:19:33,908   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 13:19:34,080   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16346ms
2019-07-14 13:19:34,158   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 13:19:34,174   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16448ms
2019-07-14 13:19:34,221   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@6abe17f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 13:19:34,221   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 13:19:34,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@150ab4ed{/jobs,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,252   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f363cf5{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3829ac1{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1bb1fde8{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@15eebbff{/stages,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@22d6f11{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@30990c1b{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2dbe250d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@553f1d75{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e1d8f9e{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e34ace1{/storage,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,267   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62fe6067{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,283   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4f071df8{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,283   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4de41af9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,283   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@56ace400{/environment,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,283   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@47404bea{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,283   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@305f7627{/executors,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,283   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d018107{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,283   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6cbcf243{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,283   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29e6eb25{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,299   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@62435e70{/static,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,299   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@21aa6d6c{/,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,314   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@b968a76{/api,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,314   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5894c8{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,314   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1433046b{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 13:19:34,314   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-14 13:19:34,596   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-14 13:19:34,705   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9803.
2019-07-14 13:19:34,705   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:9803
2019-07-14 13:19:34,705   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 13:19:34,736   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 9803, None)
2019-07-14 13:19:34,736   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:9803 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 9803, None)
2019-07-14 13:19:34,752   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 9803, None)
2019-07-14 13:19:34,752   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 9803, None)
2019-07-14 13:19:35,033   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@54336c81{/metrics/json,null,AVAILABLE,@Spark}
2019-07-14 13:19:35,971   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-14 13:19:36,158   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-14 13:19:36,174   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:9803 (size: 20.4 KB, free: 1426.5 MB)
2019-07-14 13:19:36,174   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at WordCount.scala:37
2019-07-14 13:19:36,549   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at WordCount.scala:47
2019-07-14 13:19:37,486   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-14 13:19:37,924   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at WordCount.scala:39)
2019-07-14 13:19:37,924   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at WordCount.scala:41)
2019-07-14 13:19:37,939   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at WordCount.scala:47) with 1 output partitions
2019-07-14 13:19:37,939   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at WordCount.scala:47)
2019-07-14 13:19:37,939   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-14 13:19:37,955   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-14 13:19:37,986   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:39), which has no missing parents
2019-07-14 13:19:38,111   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 5.6 KB, free 1426.3 MB)
2019-07-14 13:19:38,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1426.3 MB)
2019-07-14 13:19:38,127   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:9803 (size: 3.3 KB, free: 1426.5 MB)
2019-07-14 13:19:38,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-14 13:19:38,158   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:39) (first 15 tasks are for partitions Vector(0, 1))
2019-07-14 13:19:38,158   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-14 13:19:38,252   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7360 bytes)
2019-07-14 13:19:38,267   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7360 bytes)
2019-07-14 13:19:38,299   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-14 13:19:38,299   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-14 13:19:39,299   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:21+21
2019-07-14 13:19:39,299   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:0+21
2019-07-14 13:19:40,861   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1109 bytes result sent to driver
2019-07-14 13:19:40,877   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1109 bytes result sent to driver
2019-07-14 13:19:40,877   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 2610 ms on localhost (executor driver) (1/2)
2019-07-14 13:19:40,892   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 2656 ms on localhost (executor driver) (2/2)
2019-07-14 13:19:40,892   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-14 13:19:40,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at WordCount.scala:39) finished in 2.843 s
2019-07-14 13:19:40,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-14 13:19:40,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-14 13:19:40,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-14 13:19:40,908   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-14 13:19:40,923   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:41), which has no missing parents
2019-07-14 13:19:40,939   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 5.1 KB, free 1426.3 MB)
2019-07-14 13:19:40,939   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.0 KB, free 1426.3 MB)
2019-07-14 13:19:40,939   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:9803 (size: 3.0 KB, free: 1426.5 MB)
2019-07-14 13:19:40,939   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-14 13:19:40,955   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:41) (first 15 tasks are for partitions Vector(0))
2019-07-14 13:19:40,955   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-14 13:19:40,955   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7130 bytes)
2019-07-14 13:19:40,955   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-14 13:19:41,002   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-14 13:19:41,002   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-14 13:19:41,095   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1281 bytes result sent to driver
2019-07-14 13:19:41,095   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 140 ms on localhost (executor driver) (1/1)
2019-07-14 13:19:41,095   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-14 13:19:41,095   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at WordCount.scala:41) finished in 0.172 s
2019-07-14 13:19:41,095   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-14 13:19:41,095   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-14 13:19:41,095   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-14 13:19:41,095   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-14 13:19:41,095   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:41), which has no missing parents
2019-07-14 13:19:41,111   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 4.5 KB, free 1426.2 MB)
2019-07-14 13:19:41,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-14 13:19:41,127   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:9803 (size: 2.5 KB, free: 1426.5 MB)
2019-07-14 13:19:41,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-14 13:19:41,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:41) (first 15 tasks are for partitions Vector(0))
2019-07-14 13:19:41,127   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-14 13:19:41,127   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7141 bytes)
2019-07-14 13:19:41,127   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-14 13:19:41,142   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-14 13:19:41,142   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-14 13:19:41,173   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1387 bytes result sent to driver
2019-07-14 13:19:41,173   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 46 ms on localhost (executor driver) (1/1)
2019-07-14 13:19:41,173   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-14 13:19:41,173   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at WordCount.scala:47) finished in 0.078 s
2019-07-14 13:19:41,189   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at WordCount.scala:47, took 4.645434 s
2019-07-14 13:19:41,205   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@6abe17f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 13:19:41,205   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-14 13:19:41,236   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 13:19:41,392   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 13:19:41,392   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 13:19:41,408   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 13:19:41,408   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 13:19:41,423   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 13:19:41,423   INFO --- [main]  WordCount$(line:56) : complete!
2019-07-14 13:19:41,439   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 13:19:41,439   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-fbff2a5f-6fa9-4470-8ee8-436a8ffc91d6
2019-07-14 14:50:33,043   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 14:50:33,667   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: WordCount
2019-07-14 14:50:33,792   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 14:50:33,792   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 14:50:33,792   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 14:50:33,792   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 14:50:33,792   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 14:50:37,558   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 10622.
2019-07-14 14:50:37,589   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 14:50:37,652   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 14:50:37,652   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 14:50:37,652   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 14:50:37,667   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-98803e08-cb6f-4079-b415-ce16a522daf0
2019-07-14 14:50:37,699   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 14:50:37,730   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 14:50:37,870   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @17193ms
2019-07-14 14:50:37,964   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 14:50:37,995   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @17331ms
2019-07-14 14:50:38,027   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2b1ce92a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 14:50:38,042   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 14:50:38,089   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@611df6e3{/jobs,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,089   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@70dd7e15{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,105   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4a9f80d3{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,105   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@41fe9859{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,105   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ac86ba5{/stages,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6c67e137{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2c9399a4{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53ab0286{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@63c5efee{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d10e0b1{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c98290c{/storage,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,120   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@172ca72b{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,136   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5bda80bf{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,136   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@71e5f61d{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,136   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2ce86164{/environment,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,136   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5e8f9e2d{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,136   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@51df223b{/executors,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,136   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fd46303{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,136   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@60d8c0dc{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,136   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4204541c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,152   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6a62689d{/static,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,152   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2b58f754{/,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,152   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3ebff828{/api,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,152   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e044120{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,167   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2cf23c81{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 14:50:38,167   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-14 14:50:38,449   INFO --- [main]  org.apache.spark.executor.Executor(line:54) : Starting executor ID driver on host localhost
2019-07-14 14:50:38,605   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10643.
2019-07-14 14:50:38,605   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:10643
2019-07-14 14:50:38,605   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 14:50:38,652   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 10643, None)
2019-07-14 14:50:38,667   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:10643 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 10643, None)
2019-07-14 14:50:38,683   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 10643, None)
2019-07-14 14:50:38,683   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 10643, None)
2019-07-14 14:50:39,058   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1358b28e{/metrics/json,null,AVAILABLE,@Spark}
2019-07-14 14:50:39,886   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-14 14:50:40,011   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-14 14:50:40,027   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:10643 (size: 20.4 KB, free: 1426.5 MB)
2019-07-14 14:50:40,027   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at WordCount.scala:39
2019-07-14 14:50:40,308   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at WordCount.scala:49
2019-07-14 14:50:41,245   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-14 14:50:41,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at WordCount.scala:41)
2019-07-14 14:50:41,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at WordCount.scala:43)
2019-07-14 14:50:41,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at WordCount.scala:49) with 1 output partitions
2019-07-14 14:50:41,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at WordCount.scala:49)
2019-07-14 14:50:41,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-14 14:50:41,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-14 14:50:41,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:41), which has no missing parents
2019-07-14 14:50:41,745   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-14 14:50:41,745   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1426.3 MB)
2019-07-14 14:50:41,761   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on fc-pc:10643 (size: 2.9 KB, free: 1426.5 MB)
2019-07-14 14:50:41,761   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-14 14:50:41,777   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:41) (first 15 tasks are for partitions Vector(0, 1))
2019-07-14 14:50:41,777   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-14 14:50:41,839   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7881 bytes)
2019-07-14 14:50:41,839   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 7881 bytes)
2019-07-14 14:50:41,855   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 0.0 (TID 0)
2019-07-14 14:50:41,855   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Running task 1.0 in stage 0.0 (TID 1)
2019-07-14 14:50:42,011   INFO --- [Executor task launch worker for task 0]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:0+21
2019-07-14 14:50:42,027   INFO --- [Executor task launch worker for task 1]  org.apache.spark.rdd.HadoopRDD(line:54) : Input split: hdfs://Node02:9000/test/test.txt:21+21
2019-07-14 14:50:43,386   INFO --- [Executor task launch worker for task 1]  org.apache.spark.executor.Executor(line:54) : Finished task 1.0 in stage 0.0 (TID 1). 1146 bytes result sent to driver
2019-07-14 14:50:43,386   INFO --- [Executor task launch worker for task 0]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 0.0 (TID 0). 1146 bytes result sent to driver
2019-07-14 14:50:43,417   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1579 ms on localhost (executor driver) (1/2)
2019-07-14 14:50:43,417   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1578 ms on localhost (executor driver) (2/2)
2019-07-14 14:50:43,417   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-14 14:50:43,433   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at WordCount.scala:41) finished in 1.719 s
2019-07-14 14:50:43,433   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-14 14:50:43,433   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-14 14:50:43,448   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-14 14:50:43,448   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-14 14:50:43,448   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:43), which has no missing parents
2019-07-14 14:50:43,464   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 1426.3 MB)
2019-07-14 14:50:43,464   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-14 14:50:43,464   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on fc-pc:10643 (size: 2.5 KB, free: 1426.5 MB)
2019-07-14 14:50:43,464   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-14 14:50:43,480   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-14 14:50:43,480   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-14 14:50:43,480   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 7651 bytes)
2019-07-14 14:50:43,480   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 1.0 (TID 2)
2019-07-14 14:50:43,511   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks
2019-07-14 14:50:43,511   INFO --- [Executor task launch worker for task 2]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-14 14:50:43,605   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on fc-pc:10643 in memory (size: 2.9 KB, free: 1426.5 MB)
2019-07-14 14:50:43,667   INFO --- [Executor task launch worker for task 2]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 1.0 (TID 2). 1361 bytes result sent to driver
2019-07-14 14:50:43,667   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 187 ms on localhost (executor driver) (1/1)
2019-07-14 14:50:43,667   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-14 14:50:43,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at WordCount.scala:43) finished in 0.219 s
2019-07-14 14:50:43,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-14 14:50:43,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-14 14:50:43,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-14 14:50:43,667   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-14 14:50:43,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:43), which has no missing parents
2019-07-14 14:50:43,683   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 1426.3 MB)
2019-07-14 14:50:43,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1426.3 MB)
2019-07-14 14:50:43,698   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on fc-pc:10643 (size: 2.1 KB, free: 1426.5 MB)
2019-07-14 14:50:43,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-14 14:50:43,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:43) (first 15 tasks are for partitions Vector(0))
2019-07-14 14:50:43,698   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-14 14:50:43,698   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 7662 bytes)
2019-07-14 14:50:43,698   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Running task 0.0 in stage 2.0 (TID 3)
2019-07-14 14:50:43,714   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
2019-07-14 14:50:43,714   INFO --- [Executor task launch worker for task 3]  org.apache.spark.storage.ShuffleBlockFetcherIterator(line:54) : Started 0 remote fetches in 0 ms
2019-07-14 14:50:43,745   INFO --- [Executor task launch worker for task 3]  org.apache.spark.executor.Executor(line:54) : Finished task 0.0 in stage 2.0 (TID 3). 1381 bytes result sent to driver
2019-07-14 14:50:43,745   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 47 ms on localhost (executor driver) (1/1)
2019-07-14 14:50:43,745   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-14 14:50:43,745   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at WordCount.scala:49) finished in 0.062 s
2019-07-14 14:50:43,745   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at WordCount.scala:49, took 3.440955 s
2019-07-14 14:50:43,761   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2b1ce92a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 14:50:43,777   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://fc-pc:4040
2019-07-14 14:50:43,792   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 14:50:43,933   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 14:50:43,933   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 14:50:43,933   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 14:50:43,933   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 14:50:43,948   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 14:50:43,948   INFO --- [main]  WordCount$(line:58) : complete!
2019-07-14 14:50:43,964   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 14:50:43,964   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-99ec7062-bdfe-4f3f-834a-e947dd6db294
2019-07-14 15:58:41,229   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-14 15:58:41,854   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: sparkcore
2019-07-14 15:58:41,979   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-14 15:58:41,979   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-14 15:58:41,979   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-14 15:58:41,979   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-14 15:58:41,979   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-14 15:58:45,921   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 11270.
2019-07-14 15:58:45,984   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-14 15:58:46,015   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-14 15:58:46,015   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-14 15:58:46,015   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-14 15:58:46,031   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-e7cd1503-b850-43b5-9dfa-187d8d1be94b
2019-07-14 15:58:46,077   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-14 15:58:46,118   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-14 15:58:46,287   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @15783ms
2019-07-14 15:58:46,413   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-14 15:58:46,451   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @15950ms
2019-07-14 15:58:46,488   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@1c9cf84c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 15:58:46,488   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-14 15:58:46,550   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@117e0fe5{/jobs,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,550   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@9635fa{/jobs/json,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,550   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/job,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,550   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2d10e0b1{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,566   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c98290c{/stages,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,566   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@172ca72b{/stages/json,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,566   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5bda80bf{/stages/stage,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,566   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5e8f9e2d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,566   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@51df223b{/stages/pool,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,566   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fd46303{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,566   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@60d8c0dc{/storage,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,566   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4204541c{/storage/json,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,566   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6a62689d{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,582   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4602c2a9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,582   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@60fa3495{/environment,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,582   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e2822{/environment/json,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,582   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@79e18e38{/executors,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,582   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29a60c27{/executors/json,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,582   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1849db1a{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,582   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@69c79f09{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,597   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1ca25c47{/static,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,597   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@35fe2125{/,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,597   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@94f6bfb{/api,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,597   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1255b1d1{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,597   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@464649c{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-14 15:58:46,613   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.1.103:4040
2019-07-14 15:58:46,796   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR D:\home\code\bigdata\bigdata-learning\06spark\sparkcore\target\sparkcore-1.0-SNAPSHOT.jar at spark://192.168.1.103:11270/jars/sparkcore-1.0-SNAPSHOT.jar with timestamp 1563091126795
2019-07-14 15:58:47,059   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-14 15:58:47,198   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 71 ms (0 ms spent in bootstraps)
2019-07-14 15:58:47,406   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Connected to Spark cluster with app ID app-20190714155849-0008
2019-07-14 15:58:47,406   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190714155849-0008/0 on worker-20190714024237-192.168.1.113-33209 (192.168.1.113:33209) with 12 core(s)
2019-07-14 15:58:47,422   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190714155849-0008/0 on hostPort 192.168.1.113:33209 with 12 core(s), 1024.0 MB RAM
2019-07-14 15:58:47,422   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190714155849-0008/1 on worker-20190714024236-192.168.1.114-39793 (192.168.1.114:39793) with 8 core(s)
2019-07-14 15:58:47,422   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190714155849-0008/1 on hostPort 192.168.1.114:39793 with 8 core(s), 1024.0 MB RAM
2019-07-14 15:58:47,422   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190714155849-0008/1 is now RUNNING
2019-07-14 15:58:47,422   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190714155849-0008/0 is now RUNNING
2019-07-14 15:58:47,469   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 11292.
2019-07-14 15:58:47,470   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.1.103:11292
2019-07-14 15:58:47,475   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-14 15:58:47,551   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.1.103, 11292, None)
2019-07-14 15:58:47,558   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.103:11292 with 1426.5 MB RAM, BlockManagerId(driver, 192.168.1.103, 11292, None)
2019-07-14 15:58:47,563   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.1.103, 11292, None)
2019-07-14 15:58:47,565   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.1.103, 11292, None)
2019-07-14 15:58:48,048   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@240139e1{/metrics/json,null,AVAILABLE,@Spark}
2019-07-14 15:58:48,074   INFO --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2019-07-14 15:58:48,877   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.114:47234) with ID 1
2019-07-14 15:58:48,944   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.113:54014) with ID 0
2019-07-14 15:58:48,993   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.114:37683 with 366.3 MB RAM, BlockManagerId(1, 192.168.1.114, 37683, None)
2019-07-14 15:58:49,030   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-14 15:58:49,222   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.113:39843 with 366.3 MB RAM, BlockManagerId(0, 192.168.1.113, 39843, None)
2019-07-14 15:58:49,477   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-14 15:58:49,488   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.1.103:11292 (size: 20.4 KB, free: 1426.5 MB)
2019-07-14 15:58:49,499   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at WordCount.scala:69
2019-07-14 15:58:49,971   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at WordCount.scala:79
2019-07-14 15:58:51,082   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-14 15:58:51,714   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at WordCount.scala:71)
2019-07-14 15:58:51,716   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at WordCount.scala:73)
2019-07-14 15:58:51,718   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at WordCount.scala:79) with 1 output partitions
2019-07-14 15:58:51,719   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at WordCount.scala:79)
2019-07-14 15:58:51,720   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-14 15:58:51,723   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-14 15:58:51,732   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:71), which has no missing parents
2019-07-14 15:58:51,787   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-14 15:58:51,794   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1426.3 MB)
2019-07-14 15:58:51,795   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.1.103:11292 (size: 2.9 KB, free: 1426.5 MB)
2019-07-14 15:58:51,796   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-14 15:58:51,817   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:71) (first 15 tasks are for partitions Vector(0, 1))
2019-07-14 15:58:51,818   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-14 15:58:51,862   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.114, executor 1, partition 0, ANY, 7885 bytes)
2019-07-14 15:58:51,865   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, 192.168.1.113, executor 0, partition 1, ANY, 7885 bytes)
2019-07-14 15:58:52,141   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.1.113:39843 (size: 2.9 KB, free: 366.3 MB)
2019-07-14 15:58:52,145   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.1.114:37683 (size: 2.9 KB, free: 366.3 MB)
2019-07-14 15:58:52,250   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.1.113:39843 (size: 20.4 KB, free: 366.3 MB)
2019-07-14 15:58:52,261   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.1.114:37683 (size: 20.4 KB, free: 366.3 MB)
2019-07-14 15:58:53,130   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1262 ms on 192.168.1.113 (executor 0) (1/2)
2019-07-14 15:58:53,134   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1289 ms on 192.168.1.114 (executor 1) (2/2)
2019-07-14 15:58:53,136   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-14 15:58:53,150   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at WordCount.scala:71) finished in 1.384 s
2019-07-14 15:58:53,151   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-14 15:58:53,153   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-14 15:58:53,154   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-14 15:58:53,155   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-14 15:58:53,162   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:73), which has no missing parents
2019-07-14 15:58:53,174   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 1426.3 MB)
2019-07-14 15:58:53,180   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-14 15:58:53,185   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on 192.168.1.103:11292 (size: 2.5 KB, free: 1426.5 MB)
2019-07-14 15:58:53,186   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-14 15:58:53,188   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:73) (first 15 tasks are for partitions Vector(0))
2019-07-14 15:58:53,188   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-14 15:58:53,194   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, 192.168.1.114, executor 1, partition 0, NODE_LOCAL, 7655 bytes)
2019-07-14 15:58:53,246   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on 192.168.1.114:37683 (size: 2.5 KB, free: 366.3 MB)
2019-07-14 15:58:53,275   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : Asked to send map output locations for shuffle 1 to 192.168.1.114:47234
2019-07-14 15:58:53,358   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 168 ms on 192.168.1.114 (executor 1) (1/1)
2019-07-14 15:58:53,359   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-14 15:58:53,361   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at WordCount.scala:73) finished in 0.192 s
2019-07-14 15:58:53,361   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-14 15:58:53,362   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-14 15:58:53,363   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-14 15:58:53,363   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-14 15:58:53,365   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:73), which has no missing parents
2019-07-14 15:58:53,371   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 1426.3 MB)
2019-07-14 15:58:53,377   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1426.3 MB)
2019-07-14 15:58:53,380   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on 192.168.1.103:11292 (size: 2.1 KB, free: 1426.5 MB)
2019-07-14 15:58:53,382   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-14 15:58:53,385   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:73) (first 15 tasks are for partitions Vector(0))
2019-07-14 15:58:53,386   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-14 15:58:53,389   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, 192.168.1.114, executor 1, partition 0, NODE_LOCAL, 7666 bytes)
2019-07-14 15:58:53,417   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on 192.168.1.114:37683 (size: 2.1 KB, free: 366.3 MB)
2019-07-14 15:58:53,430   INFO --- [dispatcher-event-loop-1]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : Asked to send map output locations for shuffle 0 to 192.168.1.114:47234
2019-07-14 15:58:53,457   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 69 ms on 192.168.1.114 (executor 1) (1/1)
2019-07-14 15:58:53,458   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-14 15:58:53,459   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at WordCount.scala:79) finished in 0.090 s
2019-07-14 15:58:53,468   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at WordCount.scala:79, took 3.496069 s
2019-07-14 15:58:53,489   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@1c9cf84c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-14 15:58:53,493   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.1.103:4040
2019-07-14 15:58:53,501   INFO --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-14 15:58:53,502   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-14 15:58:53,522   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-14 15:58:53,596   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-14 15:58:53,599   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-14 15:58:53,611   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-14 15:58:53,619   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-14 15:58:53,640   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-14 15:58:53,641   INFO --- [main]  WordCount$(line:88) : complete!
2019-07-14 15:58:53,647   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-14 15:58:53,648   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-2f906f52-66d0-4902-9a82-86bf12dff7d1
2019-07-14 16:04:30,222   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-14 16:23:19,844   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-16 10:34:13,272   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-16 10:34:16,256   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: sparkcore
2019-07-16 10:34:17,194   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-16 10:34:17,209   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-16 10:34:17,225   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-16 10:34:17,225   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-16 10:34:17,256   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-16 10:34:24,443   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 6265.
2019-07-16 10:34:24,834   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-16 10:34:25,146   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-16 10:34:25,256   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-16 10:34:25,271   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-16 10:34:25,412   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-4e23e1fa-22c8-4006-9d10-18c5c2b97564
2019-07-16 10:34:25,724   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-16 10:34:25,959   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-16 10:34:26,928   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @25008ms
2019-07-16 10:34:27,381   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-16 10:34:27,521   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @25607ms
2019-07-16 10:34:27,646   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@649725e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-16 10:34:27,646   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-16 10:34:27,803   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@18a3962d{/jobs,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,803   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@53ab0286{/jobs/json,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,803   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@63c5efee{/jobs/job,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,803   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1c98290c{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@172ca72b{/stages,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5bda80bf{/stages/json,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@71e5f61d{/stages/stage,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@51df223b{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@fd46303{/stages/pool,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@60d8c0dc{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4204541c{/storage,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6a62689d{/storage/json,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4602c2a9{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@60fa3495{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,818   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3e2822{/environment,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,834   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@79e18e38{/environment/json,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,834   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@29a60c27{/executors,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,834   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1849db1a{/executors/json,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,834   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@69c79f09{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,834   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1ca25c47{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,849   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5fcacc0{/static,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,849   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@94f6bfb{/,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,849   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@34645867{/api,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,849   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@464649c{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,849   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7c22d4f{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-16 10:34:27,881   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.1.103:4040
2019-07-16 10:34:28,099   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR D:\home\code\bigdata\bigdata-learning\06spark\sparkcore\target\sparkcore-1.0-SNAPSHOT.jar at spark://192.168.1.103:6265/jars/sparkcore-1.0-SNAPSHOT.jar with timestamp 1563244468099
2019-07-16 10:34:29,287   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-16 10:34:29,584   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 103 ms (0 ms spent in bootstraps)
2019-07-16 10:34:30,021   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Connected to Spark cluster with app ID app-20190716103430-0017
2019-07-16 10:34:30,068   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190716103430-0017/0 on worker-20190714024237-192.168.1.113-33209 (192.168.1.113:33209) with 12 core(s)
2019-07-16 10:34:30,068   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190716103430-0017/0 on hostPort 192.168.1.113:33209 with 12 core(s), 1024.0 MB RAM
2019-07-16 10:34:30,068   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190716103430-0017/1 on worker-20190714024236-192.168.1.114-39793 (192.168.1.114:39793) with 8 core(s)
2019-07-16 10:34:30,068   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190716103430-0017/1 on hostPort 192.168.1.114:39793 with 8 core(s), 1024.0 MB RAM
2019-07-16 10:34:30,115   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190716103430-0017/1 is now RUNNING
2019-07-16 10:34:30,115   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190716103430-0017/0 is now RUNNING
2019-07-16 10:34:30,131   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6287.
2019-07-16 10:34:30,131   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.1.103:6287
2019-07-16 10:34:30,209   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-16 10:34:30,381   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.1.103, 6287, None)
2019-07-16 10:34:30,396   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.103:6287 with 1426.5 MB RAM, BlockManagerId(driver, 192.168.1.103, 6287, None)
2019-07-16 10:34:30,443   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.1.103, 6287, None)
2019-07-16 10:34:30,443   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.1.103, 6287, None)
2019-07-16 10:34:30,990   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@7ea4d397{/metrics/json,null,AVAILABLE,@Spark}
2019-07-16 10:34:31,224   INFO --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2019-07-16 10:34:31,959   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.113:52684) with ID 0
2019-07-16 10:34:32,021   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.114:41674) with ID 1
2019-07-16 10:34:32,115   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.113:42963 with 366.3 MB RAM, BlockManagerId(0, 192.168.1.113, 42963, None)
2019-07-16 10:34:32,131   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.114:39593 with 366.3 MB RAM, BlockManagerId(1, 192.168.1.114, 39593, None)
2019-07-16 10:34:34,724   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 214.6 KB, free 1426.3 MB)
2019-07-16 10:34:35,130   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 1426.3 MB)
2019-07-16 10:34:35,146   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.1.103:6287 (size: 20.4 KB, free: 1426.5 MB)
2019-07-16 10:34:35,193   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at WordCount.scala:69
2019-07-16 10:34:36,318   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at WordCount.scala:79
2019-07-16 10:34:41,943   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:247) : Total input paths to process : 1
2019-07-16 10:34:43,646   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at WordCount.scala:71)
2019-07-16 10:34:43,661   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at WordCount.scala:73)
2019-07-16 10:34:43,661   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at WordCount.scala:79) with 1 output partitions
2019-07-16 10:34:43,661   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at WordCount.scala:79)
2019-07-16 10:34:43,661   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-16 10:34:43,724   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-16 10:34:43,818   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:71), which has no missing parents
2019-07-16 10:34:44,302   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 1426.3 MB)
2019-07-16 10:34:44,318   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1426.3 MB)
2019-07-16 10:34:44,318   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.1.103:6287 (size: 2.9 KB, free: 1426.5 MB)
2019-07-16 10:34:44,318   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-16 10:34:44,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:71) (first 15 tasks are for partitions Vector(0, 1))
2019-07-16 10:34:44,490   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-16 10:34:44,755   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.113, executor 0, partition 0, ANY, 7885 bytes)
2019-07-16 10:34:44,849   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, 192.168.1.114, executor 1, partition 1, ANY, 7885 bytes)
2019-07-16 10:34:45,568   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.1.114:39593 (size: 2.9 KB, free: 366.3 MB)
2019-07-16 10:34:45,568   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.1.113:42963 (size: 2.9 KB, free: 366.3 MB)
2019-07-16 10:34:45,693   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.1.114:39593 (size: 20.4 KB, free: 366.3 MB)
2019-07-16 10:34:45,693   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.1.113:42963 (size: 20.4 KB, free: 366.3 MB)
2019-07-16 10:34:46,521   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1656 ms on 192.168.1.114 (executor 1) (1/2)
2019-07-16 10:34:46,552   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1891 ms on 192.168.1.113 (executor 0) (2/2)
2019-07-16 10:34:46,614   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-16 10:34:46,661   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at WordCount.scala:71) finished in 2.437 s
2019-07-16 10:34:46,677   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-16 10:34:46,677   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-16 10:34:46,677   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-16 10:34:46,677   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-16 10:34:46,724   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:73), which has no missing parents
2019-07-16 10:34:46,911   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 1426.3 MB)
2019-07-16 10:34:46,927   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.3 MB)
2019-07-16 10:34:46,927   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on 192.168.1.103:6287 (size: 2.5 KB, free: 1426.5 MB)
2019-07-16 10:34:46,927   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-16 10:34:46,943   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:73) (first 15 tasks are for partitions Vector(0))
2019-07-16 10:34:46,943   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-16 10:34:46,958   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, 192.168.1.114, executor 1, partition 0, NODE_LOCAL, 7655 bytes)
2019-07-16 10:34:47,005   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on 192.168.1.114:39593 (size: 2.5 KB, free: 366.3 MB)
2019-07-16 10:34:47,036   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : Asked to send map output locations for shuffle 1 to 192.168.1.114:41674
2019-07-16 10:34:47,130   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 187 ms on 192.168.1.114 (executor 1) (1/1)
2019-07-16 10:34:47,130   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-16 10:34:47,146   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at WordCount.scala:73) finished in 0.250 s
2019-07-16 10:34:47,146   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-16 10:34:47,146   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-16 10:34:47,146   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-16 10:34:47,146   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-16 10:34:47,146   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:73), which has no missing parents
2019-07-16 10:34:47,177   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 1426.3 MB)
2019-07-16 10:34:47,193   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1426.3 MB)
2019-07-16 10:34:47,193   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on 192.168.1.103:6287 (size: 2.1 KB, free: 1426.5 MB)
2019-07-16 10:34:47,193   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-16 10:34:47,193   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:73) (first 15 tasks are for partitions Vector(0))
2019-07-16 10:34:47,193   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-16 10:34:47,193   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, 192.168.1.114, executor 1, partition 0, NODE_LOCAL, 7666 bytes)
2019-07-16 10:34:47,224   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on 192.168.1.114:39593 (size: 2.1 KB, free: 366.3 MB)
2019-07-16 10:34:47,239   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : Asked to send map output locations for shuffle 0 to 192.168.1.114:41674
2019-07-16 10:34:47,442   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 249 ms on 192.168.1.114 (executor 1) (1/1)
2019-07-16 10:34:47,442   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-16 10:34:47,442   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at WordCount.scala:79) finished in 0.281 s
2019-07-16 10:34:47,708   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on 192.168.1.113:42963 in memory (size: 2.9 KB, free: 366.3 MB)
2019-07-16 10:34:47,708   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on 192.168.1.114:39593 in memory (size: 2.9 KB, free: 366.3 MB)
2019-07-16 10:34:47,739   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at WordCount.scala:79, took 11.423167 s
2019-07-16 10:34:47,849   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_1_piece0 on 192.168.1.103:6287 in memory (size: 2.9 KB, free: 1426.5 MB)
2019-07-16 10:34:47,880   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on 192.168.1.114:39593 in memory (size: 2.5 KB, free: 366.3 MB)
2019-07-16 10:34:47,880   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Removed broadcast_2_piece0 on 192.168.1.103:6287 in memory (size: 2.5 KB, free: 1426.5 MB)
2019-07-16 10:34:47,958   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@649725e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-16 10:34:47,958   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.1.103:4040
2019-07-16 10:34:47,974   INFO --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-16 10:34:47,974   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-16 10:34:48,083   INFO --- [dispatcher-event-loop-2]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-16 10:34:48,177   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-16 10:34:48,177   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-16 10:34:48,192   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-16 10:34:48,192   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-16 10:34:48,224   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-16 10:34:48,224   INFO --- [main]  WordCount$(line:88) : complete!
2019-07-16 10:34:48,255   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-16 10:34:48,271   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-7bbb42c8-c22e-4919-b222-45cbad4b5d4e
2019-07-16 16:25:33,492  ERROR --- [main]  org.apache.spark.SparkContext(line:91) : Error initializing SparkContext.
org.apache.spark.SparkException: A master URL must be set in your configuration
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:368)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)
	at DataFrames3$.main(DataFrames3.scala:31)
	at DataFrames3.main(DataFrames3.scala)
2019-07-16 16:25:33,523  ERROR --- [main]  org.apache.spark.util.Utils(line:91) : Uncaught exception in thread main
java.lang.NullPointerException
	at org.apache.spark.SparkContext.org$apache$spark$SparkContext$$postApplicationEnd(SparkContext.scala:2416)
	at org.apache.spark.SparkContext$$anonfun$stop$1.apply$mcV$sp(SparkContext.scala:1931)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1930)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:585)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:926)
	at DataFrames3$.main(DataFrames3.scala:31)
	at DataFrames3.main(DataFrames3.scala)
2019-07-16 16:43:17,597   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-16 16:52:24,424   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-16 16:52:29,989   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-16 16:52:29,989   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-16 16:52:55,420   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:53:10,419   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:53:25,419   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:53:40,419   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:53:55,418   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:54:10,417   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:54:25,412   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:54:40,413   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:54:55,413   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:55:10,412   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:55:25,412   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:55:40,411   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:55:55,411   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:56:10,410   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:56:25,410   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:56:40,409   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:56:55,409   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:57:10,408   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:57:25,408   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:57:40,407   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:57:55,407   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:58:10,406   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:58:25,406   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:58:40,405   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:58:55,420   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:59:10,420   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:59:25,419   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:59:40,419   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 16:59:55,418   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:00:10,418   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:00:25,418   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:00:40,417   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:00:55,417   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:01:10,416   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:01:25,416   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:01:40,415   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:01:55,415   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:02:10,414   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:02:25,414   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:02:40,413   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:02:55,413   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:03:10,412   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:03:25,412   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:03:40,411   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:03:55,416   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:04:10,414   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 17:12:35,062   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-16 17:13:51,099   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-16 18:09:44,071   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-16 21:30:59,809   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-16 21:31:43,072   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-16 21:32:04,090   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-16 21:32:19,087   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-17 16:40:10,779   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-17 16:40:55,143   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-17 19:49:35,021   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 19:50:23,036   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 19:52:33,800   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 20:33:31,705   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 20:35:07,388   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 20:37:33,023   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 20:43:37,253   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 20:44:49,299   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 20:46:50,102   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 20:50:55,104   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 20:55:56,794   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 20:56:43,092   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 20:59:02,525   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 20:59:56,377   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 21:08:05,523   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 21:09:12,675   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-17 21:10:10,849   WARN --- [main]  org.apache.hadoop.security.UserGroupInformation(line:1521) : No groups available for user hadoop
2019-07-17 21:13:38,819   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 21:16:58,353   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 21:26:04,041   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 21:27:06,267   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 21:29:26,436   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 21:31:51,382   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-17 21:33:31,480   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-18 09:20:00,432   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-18 09:21:51,778   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-18 09:23:30,541   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-18 09:26:44,239   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-18 09:28:09,691   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-18 09:31:56,095   WARN --- [main]  org.apache.spark.util.Utils(line:66) : Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2019-07-18 09:31:57,067   WARN --- [main]  org.apache.spark.SparkContext(line:66) : Using an existing SparkContext; some configuration may not take effect.
2019-07-19 10:45:23,162   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-19 10:45:23,834   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: sparkcore
2019-07-19 10:45:23,928   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-19 10:45:23,943   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-19 10:45:23,943   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-19 10:45:23,943   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-19 10:45:23,943   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-19 10:45:27,677   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 6469.
2019-07-19 10:45:27,740   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-19 10:45:27,771   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-19 10:45:27,771   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-19 10:45:27,771   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-19 10:45:27,787   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-f93eb225-bade-497b-bb69-63ddf74072cf
2019-07-19 10:45:27,834   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-19 10:45:27,849   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-19 10:45:27,990   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @15850ms
2019-07-19 10:45:28,084   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-19 10:45:28,131   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @15979ms
2019-07-19 10:45:28,162   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@2878a2d3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-19 10:45:28,162   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-19 10:45:28,209   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1255b1d1{/jobs,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,209   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f9879ac{/jobs/json,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,209   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@37f21974{/jobs/job,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,209   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e521c1e{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,209   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@224b4d61{/stages,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,209   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d5d9e5{/stages/json,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,209   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@303e3593{/stages/stage,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,224   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d9bec4d{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,224   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5c48c0c0{/stages/pool,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,224   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@10c8f62{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,224   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@674c583e{/storage,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,224   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@25f7391e{/storage/json,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,224   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3f23a3a0{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,224   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5ab14cb9{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,240   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5fb97279{/environment,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,240   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@439a8f59{/environment/json,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,240   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@61861a29{/executors,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,240   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@31024624{/executors/json,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,240   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@25bcd0c7{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,240   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@32cb636e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,256   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@63cd604c{/static,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,256   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2f162cc0{/,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,256   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5df417a7{/api,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,256   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1cb3ec38{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,271   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@403132fc{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-19 10:45:28,271   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://192.168.1.103:4040
2019-07-19 10:45:28,302   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Added JAR D:\home\code\bigdata\bigdata-learning\06spark\sparkcore\target\sparkcore-1.0-SNAPSHOT.jar at spark://192.168.1.103:6469/jars/sparkcore-1.0-SNAPSHOT.jar with timestamp 1563504328302
2019-07-19 10:45:28,474   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-19 10:45:28,568   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 57 ms (0 ms spent in bootstraps)
2019-07-19 10:45:28,709   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Connected to Spark cluster with app ID app-20190719104529-0004
2019-07-19 10:45:28,724   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719104529-0004/0 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 10:45:28,740   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719104529-0004/0 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 10:45:28,740   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719104529-0004/1 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 10:45:28,740   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719104529-0004/1 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 10:45:28,740   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719104529-0004/2 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 10:45:28,740   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719104529-0004/2 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 10:45:28,756   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719104529-0004/0 is now RUNNING
2019-07-19 10:45:28,756   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719104529-0004/1 is now RUNNING
2019-07-19 10:45:28,756   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719104529-0004/2 is now RUNNING
2019-07-19 10:45:28,771   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6491.
2019-07-19 10:45:28,771   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on 192.168.1.103:6491
2019-07-19 10:45:28,771   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-19 10:45:28,818   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, 192.168.1.103, 6491, None)
2019-07-19 10:45:28,818   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.103:6491 with 1426.5 MB RAM, BlockManagerId(driver, 192.168.1.103, 6491, None)
2019-07-19 10:45:28,818   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, 192.168.1.103, 6491, None)
2019-07-19 10:45:28,818   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, 192.168.1.103, 6491, None)
2019-07-19 10:45:29,162   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@43d455c9{/metrics/json,null,AVAILABLE,@Spark}
2019-07-19 10:45:29,209   INFO --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2019-07-19 10:45:30,130   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 230.0 KB, free 1426.3 MB)
2019-07-19 10:45:30,224   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 22.1 KB, free 1426.3 MB)
2019-07-19 10:45:30,240   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.1.103:6491 (size: 22.1 KB, free: 1426.5 MB)
2019-07-19 10:45:30,240   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from textFile at WordCount.scala:69
2019-07-19 10:45:30,318   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.114:49586) with ID 2
2019-07-19 10:45:30,334   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.112:45140) with ID 1
2019-07-19 10:45:30,334   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.1.113:38690) with ID 0
2019-07-19 10:45:30,412   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.114:37669 with 366.3 MB RAM, BlockManagerId(2, 192.168.1.114, 37669, None)
2019-07-19 10:45:30,427   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.113:33135 with 366.3 MB RAM, BlockManagerId(0, 192.168.1.113, 33135, None)
2019-07-19 10:45:30,427   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager 192.168.1.112:38815 with 366.3 MB RAM, BlockManagerId(1, 192.168.1.112, 38815, None)
2019-07-19 10:45:30,490   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at WordCount.scala:79
2019-07-19 10:45:31,396   INFO --- [dag-scheduler-event-loop]  org.apache.hadoop.mapred.FileInputFormat(line:249) : Total input paths to process : 1
2019-07-19 10:45:31,755   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 3 (map at WordCount.scala:71)
2019-07-19 10:45:31,755   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Registering RDD 5 (sortBy at WordCount.scala:73)
2019-07-19 10:45:31,771   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at WordCount.scala:79) with 1 output partitions
2019-07-19 10:45:31,771   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 2 (collect at WordCount.scala:79)
2019-07-19 10:45:31,771   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List(ShuffleMapStage 1)
2019-07-19 10:45:31,771   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List(ShuffleMapStage 1)
2019-07-19 10:45:31,771   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:71), which has no missing parents
2019-07-19 10:45:31,834   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1 stored as values in memory (estimated size 4.9 KB, free 1426.2 MB)
2019-07-19 10:45:31,834   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1426.2 MB)
2019-07-19 10:45:31,834   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.1.103:6491 (size: 2.9 KB, free: 1426.5 MB)
2019-07-19 10:45:31,834   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 1 from broadcast at DAGScheduler.scala:1161
2019-07-19 10:45:31,849   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCount.scala:71) (first 15 tasks are for partitions Vector(0, 1))
2019-07-19 10:45:31,865   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-19 10:45:31,896   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 0.0 (TID 0, 192.168.1.113, executor 0, partition 0, ANY, 7885 bytes)
2019-07-19 10:45:31,896   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 1.0 in stage 0.0 (TID 1, 192.168.1.112, executor 1, partition 1, ANY, 7885 bytes)
2019-07-19 10:45:32,209   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.1.113:33135 (size: 2.9 KB, free: 366.3 MB)
2019-07-19 10:45:32,209   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_1_piece0 in memory on 192.168.1.112:38815 (size: 2.9 KB, free: 366.3 MB)
2019-07-19 10:45:32,334   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.1.113:33135 (size: 22.1 KB, free: 366.3 MB)
2019-07-19 10:45:32,349   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on 192.168.1.112:38815 (size: 22.1 KB, free: 366.3 MB)
2019-07-19 10:45:33,208   INFO --- [task-result-getter-1]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 1.0 in stage 0.0 (TID 1) in 1297 ms on 192.168.1.112 (executor 1) (1/2)
2019-07-19 10:45:33,208   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 0.0 (TID 0) in 1328 ms on 192.168.1.113 (executor 0) (2/2)
2019-07-19 10:45:33,208   INFO --- [task-result-getter-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 0.0, whose tasks have all completed, from pool 
2019-07-19 10:45:33,224   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 0 (map at WordCount.scala:71) finished in 1.422 s
2019-07-19 10:45:33,224   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-19 10:45:33,224   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-19 10:45:33,224   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ShuffleMapStage 1, ResultStage 2)
2019-07-19 10:45:33,224   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-19 10:45:33,240   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:73), which has no missing parents
2019-07-19 10:45:33,240   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2 stored as values in memory (estimated size 4.3 KB, free 1426.2 MB)
2019-07-19 10:45:33,255   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.5 KB, free 1426.2 MB)
2019-07-19 10:45:33,255   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on 192.168.1.103:6491 (size: 2.5 KB, free: 1426.5 MB)
2019-07-19 10:45:33,255   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 2 from broadcast at DAGScheduler.scala:1161
2019-07-19 10:45:33,255   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at sortBy at WordCount.scala:73) (first 15 tasks are for partitions Vector(0))
2019-07-19 10:45:33,255   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 1.0 with 1 tasks
2019-07-19 10:45:33,287   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 1.0 (TID 2, 192.168.1.112, executor 1, partition 0, NODE_LOCAL, 7655 bytes)
2019-07-19 10:45:33,333   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_2_piece0 in memory on 192.168.1.112:38815 (size: 2.5 KB, free: 366.3 MB)
2019-07-19 10:45:33,365   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : Asked to send map output locations for shuffle 1 to 192.168.1.112:45140
2019-07-19 10:45:33,443   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 1.0 (TID 2) in 156 ms on 192.168.1.112 (executor 1) (1/1)
2019-07-19 10:45:33,443   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ShuffleMapStage 1 (sortBy at WordCount.scala:73) finished in 0.203 s
2019-07-19 10:45:33,443   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : looking for newly runnable stages
2019-07-19 10:45:33,443   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : running: Set()
2019-07-19 10:45:33,443   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : waiting: Set(ResultStage 2)
2019-07-19 10:45:33,443   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : failed: Set()
2019-07-19 10:45:33,443   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:73), which has no missing parents
2019-07-19 10:45:33,443   INFO --- [task-result-getter-2]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 1.0, whose tasks have all completed, from pool 
2019-07-19 10:45:33,458   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3 stored as values in memory (estimated size 3.6 KB, free 1426.2 MB)
2019-07-19 10:45:33,458   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.1 KB, free 1426.2 MB)
2019-07-19 10:45:33,458   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on 192.168.1.103:6491 (size: 2.1 KB, free: 1426.5 MB)
2019-07-19 10:45:33,458   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 3 from broadcast at DAGScheduler.scala:1161
2019-07-19 10:45:33,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at sortBy at WordCount.scala:73) (first 15 tasks are for partitions Vector(0))
2019-07-19 10:45:33,474   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 2.0 with 1 tasks
2019-07-19 10:45:33,474   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Starting task 0.0 in stage 2.0 (TID 3, 192.168.1.112, executor 1, partition 0, NODE_LOCAL, 7666 bytes)
2019-07-19 10:45:33,505   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_3_piece0 in memory on 192.168.1.112:38815 (size: 2.1 KB, free: 366.3 MB)
2019-07-19 10:45:33,505   INFO --- [dispatcher-event-loop-3]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : Asked to send map output locations for shuffle 0 to 192.168.1.112:45140
2019-07-19 10:45:33,537   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSetManager(line:54) : Finished task 0.0 in stage 2.0 (TID 3) in 63 ms on 192.168.1.112 (executor 1) (1/1)
2019-07-19 10:45:33,537   INFO --- [task-result-getter-3]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Removed TaskSet 2.0, whose tasks have all completed, from pool 
2019-07-19 10:45:33,537   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : ResultStage 2 (collect at WordCount.scala:79) finished in 0.079 s
2019-07-19 10:45:33,537   INFO --- [main]  org.apache.spark.scheduler.DAGScheduler(line:54) : Job 0 finished: collect at WordCount.scala:79, took 3.055628 s
2019-07-19 10:45:33,552   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:318) : Stopped Spark@2878a2d3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-19 10:45:33,568   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Stopped Spark web UI at http://192.168.1.103:4040
2019-07-19 10:45:33,568   INFO --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Shutting down all executors
2019-07-19 10:45:33,568   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asking each executor to shut down
2019-07-19 10:45:33,583   INFO --- [dispatcher-event-loop-0]  org.apache.spark.MapOutputTrackerMasterEndpoint(line:54) : MapOutputTrackerMasterEndpoint stopped!
2019-07-19 10:45:33,630   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore cleared
2019-07-19 10:45:33,630   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : BlockManager stopped
2019-07-19 10:45:33,646   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : BlockManagerMaster stopped
2019-07-19 10:45:33,646   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint(line:54) : OutputCommitCoordinator stopped!
2019-07-19 10:45:33,662   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Successfully stopped SparkContext
2019-07-19 10:45:33,662   INFO --- [main]  WordCount$(line:88) : complete!
2019-07-19 10:45:33,677   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Shutdown hook called
2019-07-19 10:45:33,677   INFO --- [Thread-1]  org.apache.spark.util.ShutdownHookManager(line:54) : Deleting directory C:\Users\fucheng\AppData\Local\Temp\spark-0006ff54-4635-4540-a781-79226ce40173
2019-07-19 10:52:58,739   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 10:53:13,740   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 10:53:28,739   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 10:53:43,730   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 13:10:19,634   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 13:10:34,630   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 13:10:49,642   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 13:11:04,630   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 13:11:19,631   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 15:33:35,874   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Running Spark version 2.4.3
2019-07-19 15:33:36,488   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Submitted application: CreateRDD
2019-07-19 15:33:36,601   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls to: fucheng
2019-07-19 15:33:36,605   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls to: fucheng
2019-07-19 15:33:36,607   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing view acls groups to: 
2019-07-19 15:33:36,608   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : Changing modify acls groups to: 
2019-07-19 15:33:36,610   INFO --- [main]  org.apache.spark.SecurityManager(line:54) : SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(fucheng); groups with view permissions: Set(); users  with modify permissions: Set(fucheng); groups with modify permissions: Set()
2019-07-19 15:33:40,610   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'sparkDriver' on port 10384.
2019-07-19 15:33:40,693   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering MapOutputTracker
2019-07-19 15:33:40,719   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering BlockManagerMaster
2019-07-19 15:33:40,723   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2019-07-19 15:33:40,724   INFO --- [main]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : BlockManagerMasterEndpoint up
2019-07-19 15:33:40,741   INFO --- [main]  org.apache.spark.storage.DiskBlockManager(line:54) : Created local directory at C:\Users\fucheng\AppData\Local\Temp\blockmgr-a624762d-4ef0-42b1-a31a-3a79036600a6
2019-07-19 15:33:40,778   INFO --- [main]  org.apache.spark.storage.memory.MemoryStore(line:54) : MemoryStore started with capacity 1426.5 MB
2019-07-19 15:33:40,808   INFO --- [main]  org.apache.spark.SparkEnv(line:54) : Registering OutputCommitCoordinator
2019-07-19 15:33:40,936   INFO --- [main]  org.spark_project.jetty.util.log(line:192) : Logging initialized @16299ms
2019-07-19 15:33:41,044   INFO --- [main]  org.spark_project.jetty.server.Server(line:351) : jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
2019-07-19 15:33:41,073   INFO --- [main]  org.spark_project.jetty.server.Server(line:419) : Started @16437ms
2019-07-19 15:33:41,123   INFO --- [main]  org.spark_project.jetty.server.AbstractConnector(line:278) : Started ServerConnector@4202953d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-19 15:33:41,139   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'SparkUI' on port 4040.
2019-07-19 15:33:41,197   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@2552f2cb{/jobs,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,199   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@66629f63{/jobs/json,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,200   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@841e575{/jobs/job,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,203   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1e5f4170{/jobs/job/json,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,204   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6c345c5f{/stages,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,205   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6b5966e1{/stages/json,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,206   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@65e61854{/stages/stage,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,209   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6f80fafe{/stages/stage/json,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,211   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3af17be2{/stages/pool,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,212   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@f9879ac{/stages/pool/json,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,214   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@37f21974{/storage,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,215   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5f4d427e{/storage/json,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,217   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6e521c1e{/storage/rdd,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,220   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@224b4d61{/storage/rdd/json,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,222   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5d5d9e5{/environment,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,227   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@303e3593{/environment/json,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,229   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@4ef27d66{/executors,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,231   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@362a019c{/executors/json,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,232   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@1d9bec4d{/executors/threadDump,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,233   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@5c48c0c0{/executors/threadDump/json,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,246   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@10c8f62{/static,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,248   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@591e58fa{/,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,251   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@3954d008{/api,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,259   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@6d8792db{/jobs/job/kill,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,261   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@64bc21ac{/stages/stage/kill,null,AVAILABLE,@Spark}
2019-07-19 15:33:41,265   INFO --- [main]  org.apache.spark.ui.SparkUI(line:54) : Bound SparkUI to 0.0.0.0, and started at http://fc-pc:4040
2019-07-19 15:33:41,511   INFO --- [appclient-register-master-threadpool-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Connecting to master spark://Node02:7077...
2019-07-19 15:33:41,603   INFO --- [netty-rpc-connection-0]  org.apache.spark.network.client.TransportClientFactory(line:267) : Successfully created connection to Node02/192.168.1.112:7077 after 58 ms (0 ms spent in bootstraps)
2019-07-19 15:33:41,751   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Connected to Spark cluster with app ID app-20190719153342-0011
2019-07-19 15:33:41,763   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/0 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:41,781   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/0 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:41,782   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/1 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:41,787   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/1 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:41,788   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/2 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:41,790   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/2 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:41,794   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/2 is now RUNNING
2019-07-19 15:33:41,795   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/1 is now RUNNING
2019-07-19 15:33:41,796   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/0 is now RUNNING
2019-07-19 15:33:41,804   INFO --- [main]  org.apache.spark.util.Utils(line:54) : Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 10406.
2019-07-19 15:33:41,806   INFO --- [main]  org.apache.spark.network.netty.NettyBlockTransferService(line:54) : Server created on fc-pc:10406
2019-07-19 15:33:41,809   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2019-07-19 15:33:41,869   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registering BlockManager BlockManagerId(driver, fc-pc, 10406, None)
2019-07-19 15:33:41,875   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Registering block manager fc-pc:10406 with 1426.5 MB RAM, BlockManagerId(driver, fc-pc, 10406, None)
2019-07-19 15:33:41,880   INFO --- [main]  org.apache.spark.storage.BlockManagerMaster(line:54) : Registered BlockManager BlockManagerId(driver, fc-pc, 10406, None)
2019-07-19 15:33:41,882   INFO --- [main]  org.apache.spark.storage.BlockManager(line:54) : Initialized BlockManager: BlockManagerId(driver, fc-pc, 10406, None)
2019-07-19 15:33:42,336   INFO --- [main]  org.spark_project.jetty.server.handler.ContextHandler(line:781) : Started o.s.j.s.ServletContextHandler@49d98dc5{/metrics/json,null,AVAILABLE,@Spark}
2019-07-19 15:33:42,408   INFO --- [main]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2019-07-19 15:33:43,086   INFO --- [main]  org.apache.spark.SparkContext(line:54) : Starting job: collect at CreateRDD.scala:16
2019-07-19 15:33:43,132   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Got job 0 (collect at CreateRDD.scala:16) with 2 output partitions
2019-07-19 15:33:43,134   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Final stage: ResultStage 0 (collect at CreateRDD.scala:16)
2019-07-19 15:33:43,135   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Parents of final stage: List()
2019-07-19 15:33:43,138   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Missing parents: List()
2019-07-19 15:33:43,159   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at CreateRDD.scala:15), which has no missing parents
2019-07-19 15:33:43,160   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/2 is now EXITED (Command exited with code 1)
2019-07-19 15:33:43,163   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/2 removed: Command exited with code 1
2019-07-19 15:33:43,169   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/3 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:43,172   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/3 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:43,173   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/3 is now RUNNING
2019-07-19 15:33:43,185   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 2 requested
2019-07-19 15:33:43,188   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 2
2019-07-19 15:33:43,201   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/0 is now EXITED (Command exited with code 1)
2019-07-19 15:33:43,201   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/0 removed: Command exited with code 1
2019-07-19 15:33:43,202   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/4 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:43,203   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/4 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:43,203   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/4 is now RUNNING
2019-07-19 15:33:43,204   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 0 requested
2019-07-19 15:33:43,204   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 0
2019-07-19 15:33:43,225   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/1 is now EXITED (Command exited with code 1)
2019-07-19 15:33:43,226   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/1 removed: Command exited with code 1
2019-07-19 15:33:43,227   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/5 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:43,227   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 1 requested
2019-07-19 15:33:43,228   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 1
2019-07-19 15:33:43,229   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/5 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:43,230   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/5 is now RUNNING
2019-07-19 15:33:43,233   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 2 from BlockManagerMaster.
2019-07-19 15:33:43,234   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 0 from BlockManagerMaster.
2019-07-19 15:33:43,236   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 1 from BlockManagerMaster.
2019-07-19 15:33:43,553   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0 stored as values in memory (estimated size 1464.0 B, free 1426.5 MB)
2019-07-19 15:33:43,618   INFO --- [dag-scheduler-event-loop]  org.apache.spark.storage.memory.MemoryStore(line:54) : Block broadcast_0_piece0 stored as bytes in memory (estimated size 988.0 B, free 1426.5 MB)
2019-07-19 15:33:43,623   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerInfo(line:54) : Added broadcast_0_piece0 in memory on fc-pc:10406 (size: 988.0 B, free: 1426.5 MB)
2019-07-19 15:33:43,627   INFO --- [dag-scheduler-event-loop]  org.apache.spark.SparkContext(line:54) : Created broadcast 0 from broadcast at DAGScheduler.scala:1161
2019-07-19 15:33:43,657   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.DAGScheduler(line:54) : Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at CreateRDD.scala:15) (first 15 tasks are for partitions Vector(0, 1))
2019-07-19 15:33:43,659   INFO --- [dag-scheduler-event-loop]  org.apache.spark.scheduler.TaskSchedulerImpl(line:54) : Adding task set 0.0 with 2 tasks
2019-07-19 15:33:44,500   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/3 is now EXITED (Command exited with code 1)
2019-07-19 15:33:44,501   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/3 removed: Command exited with code 1
2019-07-19 15:33:44,502   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/6 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:44,502   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 3 from BlockManagerMaster.
2019-07-19 15:33:44,503   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/6 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:44,502   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 3 requested
2019-07-19 15:33:44,504   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/6 is now RUNNING
2019-07-19 15:33:44,504   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 3
2019-07-19 15:33:44,642   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/4 is now EXITED (Command exited with code 1)
2019-07-19 15:33:44,652   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/4 removed: Command exited with code 1
2019-07-19 15:33:44,653   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/7 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:44,653   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 4 from BlockManagerMaster.
2019-07-19 15:33:44,653   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 4 requested
2019-07-19 15:33:44,655   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/7 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:44,656   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 4
2019-07-19 15:33:44,657   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/7 is now RUNNING
2019-07-19 15:33:44,708   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/5 is now EXITED (Command exited with code 1)
2019-07-19 15:33:44,709   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/5 removed: Command exited with code 1
2019-07-19 15:33:44,710   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/8 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:44,711   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/8 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:44,712   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 5 requested
2019-07-19 15:33:44,712   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 5
2019-07-19 15:33:44,712   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/8 is now RUNNING
2019-07-19 15:33:44,712   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 5 from BlockManagerMaster.
2019-07-19 15:33:45,888   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/6 is now EXITED (Command exited with code 1)
2019-07-19 15:33:45,895   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/6 removed: Command exited with code 1
2019-07-19 15:33:45,897   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/9 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:45,898   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/9 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:45,900   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/9 is now RUNNING
2019-07-19 15:33:45,901   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 6 from BlockManagerMaster.
2019-07-19 15:33:45,899   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 6 requested
2019-07-19 15:33:45,903   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 6
2019-07-19 15:33:46,101   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/7 is now EXITED (Command exited with code 1)
2019-07-19 15:33:46,106   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/7 removed: Command exited with code 1
2019-07-19 15:33:46,107   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/10 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:46,108   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 7 from BlockManagerMaster.
2019-07-19 15:33:46,108   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 7 requested
2019-07-19 15:33:46,112   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/10 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:46,113   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 7
2019-07-19 15:33:46,114   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/10 is now RUNNING
2019-07-19 15:33:46,199   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/8 is now EXITED (Command exited with code 1)
2019-07-19 15:33:46,200   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/8 removed: Command exited with code 1
2019-07-19 15:33:46,201   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 8 from BlockManagerMaster.
2019-07-19 15:33:46,201   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 8 requested
2019-07-19 15:33:46,202   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 8
2019-07-19 15:33:46,201   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/11 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:46,205   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/11 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:46,206   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/11 is now RUNNING
2019-07-19 15:33:47,203   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/9 is now EXITED (Command exited with code 1)
2019-07-19 15:33:47,204   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/9 removed: Command exited with code 1
2019-07-19 15:33:47,207   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/12 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:47,209   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 9 requested
2019-07-19 15:33:47,212   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 9
2019-07-19 15:33:47,213   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 9 from BlockManagerMaster.
2019-07-19 15:33:47,209   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/12 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:47,218   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/12 is now RUNNING
2019-07-19 15:33:47,557   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/10 is now EXITED (Command exited with code 1)
2019-07-19 15:33:47,558   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/10 removed: Command exited with code 1
2019-07-19 15:33:47,558   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/13 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:47,559   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 10 from BlockManagerMaster.
2019-07-19 15:33:47,559   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/13 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:47,559   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 10 requested
2019-07-19 15:33:47,560   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 10
2019-07-19 15:33:47,561   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/13 is now RUNNING
2019-07-19 15:33:47,688   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/11 is now EXITED (Command exited with code 1)
2019-07-19 15:33:47,689   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/11 removed: Command exited with code 1
2019-07-19 15:33:47,698   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/14 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:47,703   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/14 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:47,709   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/14 is now RUNNING
2019-07-19 15:33:47,717   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 11 requested
2019-07-19 15:33:47,718   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 11
2019-07-19 15:33:47,718   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 11 from BlockManagerMaster.
2019-07-19 15:33:48,559   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/12 is now EXITED (Command exited with code 1)
2019-07-19 15:33:48,561   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/12 removed: Command exited with code 1
2019-07-19 15:33:48,562   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/15 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:48,569   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/15 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:48,568   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 12 requested
2019-07-19 15:33:48,571   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/15 is now RUNNING
2019-07-19 15:33:48,570   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 12 from BlockManagerMaster.
2019-07-19 15:33:48,572   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 12
2019-07-19 15:33:49,030   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/13 is now EXITED (Command exited with code 1)
2019-07-19 15:33:49,031   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/13 removed: Command exited with code 1
2019-07-19 15:33:49,033   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 13 from BlockManagerMaster.
2019-07-19 15:33:49,033   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/16 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:49,035   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/16 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:49,033   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 13 requested
2019-07-19 15:33:49,037   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 13
2019-07-19 15:33:49,037   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/16 is now RUNNING
2019-07-19 15:33:49,209   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/14 is now EXITED (Command exited with code 1)
2019-07-19 15:33:49,210   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/14 removed: Command exited with code 1
2019-07-19 15:33:49,212   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 14 requested
2019-07-19 15:33:49,214   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 14
2019-07-19 15:33:49,213   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 14 from BlockManagerMaster.
2019-07-19 15:33:49,213   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/17 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:49,217   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/17 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:49,217   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/17 is now RUNNING
2019-07-19 15:33:49,959   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/15 is now EXITED (Command exited with code 1)
2019-07-19 15:33:49,962   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/15 removed: Command exited with code 1
2019-07-19 15:33:49,964   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 15 from BlockManagerMaster.
2019-07-19 15:33:49,964   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/18 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:49,964   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 15 requested
2019-07-19 15:33:49,966   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 15
2019-07-19 15:33:49,966   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/18 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:49,968   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/18 is now RUNNING
2019-07-19 15:33:50,534   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/16 is now EXITED (Command exited with code 1)
2019-07-19 15:33:50,535   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/16 removed: Command exited with code 1
2019-07-19 15:33:50,536   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 16 requested
2019-07-19 15:33:50,536   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 16 from BlockManagerMaster.
2019-07-19 15:33:50,536   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/19 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:50,536   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 16
2019-07-19 15:33:50,538   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/19 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:50,539   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/19 is now RUNNING
2019-07-19 15:33:50,702   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/17 is now EXITED (Command exited with code 1)
2019-07-19 15:33:50,704   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/17 removed: Command exited with code 1
2019-07-19 15:33:50,706   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/20 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:50,706   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 17 from BlockManagerMaster.
2019-07-19 15:33:50,706   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 17 requested
2019-07-19 15:33:50,707   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/20 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:50,708   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 17
2019-07-19 15:33:50,710   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/20 is now RUNNING
2019-07-19 15:33:51,254   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/18 is now EXITED (Command exited with code 1)
2019-07-19 15:33:51,255   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/18 removed: Command exited with code 1
2019-07-19 15:33:51,256   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 18 from BlockManagerMaster.
2019-07-19 15:33:51,256   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/21 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:51,256   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 18 requested
2019-07-19 15:33:51,260   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 18
2019-07-19 15:33:51,260   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/21 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:51,262   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/21 is now RUNNING
2019-07-19 15:33:51,984   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/19 is now EXITED (Command exited with code 1)
2019-07-19 15:33:51,985   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/19 removed: Command exited with code 1
2019-07-19 15:33:51,986   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 19 from BlockManagerMaster.
2019-07-19 15:33:51,986   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 19 requested
2019-07-19 15:33:51,986   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/22 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:51,989   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 19
2019-07-19 15:33:51,991   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/22 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:51,992   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/22 is now RUNNING
2019-07-19 15:33:52,201   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/20 is now EXITED (Command exited with code 1)
2019-07-19 15:33:52,205   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/20 removed: Command exited with code 1
2019-07-19 15:33:52,206   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/23 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:52,207   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 20 from BlockManagerMaster.
2019-07-19 15:33:52,208   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/23 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:52,206   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 20 requested
2019-07-19 15:33:52,209   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/23 is now RUNNING
2019-07-19 15:33:52,209   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 20
2019-07-19 15:33:52,613   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/21 is now EXITED (Command exited with code 1)
2019-07-19 15:33:52,614   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/21 removed: Command exited with code 1
2019-07-19 15:33:52,616   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 21 requested
2019-07-19 15:33:52,616   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 21
2019-07-19 15:33:52,616   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 21 from BlockManagerMaster.
2019-07-19 15:33:52,616   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/24 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:52,624   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/24 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:52,626   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/24 is now RUNNING
2019-07-19 15:33:53,441   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/22 is now EXITED (Command exited with code 1)
2019-07-19 15:33:53,442   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/22 removed: Command exited with code 1
2019-07-19 15:33:53,443   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 22 from BlockManagerMaster.
2019-07-19 15:33:53,443   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/25 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:53,443   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 22 requested
2019-07-19 15:33:53,444   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 22
2019-07-19 15:33:53,444   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/25 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:53,444   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/25 is now RUNNING
2019-07-19 15:33:53,687   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/23 is now EXITED (Command exited with code 1)
2019-07-19 15:33:53,688   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/23 removed: Command exited with code 1
2019-07-19 15:33:53,689   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 23 requested
2019-07-19 15:33:53,689   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/26 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:53,689   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 23 from BlockManagerMaster.
2019-07-19 15:33:53,689   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 23
2019-07-19 15:33:53,690   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/26 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:53,690   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/26 is now RUNNING
2019-07-19 15:33:53,996   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/24 is now EXITED (Command exited with code 1)
2019-07-19 15:33:53,999   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/24 removed: Command exited with code 1
2019-07-19 15:33:54,001   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/27 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:54,001   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 24 requested
2019-07-19 15:33:54,002   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 24
2019-07-19 15:33:54,001   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 24 from BlockManagerMaster.
2019-07-19 15:33:54,004   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/27 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:54,005   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/27 is now RUNNING
2019-07-19 15:33:54,993   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/25 is now EXITED (Command exited with code 1)
2019-07-19 15:33:54,994   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/25 removed: Command exited with code 1
2019-07-19 15:33:54,995   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 25 requested
2019-07-19 15:33:54,996   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 25 from BlockManagerMaster.
2019-07-19 15:33:54,996   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 25
2019-07-19 15:33:54,995   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/28 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:54,999   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/28 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:55,000   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/28 is now RUNNING
2019-07-19 15:33:55,181   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/26 is now EXITED (Command exited with code 1)
2019-07-19 15:33:55,183   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/26 removed: Command exited with code 1
2019-07-19 15:33:55,184   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 26 requested
2019-07-19 15:33:55,185   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 26
2019-07-19 15:33:55,185   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 26 from BlockManagerMaster.
2019-07-19 15:33:55,185   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/29 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:55,188   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/29 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:55,189   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/29 is now RUNNING
2019-07-19 15:33:55,327   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/27 is now EXITED (Command exited with code 1)
2019-07-19 15:33:55,327   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/27 removed: Command exited with code 1
2019-07-19 15:33:55,328   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/30 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:55,328   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 27 from BlockManagerMaster.
2019-07-19 15:33:55,328   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 27 requested
2019-07-19 15:33:55,328   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/30 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:55,329   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 27
2019-07-19 15:33:55,330   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/30 is now RUNNING
2019-07-19 15:33:56,457   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/28 is now EXITED (Command exited with code 1)
2019-07-19 15:33:56,458   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/28 removed: Command exited with code 1
2019-07-19 15:33:56,460   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 28 requested
2019-07-19 15:33:56,460   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 28 from BlockManagerMaster.
2019-07-19 15:33:56,460   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 28
2019-07-19 15:33:56,460   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/31 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:56,463   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/31 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:56,464   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/31 is now RUNNING
2019-07-19 15:33:56,669   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/29 is now EXITED (Command exited with code 1)
2019-07-19 15:33:56,671   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/29 removed: Command exited with code 1
2019-07-19 15:33:56,672   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/32 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:56,673   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 29 requested
2019-07-19 15:33:56,674   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 29
2019-07-19 15:33:56,673   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/32 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:56,675   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 29 from BlockManagerMaster.
2019-07-19 15:33:56,678   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/32 is now RUNNING
2019-07-19 15:33:56,696   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/30 is now EXITED (Command exited with code 1)
2019-07-19 15:33:56,699   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/30 removed: Command exited with code 1
2019-07-19 15:33:56,700   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/33 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:56,700   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 30 from BlockManagerMaster.
2019-07-19 15:33:56,700   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/33 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:56,700   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 30 requested
2019-07-19 15:33:56,701   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/33 is now RUNNING
2019-07-19 15:33:56,701   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 30
2019-07-19 15:33:57,909   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/31 is now EXITED (Command exited with code 1)
2019-07-19 15:33:57,910   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/31 removed: Command exited with code 1
2019-07-19 15:33:57,911   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/34 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:57,911   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 31 requested
2019-07-19 15:33:57,912   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 31
2019-07-19 15:33:57,911   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 31 from BlockManagerMaster.
2019-07-19 15:33:57,912   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/34 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:57,918   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/34 is now RUNNING
2019-07-19 15:33:58,015   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/33 is now EXITED (Command exited with code 1)
2019-07-19 15:33:58,016   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/33 removed: Command exited with code 1
2019-07-19 15:33:58,019   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/35 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:58,021   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 33 requested
2019-07-19 15:33:58,022   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 33
2019-07-19 15:33:58,021   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 33 from BlockManagerMaster.
2019-07-19 15:33:58,021   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/35 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:58,024   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/35 is now RUNNING
2019-07-19 15:33:58,152   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/32 is now EXITED (Command exited with code 1)
2019-07-19 15:33:58,153   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/32 removed: Command exited with code 1
2019-07-19 15:33:58,153   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 32 requested
2019-07-19 15:33:58,154   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 32
2019-07-19 15:33:58,154   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 32 from BlockManagerMaster.
2019-07-19 15:33:58,154   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/36 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:58,156   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/36 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:58,156   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/36 is now RUNNING
2019-07-19 15:33:58,683   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 15:33:59,362   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/34 is now EXITED (Command exited with code 1)
2019-07-19 15:33:59,363   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/34 removed: Command exited with code 1
2019-07-19 15:33:59,364   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/37 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:33:59,364   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 34 requested
2019-07-19 15:33:59,364   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 34 from BlockManagerMaster.
2019-07-19 15:33:59,366   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/37 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:59,365   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 34
2019-07-19 15:33:59,367   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/37 is now RUNNING
2019-07-19 15:33:59,398   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/35 is now EXITED (Command exited with code 1)
2019-07-19 15:33:59,399   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/35 removed: Command exited with code 1
2019-07-19 15:33:59,400   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 35 from BlockManagerMaster.
2019-07-19 15:33:59,400   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 35 requested
2019-07-19 15:33:59,401   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 35
2019-07-19 15:33:59,400   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/38 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:33:59,403   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/38 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:33:59,407   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/38 is now RUNNING
2019-07-19 15:33:59,637   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/36 is now EXITED (Command exited with code 1)
2019-07-19 15:33:59,638   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/36 removed: Command exited with code 1
2019-07-19 15:33:59,639   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/39 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:33:59,641   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/39 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:33:59,641   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/39 is now RUNNING
2019-07-19 15:33:59,642   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 36 from BlockManagerMaster.
2019-07-19 15:33:59,643   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 36 requested
2019-07-19 15:33:59,645   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 36
2019-07-19 15:34:00,767   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/38 is now EXITED (Command exited with code 1)
2019-07-19 15:34:00,768   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/38 removed: Command exited with code 1
2019-07-19 15:34:00,770   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 38 from BlockManagerMaster.
2019-07-19 15:34:00,771   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 38 requested
2019-07-19 15:34:00,771   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/40 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:00,772   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 38
2019-07-19 15:34:00,773   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/40 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:00,773   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/40 is now RUNNING
2019-07-19 15:34:00,824   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/37 is now EXITED (Command exited with code 1)
2019-07-19 15:34:00,825   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/37 removed: Command exited with code 1
2019-07-19 15:34:00,825   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/41 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:00,825   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 37 requested
2019-07-19 15:34:00,826   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 37
2019-07-19 15:34:00,825   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 37 from BlockManagerMaster.
2019-07-19 15:34:00,826   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/41 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:00,830   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/41 is now RUNNING
2019-07-19 15:34:01,137   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/39 is now EXITED (Command exited with code 1)
2019-07-19 15:34:01,138   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/39 removed: Command exited with code 1
2019-07-19 15:34:01,138   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/42 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:01,138   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 39 from BlockManagerMaster.
2019-07-19 15:34:01,138   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 39 requested
2019-07-19 15:34:01,139   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/42 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:01,140   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 39
2019-07-19 15:34:01,142   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/42 is now RUNNING
2019-07-19 15:34:02,125   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/40 is now EXITED (Command exited with code 1)
2019-07-19 15:34:02,126   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/40 removed: Command exited with code 1
2019-07-19 15:34:02,127   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/43 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:02,128   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/43 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:02,129   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/43 is now RUNNING
2019-07-19 15:34:02,129   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 40 requested
2019-07-19 15:34:02,130   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 40
2019-07-19 15:34:02,130   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 40 from BlockManagerMaster.
2019-07-19 15:34:02,309   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/41 is now EXITED (Command exited with code 1)
2019-07-19 15:34:02,311   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/41 removed: Command exited with code 1
2019-07-19 15:34:02,312   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/44 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:02,313   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 41 requested
2019-07-19 15:34:02,314   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 41
2019-07-19 15:34:02,314   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 41 from BlockManagerMaster.
2019-07-19 15:34:02,314   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/44 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:02,318   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/44 is now RUNNING
2019-07-19 15:34:02,641   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/42 is now EXITED (Command exited with code 1)
2019-07-19 15:34:02,641   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/42 removed: Command exited with code 1
2019-07-19 15:34:02,642   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 42 requested
2019-07-19 15:34:02,643   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 42 from BlockManagerMaster.
2019-07-19 15:34:02,643   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 42
2019-07-19 15:34:02,643   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/45 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:02,648   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/45 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:02,649   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/45 is now RUNNING
2019-07-19 15:34:03,510   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/43 is now EXITED (Command exited with code 1)
2019-07-19 15:34:03,511   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/43 removed: Command exited with code 1
2019-07-19 15:34:03,511   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/46 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:03,511   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 43 requested
2019-07-19 15:34:03,512   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 43
2019-07-19 15:34:03,511   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 43 from BlockManagerMaster.
2019-07-19 15:34:03,512   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/46 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:03,514   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/46 is now RUNNING
2019-07-19 15:34:03,806   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/44 is now EXITED (Command exited with code 1)
2019-07-19 15:34:03,806   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/44 removed: Command exited with code 1
2019-07-19 15:34:03,806   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 44 from BlockManagerMaster.
2019-07-19 15:34:03,806   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/47 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:03,806   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 44 requested
2019-07-19 15:34:03,807   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 44
2019-07-19 15:34:03,807   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/47 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:03,809   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/47 is now RUNNING
2019-07-19 15:34:04,143   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/45 is now EXITED (Command exited with code 1)
2019-07-19 15:34:04,144   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/45 removed: Command exited with code 1
2019-07-19 15:34:04,144   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/48 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:04,145   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 45 requested
2019-07-19 15:34:04,145   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 45
2019-07-19 15:34:04,145   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 45 from BlockManagerMaster.
2019-07-19 15:34:04,145   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/48 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:04,147   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/48 is now RUNNING
2019-07-19 15:34:04,876   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/46 is now EXITED (Command exited with code 1)
2019-07-19 15:34:04,877   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/46 removed: Command exited with code 1
2019-07-19 15:34:04,879   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/49 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:04,882   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/49 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:04,882   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 46 requested
2019-07-19 15:34:04,884   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 46
2019-07-19 15:34:04,884   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 46 from BlockManagerMaster.
2019-07-19 15:34:04,884   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/49 is now RUNNING
2019-07-19 15:34:05,311   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/47 is now EXITED (Command exited with code 1)
2019-07-19 15:34:05,313   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/47 removed: Command exited with code 1
2019-07-19 15:34:05,327   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/50 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:05,329   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/50 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:05,330   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/50 is now RUNNING
2019-07-19 15:34:05,331   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 47 requested
2019-07-19 15:34:05,332   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 47
2019-07-19 15:34:05,332   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 47 from BlockManagerMaster.
2019-07-19 15:34:05,787   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/48 is now EXITED (Command exited with code 1)
2019-07-19 15:34:05,788   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/48 removed: Command exited with code 1
2019-07-19 15:34:05,789   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/51 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:05,791   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/51 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:05,791   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/51 is now RUNNING
2019-07-19 15:34:05,793   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 48 requested
2019-07-19 15:34:05,793   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 48
2019-07-19 15:34:05,793   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 48 from BlockManagerMaster.
2019-07-19 15:34:06,190   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/49 is now EXITED (Command exited with code 1)
2019-07-19 15:34:06,191   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/49 removed: Command exited with code 1
2019-07-19 15:34:06,192   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/52 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:06,192   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/52 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:06,193   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/52 is now RUNNING
2019-07-19 15:34:06,194   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 49 requested
2019-07-19 15:34:06,195   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 49
2019-07-19 15:34:06,195   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 49 from BlockManagerMaster.
2019-07-19 15:34:06,815   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/50 is now EXITED (Command exited with code 1)
2019-07-19 15:34:06,816   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/50 removed: Command exited with code 1
2019-07-19 15:34:06,817   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/53 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:06,817   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 50 requested
2019-07-19 15:34:06,823   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 50
2019-07-19 15:34:06,823   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 50 from BlockManagerMaster.
2019-07-19 15:34:06,823   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/53 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:06,824   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/53 is now RUNNING
2019-07-19 15:34:07,125   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/51 is now EXITED (Command exited with code 1)
2019-07-19 15:34:07,127   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/51 removed: Command exited with code 1
2019-07-19 15:34:07,128   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 51 requested
2019-07-19 15:34:07,128   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 51 from BlockManagerMaster.
2019-07-19 15:34:07,128   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/54 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:07,128   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 51
2019-07-19 15:34:07,129   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/54 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:07,129   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/54 is now RUNNING
2019-07-19 15:34:07,615   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/52 is now EXITED (Command exited with code 1)
2019-07-19 15:34:07,616   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/52 removed: Command exited with code 1
2019-07-19 15:34:07,638   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/55 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:07,642   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/55 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:07,642   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/55 is now RUNNING
2019-07-19 15:34:07,643   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 52 requested
2019-07-19 15:34:07,644   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 52
2019-07-19 15:34:07,649   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 52 from BlockManagerMaster.
2019-07-19 15:34:08,269   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/53 is now EXITED (Command exited with code 1)
2019-07-19 15:34:08,269   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/53 removed: Command exited with code 1
2019-07-19 15:34:08,270   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/56 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:08,270   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 53 from BlockManagerMaster.
2019-07-19 15:34:08,270   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 53 requested
2019-07-19 15:34:08,272   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 53
2019-07-19 15:34:08,270   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/56 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:08,273   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/56 is now RUNNING
2019-07-19 15:34:08,619   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/54 is now EXITED (Command exited with code 1)
2019-07-19 15:34:08,619   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/54 removed: Command exited with code 1
2019-07-19 15:34:08,620   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/57 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:08,620   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 54 requested
2019-07-19 15:34:08,620   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 54
2019-07-19 15:34:08,620   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 54 from BlockManagerMaster.
2019-07-19 15:34:08,620   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/57 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:08,622   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/57 is now RUNNING
2019-07-19 15:34:08,952   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/55 is now EXITED (Command exited with code 1)
2019-07-19 15:34:08,952   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/55 removed: Command exited with code 1
2019-07-19 15:34:08,953   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/58 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:08,953   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 55 requested
2019-07-19 15:34:08,953   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 55
2019-07-19 15:34:08,953   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 55 from BlockManagerMaster.
2019-07-19 15:34:08,954   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/58 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:08,956   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/58 is now RUNNING
2019-07-19 15:34:09,733   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/56 is now EXITED (Command exited with code 1)
2019-07-19 15:34:09,733   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/56 removed: Command exited with code 1
2019-07-19 15:34:09,733   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 56 from BlockManagerMaster.
2019-07-19 15:34:09,733   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 56 requested
2019-07-19 15:34:09,733   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/59 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:09,734   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 56
2019-07-19 15:34:09,734   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/59 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:09,736   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/59 is now RUNNING
2019-07-19 15:34:10,128   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/57 is now EXITED (Command exited with code 1)
2019-07-19 15:34:10,128   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/57 removed: Command exited with code 1
2019-07-19 15:34:10,130   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/60 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:10,131   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/60 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:10,131   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/60 is now RUNNING
2019-07-19 15:34:10,132   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 57 from BlockManagerMaster.
2019-07-19 15:34:10,133   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 57 requested
2019-07-19 15:34:10,134   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 57
2019-07-19 15:34:10,328   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/58 is now EXITED (Command exited with code 1)
2019-07-19 15:34:10,330   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/58 removed: Command exited with code 1
2019-07-19 15:34:10,330   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/61 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:10,331   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 58 requested
2019-07-19 15:34:10,334   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 58
2019-07-19 15:34:10,335   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 58 from BlockManagerMaster.
2019-07-19 15:34:10,336   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/61 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:10,336   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/61 is now RUNNING
2019-07-19 15:34:11,219   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/59 is now EXITED (Command exited with code 1)
2019-07-19 15:34:11,219   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/59 removed: Command exited with code 1
2019-07-19 15:34:11,221   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/62 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:11,221   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 59 requested
2019-07-19 15:34:11,222   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/62 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:11,223   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/62 is now RUNNING
2019-07-19 15:34:11,223   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 59 from BlockManagerMaster.
2019-07-19 15:34:11,222   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 59
2019-07-19 15:34:11,627   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/60 is now EXITED (Command exited with code 1)
2019-07-19 15:34:11,628   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/60 removed: Command exited with code 1
2019-07-19 15:34:11,630   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/63 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:11,632   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 60 requested
2019-07-19 15:34:11,634   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 60
2019-07-19 15:34:11,649   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 60 from BlockManagerMaster.
2019-07-19 15:34:11,634   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/63 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:11,651   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/63 is now RUNNING
2019-07-19 15:34:11,652   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/61 is now EXITED (Command exited with code 1)
2019-07-19 15:34:11,654   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/61 removed: Command exited with code 1
2019-07-19 15:34:11,655   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/64 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:11,656   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 61 from BlockManagerMaster.
2019-07-19 15:34:11,656   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 61 requested
2019-07-19 15:34:11,657   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/64 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:11,657   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 61
2019-07-19 15:34:11,657   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/64 is now RUNNING
2019-07-19 15:34:12,668   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/62 is now EXITED (Command exited with code 1)
2019-07-19 15:34:12,668   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/62 removed: Command exited with code 1
2019-07-19 15:34:12,670   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/65 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:12,671   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 62 requested
2019-07-19 15:34:12,671   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 62
2019-07-19 15:34:12,671   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/65 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:12,671   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 62 from BlockManagerMaster.
2019-07-19 15:34:12,672   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/65 is now RUNNING
2019-07-19 15:34:12,992   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/64 is now EXITED (Command exited with code 1)
2019-07-19 15:34:12,994   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/64 removed: Command exited with code 1
2019-07-19 15:34:12,997   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/66 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:12,998   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 64 requested
2019-07-19 15:34:12,998   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 64
2019-07-19 15:34:12,999   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 64 from BlockManagerMaster.
2019-07-19 15:34:12,999   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/66 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:13,000   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/66 is now RUNNING
2019-07-19 15:34:13,121   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/63 is now EXITED (Command exited with code 1)
2019-07-19 15:34:13,123   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/63 removed: Command exited with code 1
2019-07-19 15:34:13,124   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/67 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:13,124   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 63 requested
2019-07-19 15:34:13,125   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 63
2019-07-19 15:34:13,125   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/67 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:13,126   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/67 is now RUNNING
2019-07-19 15:34:13,125   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 63 from BlockManagerMaster.
2019-07-19 15:34:13,681   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 15:34:14,131   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/65 is now EXITED (Command exited with code 1)
2019-07-19 15:34:14,132   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/65 removed: Command exited with code 1
2019-07-19 15:34:14,132   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/68 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:14,133   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 65 from BlockManagerMaster.
2019-07-19 15:34:14,133   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/68 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:14,133   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 65 requested
2019-07-19 15:34:14,133   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 65
2019-07-19 15:34:14,137   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/68 is now RUNNING
2019-07-19 15:34:14,394   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/66 is now EXITED (Command exited with code 1)
2019-07-19 15:34:14,394   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/66 removed: Command exited with code 1
2019-07-19 15:34:14,395   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/69 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:14,395   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 66 requested
2019-07-19 15:34:14,395   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 66 from BlockManagerMaster.
2019-07-19 15:34:14,395   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/69 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:14,395   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 66
2019-07-19 15:34:14,398   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/69 is now RUNNING
2019-07-19 15:34:14,612   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/67 is now EXITED (Command exited with code 1)
2019-07-19 15:34:14,612   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/67 removed: Command exited with code 1
2019-07-19 15:34:14,612   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 67 from BlockManagerMaster.
2019-07-19 15:34:14,612   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/70 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:14,612   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 67 requested
2019-07-19 15:34:14,613   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 67
2019-07-19 15:34:14,613   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/70 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:14,614   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/70 is now RUNNING
2019-07-19 15:34:15,620   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/68 is now EXITED (Command exited with code 1)
2019-07-19 15:34:15,620   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/68 removed: Command exited with code 1
2019-07-19 15:34:15,620   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 68 from BlockManagerMaster.
2019-07-19 15:34:15,620   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 68 requested
2019-07-19 15:34:15,620   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/71 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:15,621   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 68
2019-07-19 15:34:15,621   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/71 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:15,623   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/71 is now RUNNING
2019-07-19 15:34:15,745   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/69 is now EXITED (Command exited with code 1)
2019-07-19 15:34:15,746   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/69 removed: Command exited with code 1
2019-07-19 15:34:15,746   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 69 from BlockManagerMaster.
2019-07-19 15:34:15,746   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/72 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:15,746   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 69 requested
2019-07-19 15:34:15,747   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 69
2019-07-19 15:34:15,747   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/72 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:15,748   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/72 is now RUNNING
2019-07-19 15:34:16,100   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/70 is now EXITED (Command exited with code 1)
2019-07-19 15:34:16,101   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/70 removed: Command exited with code 1
2019-07-19 15:34:16,101   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 70 from BlockManagerMaster.
2019-07-19 15:34:16,101   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 70 requested
2019-07-19 15:34:16,101   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/73 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:16,102   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 70
2019-07-19 15:34:16,102   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/73 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:16,103   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/73 is now RUNNING
2019-07-19 15:34:17,068   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/71 is now EXITED (Command exited with code 1)
2019-07-19 15:34:17,069   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/71 removed: Command exited with code 1
2019-07-19 15:34:17,069   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/74 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:17,069   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 71 requested
2019-07-19 15:34:17,070   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 71
2019-07-19 15:34:17,069   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 71 from BlockManagerMaster.
2019-07-19 15:34:17,070   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/74 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:17,073   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/74 is now RUNNING
2019-07-19 15:34:17,105   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/72 is now EXITED (Command exited with code 1)
2019-07-19 15:34:17,107   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/72 removed: Command exited with code 1
2019-07-19 15:34:17,115   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/75 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:17,116   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 72 requested
2019-07-19 15:34:17,117   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 72
2019-07-19 15:34:17,117   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/75 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:17,117   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 72 from BlockManagerMaster.
2019-07-19 15:34:17,118   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/75 is now RUNNING
2019-07-19 15:34:17,586   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/73 is now EXITED (Command exited with code 1)
2019-07-19 15:34:17,587   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/73 removed: Command exited with code 1
2019-07-19 15:34:17,587   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/76 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:17,588   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 73 from BlockManagerMaster.
2019-07-19 15:34:17,588   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/76 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:17,588   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 73 requested
2019-07-19 15:34:17,588   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/76 is now RUNNING
2019-07-19 15:34:17,589   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 73
2019-07-19 15:34:18,472   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/75 is now EXITED (Command exited with code 1)
2019-07-19 15:34:18,472   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/75 removed: Command exited with code 1
2019-07-19 15:34:18,473   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 75 from BlockManagerMaster.
2019-07-19 15:34:18,473   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 75 requested
2019-07-19 15:34:18,473   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/77 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:18,473   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 75
2019-07-19 15:34:18,474   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/77 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:18,474   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/77 is now RUNNING
2019-07-19 15:34:18,522   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/74 is now EXITED (Command exited with code 1)
2019-07-19 15:34:18,523   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/74 removed: Command exited with code 1
2019-07-19 15:34:18,523   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/78 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:18,523   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 74 requested
2019-07-19 15:34:18,523   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 74 from BlockManagerMaster.
2019-07-19 15:34:18,524   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/78 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:18,524   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 74
2019-07-19 15:34:18,525   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/78 is now RUNNING
2019-07-19 15:34:19,076   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/76 is now EXITED (Command exited with code 1)
2019-07-19 15:34:19,077   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/76 removed: Command exited with code 1
2019-07-19 15:34:19,077   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/79 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:19,078   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/79 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:19,077   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 76 from BlockManagerMaster.
2019-07-19 15:34:19,077   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 76 requested
2019-07-19 15:34:19,079   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 76
2019-07-19 15:34:19,079   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/79 is now RUNNING
2019-07-19 15:34:19,812   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/77 is now EXITED (Command exited with code 1)
2019-07-19 15:34:19,813   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/77 removed: Command exited with code 1
2019-07-19 15:34:19,817   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 77 requested
2019-07-19 15:34:19,818   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 77
2019-07-19 15:34:19,817   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 77 from BlockManagerMaster.
2019-07-19 15:34:19,817   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/80 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:19,820   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/80 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:19,821   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/80 is now RUNNING
2019-07-19 15:34:19,969   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/78 is now EXITED (Command exited with code 1)
2019-07-19 15:34:19,971   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/78 removed: Command exited with code 1
2019-07-19 15:34:19,987   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/81 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:19,991   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/81 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:19,992   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/81 is now RUNNING
2019-07-19 15:34:19,995   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 78 requested
2019-07-19 15:34:19,996   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 78
2019-07-19 15:34:19,997   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 78 from BlockManagerMaster.
2019-07-19 15:34:20,580   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/79 is now EXITED (Command exited with code 1)
2019-07-19 15:34:20,581   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/79 removed: Command exited with code 1
2019-07-19 15:34:20,582   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/82 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:20,582   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 79 requested
2019-07-19 15:34:20,583   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 79
2019-07-19 15:34:20,583   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/82 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:20,583   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 79 from BlockManagerMaster.
2019-07-19 15:34:20,587   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/82 is now RUNNING
2019-07-19 15:34:21,164   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/80 is now EXITED (Command exited with code 1)
2019-07-19 15:34:21,164   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/80 removed: Command exited with code 1
2019-07-19 15:34:21,165   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/83 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:21,165   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 80 from BlockManagerMaster.
2019-07-19 15:34:21,165   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 80 requested
2019-07-19 15:34:21,165   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/83 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:21,166   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 80
2019-07-19 15:34:21,168   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/83 is now RUNNING
2019-07-19 15:34:21,426   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/81 is now EXITED (Command exited with code 1)
2019-07-19 15:34:21,426   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/81 removed: Command exited with code 1
2019-07-19 15:34:21,429   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/84 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:21,431   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 81 from BlockManagerMaster.
2019-07-19 15:34:21,431   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/84 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:21,431   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 81 requested
2019-07-19 15:34:21,432   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 81
2019-07-19 15:34:21,432   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/84 is now RUNNING
2019-07-19 15:34:22,077   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/82 is now EXITED (Command exited with code 1)
2019-07-19 15:34:22,077   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/82 removed: Command exited with code 1
2019-07-19 15:34:22,078   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/85 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:22,079   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 82 from BlockManagerMaster.
2019-07-19 15:34:22,079   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/85 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:22,078   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 82 requested
2019-07-19 15:34:22,080   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/85 is now RUNNING
2019-07-19 15:34:22,080   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 82
2019-07-19 15:34:22,542   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/83 is now EXITED (Command exited with code 1)
2019-07-19 15:34:22,544   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/83 removed: Command exited with code 1
2019-07-19 15:34:22,545   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/86 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:22,547   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/86 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:22,548   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/86 is now RUNNING
2019-07-19 15:34:22,549   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 83 from BlockManagerMaster.
2019-07-19 15:34:22,546   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 83 requested
2019-07-19 15:34:22,550   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 83
2019-07-19 15:34:22,885   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/84 is now EXITED (Command exited with code 1)
2019-07-19 15:34:22,886   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/84 removed: Command exited with code 1
2019-07-19 15:34:22,886   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/87 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:22,886   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 84 from BlockManagerMaster.
2019-07-19 15:34:22,888   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/87 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:22,887   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 84 requested
2019-07-19 15:34:22,889   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/87 is now RUNNING
2019-07-19 15:34:22,889   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 84
2019-07-19 15:34:23,564   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/85 is now EXITED (Command exited with code 1)
2019-07-19 15:34:23,565   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/85 removed: Command exited with code 1
2019-07-19 15:34:23,565   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/88 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:23,565   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 85 requested
2019-07-19 15:34:23,565   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 85 from BlockManagerMaster.
2019-07-19 15:34:23,566   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/88 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:23,566   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 85
2019-07-19 15:34:23,569   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/88 is now RUNNING
2019-07-19 15:34:23,853   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/86 is now EXITED (Command exited with code 1)
2019-07-19 15:34:23,853   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/86 removed: Command exited with code 1
2019-07-19 15:34:23,854   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 86 from BlockManagerMaster.
2019-07-19 15:34:23,854   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 86 requested
2019-07-19 15:34:23,855   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 86
2019-07-19 15:34:23,854   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/89 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:23,855   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/89 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:23,856   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/89 is now RUNNING
2019-07-19 15:34:24,347   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/87 is now EXITED (Command exited with code 1)
2019-07-19 15:34:24,347   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/87 removed: Command exited with code 1
2019-07-19 15:34:24,348   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 87 requested
2019-07-19 15:34:24,349   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 87
2019-07-19 15:34:24,348   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/90 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:24,349   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 87 from BlockManagerMaster.
2019-07-19 15:34:24,351   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/90 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:24,352   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/90 is now RUNNING
2019-07-19 15:34:25,057   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/88 is now EXITED (Command exited with code 1)
2019-07-19 15:34:25,058   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/88 removed: Command exited with code 1
2019-07-19 15:34:25,059   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/91 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:25,059   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 88 requested
2019-07-19 15:34:25,060   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/91 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:25,060   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 88
2019-07-19 15:34:25,059   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 88 from BlockManagerMaster.
2019-07-19 15:34:25,061   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/91 is now RUNNING
2019-07-19 15:34:25,207   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/89 is now EXITED (Command exited with code 1)
2019-07-19 15:34:25,208   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/89 removed: Command exited with code 1
2019-07-19 15:34:25,209   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/92 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:25,210   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 89 from BlockManagerMaster.
2019-07-19 15:34:25,210   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/92 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:25,209   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 89 requested
2019-07-19 15:34:25,211   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 89
2019-07-19 15:34:25,211   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/92 is now RUNNING
2019-07-19 15:34:25,793   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/90 is now EXITED (Command exited with code 1)
2019-07-19 15:34:25,794   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/90 removed: Command exited with code 1
2019-07-19 15:34:25,795   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/93 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:25,795   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 90 requested
2019-07-19 15:34:25,796   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 90
2019-07-19 15:34:25,796   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 90 from BlockManagerMaster.
2019-07-19 15:34:25,796   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/93 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:25,798   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/93 is now RUNNING
2019-07-19 15:34:26,544   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/91 is now EXITED (Command exited with code 1)
2019-07-19 15:34:26,545   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/91 removed: Command exited with code 1
2019-07-19 15:34:26,545   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/94 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:26,548   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 91 from BlockManagerMaster.
2019-07-19 15:34:26,546   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 91 requested
2019-07-19 15:34:26,548   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/94 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:26,550   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 91
2019-07-19 15:34:26,550   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/94 is now RUNNING
2019-07-19 15:34:26,584   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/92 is now EXITED (Command exited with code 1)
2019-07-19 15:34:26,585   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/92 removed: Command exited with code 1
2019-07-19 15:34:26,586   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/95 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:26,586   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 92 from BlockManagerMaster.
2019-07-19 15:34:26,586   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 92 requested
2019-07-19 15:34:26,586   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/95 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:26,587   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 92
2019-07-19 15:34:26,588   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/95 is now RUNNING
2019-07-19 15:34:27,245   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/93 is now EXITED (Command exited with code 1)
2019-07-19 15:34:27,246   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/93 removed: Command exited with code 1
2019-07-19 15:34:27,247   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 93 from BlockManagerMaster.
2019-07-19 15:34:27,247   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 93 requested
2019-07-19 15:34:27,248   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 93
2019-07-19 15:34:27,247   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/96 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:27,249   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/96 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:27,250   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/96 is now RUNNING
2019-07-19 15:34:27,953   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/95 is now EXITED (Command exited with code 1)
2019-07-19 15:34:27,954   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/95 removed: Command exited with code 1
2019-07-19 15:34:27,956   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/97 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:27,956   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 95 requested
2019-07-19 15:34:27,957   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 95
2019-07-19 15:34:27,958   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 95 from BlockManagerMaster.
2019-07-19 15:34:27,957   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/97 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:27,959   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/97 is now RUNNING
2019-07-19 15:34:28,042   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/94 is now EXITED (Command exited with code 1)
2019-07-19 15:34:28,043   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/94 removed: Command exited with code 1
2019-07-19 15:34:28,044   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/98 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:28,044   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 94 requested
2019-07-19 15:34:28,045   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 94
2019-07-19 15:34:28,044   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 94 from BlockManagerMaster.
2019-07-19 15:34:28,045   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/98 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:28,046   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/98 is now RUNNING
2019-07-19 15:34:28,682   WARN --- [Timer-0]  org.apache.spark.scheduler.TaskSchedulerImpl(line:66) : Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources
2019-07-19 15:34:28,692   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/96 is now EXITED (Command exited with code 1)
2019-07-19 15:34:28,693   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/96 removed: Command exited with code 1
2019-07-19 15:34:28,694   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/99 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:28,694   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 96 from BlockManagerMaster.
2019-07-19 15:34:28,694   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 96 requested
2019-07-19 15:34:28,695   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/99 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:28,696   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 96
2019-07-19 15:34:28,696   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/99 is now RUNNING
2019-07-19 15:34:29,297   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/97 is now EXITED (Command exited with code 1)
2019-07-19 15:34:29,297   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/97 removed: Command exited with code 1
2019-07-19 15:34:29,297   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 97 from BlockManagerMaster.
2019-07-19 15:34:29,297   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 97 requested
2019-07-19 15:34:29,297   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/100 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:29,298   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 97
2019-07-19 15:34:29,298   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/100 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:29,300   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/100 is now RUNNING
2019-07-19 15:34:29,526   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/98 is now EXITED (Command exited with code 1)
2019-07-19 15:34:29,527   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/98 removed: Command exited with code 1
2019-07-19 15:34:29,528   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 98 from BlockManagerMaster.
2019-07-19 15:34:29,528   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 98 requested
2019-07-19 15:34:29,528   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 98
2019-07-19 15:34:29,528   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/101 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:29,529   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/101 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:29,530   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/101 is now RUNNING
2019-07-19 15:34:30,164   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/99 is now EXITED (Command exited with code 1)
2019-07-19 15:34:30,164   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/99 removed: Command exited with code 1
2019-07-19 15:34:30,165   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 99 from BlockManagerMaster.
2019-07-19 15:34:30,165   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 99 requested
2019-07-19 15:34:30,165   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 99
2019-07-19 15:34:30,175   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/102 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:30,176   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/102 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:30,179   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/102 is now RUNNING
2019-07-19 15:34:30,642   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/100 is now EXITED (Command exited with code 1)
2019-07-19 15:34:30,642   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/100 removed: Command exited with code 1
2019-07-19 15:34:30,643   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/103 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:30,643   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 100 from BlockManagerMaster.
2019-07-19 15:34:30,643   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 100 requested
2019-07-19 15:34:30,643   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/103 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:30,643   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 100
2019-07-19 15:34:30,646   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/103 is now RUNNING
2019-07-19 15:34:31,029   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/101 is now EXITED (Command exited with code 1)
2019-07-19 15:34:31,030   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/101 removed: Command exited with code 1
2019-07-19 15:34:31,030   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/104 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:31,030   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 101 requested
2019-07-19 15:34:31,031   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 101
2019-07-19 15:34:31,030   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 101 from BlockManagerMaster.
2019-07-19 15:34:31,031   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/104 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:31,033   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/104 is now RUNNING
2019-07-19 15:34:31,630   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/102 is now EXITED (Command exited with code 1)
2019-07-19 15:34:31,630   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/102 removed: Command exited with code 1
2019-07-19 15:34:31,631   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/105 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:31,631   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 102 requested
2019-07-19 15:34:31,631   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 102 from BlockManagerMaster.
2019-07-19 15:34:31,633   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 102
2019-07-19 15:34:31,632   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/105 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:31,634   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/105 is now RUNNING
2019-07-19 15:34:31,982   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/103 is now EXITED (Command exited with code 1)
2019-07-19 15:34:31,982   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/103 removed: Command exited with code 1
2019-07-19 15:34:31,983   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/106 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:31,983   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 103 requested
2019-07-19 15:34:31,984   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 103
2019-07-19 15:34:31,983   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 103 from BlockManagerMaster.
2019-07-19 15:34:31,986   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/106 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:31,987   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/106 is now RUNNING
2019-07-19 15:34:32,513   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/104 is now EXITED (Command exited with code 1)
2019-07-19 15:34:32,513   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/104 removed: Command exited with code 1
2019-07-19 15:34:32,514   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/107 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:32,514   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 104 requested
2019-07-19 15:34:32,514   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 104 from BlockManagerMaster.
2019-07-19 15:34:32,514   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/107 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:32,514   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 104
2019-07-19 15:34:32,515   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/107 is now RUNNING
2019-07-19 15:34:33,131   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/105 is now EXITED (Command exited with code 1)
2019-07-19 15:34:33,132   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/105 removed: Command exited with code 1
2019-07-19 15:34:33,133   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/108 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:33,134   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 105 requested
2019-07-19 15:34:33,136   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 105
2019-07-19 15:34:33,136   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 105 from BlockManagerMaster.
2019-07-19 15:34:33,134   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/108 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:33,137   INFO --- [dispatcher-event-loop-3]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/108 is now RUNNING
2019-07-19 15:34:33,356   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/106 is now EXITED (Command exited with code 1)
2019-07-19 15:34:33,357   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/106 removed: Command exited with code 1
2019-07-19 15:34:33,359   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/109 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:33,359   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 106 requested
2019-07-19 15:34:33,360   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 106
2019-07-19 15:34:33,359   INFO --- [dispatcher-event-loop-0]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 106 from BlockManagerMaster.
2019-07-19 15:34:33,360   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/109 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:33,364   INFO --- [dispatcher-event-loop-1]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/109 is now RUNNING
2019-07-19 15:34:34,017   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/107 is now EXITED (Command exited with code 1)
2019-07-19 15:34:34,017   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/107 removed: Command exited with code 1
2019-07-19 15:34:34,018   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/110 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:34,018   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 107 from BlockManagerMaster.
2019-07-19 15:34:34,018   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/110 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:34,018   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 107 requested
2019-07-19 15:34:34,019   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 107
2019-07-19 15:34:34,019   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/110 is now RUNNING
2019-07-19 15:34:34,580   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/108 is now EXITED (Command exited with code 1)
2019-07-19 15:34:34,583   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/108 removed: Command exited with code 1
2019-07-19 15:34:34,584   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 108 from BlockManagerMaster.
2019-07-19 15:34:34,584   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/111 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
2019-07-19 15:34:34,584   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 108 requested
2019-07-19 15:34:34,586   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 108
2019-07-19 15:34:34,585   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/111 on hostPort 192.168.1.113:33373 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:34,587   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/111 is now RUNNING
2019-07-19 15:34:34,709   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/109 is now EXITED (Command exited with code 1)
2019-07-19 15:34:34,710   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/109 removed: Command exited with code 1
2019-07-19 15:34:34,711   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/112 on worker-20190718123550-192.168.1.114-43293 (192.168.1.114:43293) with 8 core(s)
2019-07-19 15:34:34,711   INFO --- [dispatcher-event-loop-2]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 109 from BlockManagerMaster.
2019-07-19 15:34:34,711   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 109 requested
2019-07-19 15:34:34,713   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/112 on hostPort 192.168.1.114:43293 with 8 core(s), 1024.0 MB RAM
2019-07-19 15:34:34,713   INFO --- [dispatcher-event-loop-3]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 109
2019-07-19 15:34:34,713   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/112 is now RUNNING
2019-07-19 15:34:35,520   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/110 is now EXITED (Command exited with code 1)
2019-07-19 15:34:35,520   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/110 removed: Command exited with code 1
2019-07-19 15:34:35,521   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/113 on worker-20190718203550-192.168.1.112-39355 (192.168.1.112:39355) with 12 core(s)
2019-07-19 15:34:35,525   INFO --- [dispatcher-event-loop-2]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Granted executor ID app-20190719153342-0011/113 on hostPort 192.168.1.112:39355 with 12 core(s), 1024.0 MB RAM
2019-07-19 15:34:35,526   INFO --- [dispatcher-event-loop-2]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/113 is now RUNNING
2019-07-19 15:34:35,526   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 110 requested
2019-07-19 15:34:35,527   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 110
2019-07-19 15:34:35,527   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 110 from BlockManagerMaster.
2019-07-19 15:34:36,027   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor updated: app-20190719153342-0011/111 is now EXITED (Command exited with code 1)
2019-07-19 15:34:36,028   INFO --- [dispatcher-event-loop-0]  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend(line:54) : Executor app-20190719153342-0011/111 removed: Command exited with code 1
2019-07-19 15:34:36,028   INFO --- [dispatcher-event-loop-1]  org.apache.spark.storage.BlockManagerMaster(line:54) : Removal of executor 111 requested
2019-07-19 15:34:36,029   INFO --- [dispatcher-event-loop-1]  org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint(line:54) : Asked to remove non-existent executor 111
2019-07-19 15:34:36,028   INFO --- [dispatcher-event-loop-3]  org.apache.spark.storage.BlockManagerMasterEndpoint(line:54) : Trying to remove executor 111 from BlockManagerMaster.
2019-07-19 15:34:36,028   INFO --- [dispatcher-event-loop-0]  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint(line:54) : Executor added: app-20190719153342-0011/114 on worker-20190718123550-192.168.1.113-33373 (192.168.1.113:33373) with 12 core(s)
